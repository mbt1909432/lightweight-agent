{
  "workflow_name": "FigureGenerationWorkflow",
  "session_id": "c0ab150e-6665-44d2-ab5c-5070f1805f50",
  "created_at": "2026-01-17T06:37:15.826764+00:00",
  "interface_type": "interface1",
  "stages": {
    "extraction": {
      "status": "completed",
      "ablation_extracted": true,
      "main_result_extracted": true
    },
    "planning": {
      "status": "completed",
      "ablation_figures_count": 4,
      "main_result_figures_count": 5,
      "motivation_prompts_generated": true,
      "algorithm_prompts_generated": true
    },
    "generation": {
      "status": "completed",
      "ablation": {
        "success_count": 4,
        "failed_count": 0
      },
      "main_result": {
        "success_count": 5,
        "failed_count": 0
      },
      "motivation": {
        "success": true,
        "images_count": 5
      },
      "algorithm": {
        "success": true,
        "images_count": 5
      }
    }
  },
  "artifacts": {
    "original_tex": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\original_tex.tex",
    "ablation_tex": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\extracted_snippets\\ablation_tex.tex",
    "main_result_tex": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\extracted_snippets\\main_result_tex.tex",
    "generated_figures": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures",
    "workspace": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\workspace",
    "workspace_figures": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\workspace\\figures",
    "workspace_scripts": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\workspace\\scripts",
    "workspace_planning_results": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\workspace\\planning_results.json",
    "workspace_original_tex": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\workspace\\original_tex.tex",
    "metadata": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\metadata\\metadata.json",
    "extraction_results": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\metadata\\extraction_results.json",
    "planning_results": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\metadata\\planning_results.json",
    "generation_results": "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\metadata\\generation_results.json"
  },
  "figures": {
    "ablation": [
      {
        "filename": "ablation_components_line_plot",
        "description": "A line plot showing the cumulative improvement in Average F1 score as each proposed component is sequentially added to the baseline model. X-axis represents the variants in order: Baseline, +HMT, +SMPE, +SCL, +AMTD. Y-axis shows the Average F1 score (ranging from 0.715 to 0.778). The line connects the points with markers at each variant, and annotations indicate the delta improvements (+0.018, +0.021, +0.015, +0.029). This highlights the synergistic effect of all components, with the full model achieving the highest performance.",
        "success": true,
        "image_paths": [
          "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\ablation\\ablation_components_line_plot.png"
        ],
        "error": null,
        "usage": {
          "input_tokens": 3790,
          "output_tokens": 1528
        }
      },
      {
        "filename": "ablation_smpe_bar_chart",
        "description": "A bar chart comparing the Average F1 scores for different combinations of error components in the Structured Multi-view Prediction Error (SMPE). X-axis lists the variants: Full SMPE, w/o E^{(r)}, w/o E^{(s)}, w/o E^{(m)}, Only E^{(r)}, Only E^{(s)}, Only E^{(m)}. Y-axis shows Average F1 scores (ranging from 0.738 to 0.778). Bars are colored differently for full, ablated, and single-component variants. This illustrates that the full SMPE outperforms all partial or single-error configurations, with the largest drop when removing Predictive Smoothness Error.",
        "success": true,
        "image_paths": [
          "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\ablation\\ablation_smpe_bar_chart.png"
        ],
        "error": null,
        "usage": {
          "input_tokens": 3826,
          "output_tokens": 1273
        }
      },
      {
        "filename": "ablation_amtd_scatter_plot",
        "description": "A scatter plot visualizing the Average F1 scores for variants of the Adaptive Multi-scale Temporal Discriminator (AMTD). X-axis categorically positions the variants: Full AMTD, Replace with OBD, Single-scale TCN, Multi-scale TCN w/o Self-Attention Fusion. Y-axis shows Average F1 scores (ranging from 0.748 to 0.778). Points are marked with different shapes and colors, connected by lines to show performance drops from the full model. This emphasizes the contributions of multi-scale perception and adaptive fusion to the AMTD's effectiveness.",
        "success": true,
        "image_paths": [
          "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\ablation\\ablation_amtd_scatter_plot.png"
        ],
        "error": null,
        "usage": {
          "input_tokens": 3799,
          "output_tokens": 1215
        }
      },
      {
        "filename": "ablation_scl_box_plot",
        "description": "A box plot (simulated for single values as points with pseudo-distributions for visualization) comparing Average F1 scores for different combinations of the Structural Consistency Loss (SCL) terms. X-axis lists the variants: L_{Focal} only, L_{Focal} + L_{Smooth}, L_{Focal} + L_{Cluster}, Full SCL. Y-axis shows Average F1 scores (ranging from 0.754 to 0.769). Each 'box' represents the score with minimal spread for illustration, highlighting improvements from adding smoothness and clustering losses. This demonstrates that the full SCL provides the best performance by optimizing local and global coherence.",
        "success": true,
        "image_paths": [
          "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\ablation\\ablation_scl_box_plot.png"
        ],
        "error": null,
        "usage": {
          "input_tokens": 3802,
          "output_tokens": 1395
        }
      }
    ],
    "main_result": [
      {
        "filename": "online_kinetics_f1_lineplot",
        "description": "A line plot comparing F1 scores of various online methods (TeSTra-BC, Sim-On-BC, OadTR-BC, MiniROAD-BC, ESTimator, ESTimator++) across Rel.Dis thresholds from 0.05 to 0.5 on the Kinetics-GEBD validation set. X-axis: Rel.Dis thresholds (0.05 to 0.5 in steps of 0.05), Y-axis: F1 scores. Each method is represented by a distinct line with markers. Highlights that ESTimator++ consistently outperforms others, especially at lower thresholds, with an average F1 of 0.778, showing +3.0% improvement over ESTimator.",
        "success": true,
        "image_paths": [
          "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\main_result\\online_kinetics_f1_lineplot.png"
        ],
        "error": null,
        "usage": {
          "input_tokens": 3840,
          "output_tokens": 1901
        }
      },
      {
        "filename": "online_tapos_f1_heatmap",
        "description": "A heatmap visualizing F1 scores for online methods (TeSTra-BC, Sim-On-BC, OadTR-BC, MiniROAD-BC, ESTimator, ESTimator++) across Rel.Dis thresholds from 0.05 to 0.5 and the average on the TAPOS validation set. Rows: methods, Columns: Rel.Dis thresholds plus average. Color intensity represents F1 score values (darker for higher scores). Emphasizes ESTimator++'s superiority at stricter thresholds, achieving an average F1 of 0.569, with +2.2% over ESTimator.",
        "success": true,
        "image_paths": [
          "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\main_result\\online_tapos_f1_heatmap.png"
        ],
        "error": null,
        "usage": {
          "input_tokens": 3824,
          "output_tokens": 1753
        }
      },
      {
        "filename": "offline_kinetics_avgf1_barchart",
        "description": "A bar chart comparing average F1 scores of offline and online methods (including supervised and unsupervised offline like BMN, TCN, PC, CoSeg, and online like ESTimator, ESTimator++) on the Kinetics-GEBD dataset. X-axis: methods grouped by setting (offline supervised, offline unsupervised, online), Y-axis: average F1 scores. Bars are colored by setting, with error bars if applicable (though not in data). Illustrates that ESTimator++ (0.778) rivals top offline methods like CoSeg (0.782) and surpasses most supervised offline ones except PC.",
        "success": true,
        "image_paths": [
          "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\main_result\\offline_kinetics_avgf1_barchart.png"
        ],
        "error": null,
        "usage": {
          "input_tokens": 3816,
          "output_tokens": 1288
        }
      },
      {
        "filename": "offline_tapos_f1_scatterplot",
        "description": "A scatter plot showing F1 scores versus Rel.Dis thresholds for offline and online methods on the TAPOS dataset. X-axis: Rel.Dis thresholds (0.05 to 0.5), Y-axis: F1 scores. Each method is a scatter line with points, differentiated by colors and markers (e.g., PC, TCN, PA, ESTimator, ESTimator++). Demonstrates ESTimator++'s performance (avg 0.569) outperforming most offline supervised methods except PC, with strong results at lower thresholds.",
        "success": true,
        "image_paths": [
          "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\main_result\\offline_tapos_f1_scatterplot.png"
        ],
        "error": null,
        "usage": {
          "input_tokens": 3801,
          "output_tokens": 1817
        }
      },
      {
        "filename": "inference_efficiency_barchart",
        "description": "A bar chart comparing inference time per frame (in ms) for online methods (TeSTra-BC, MiniROAD-BC, ESTimator, ESTimator++) and noting total clip times for offline methods (TCN, PC). X-axis: methods, separated into online and offline groups, Y-axis: time (ms), with logarithmic scale for offline to show scale difference. Highlights ESTimator++'s efficiency at 8.2 ms per frame, balancing speed and accuracy for real-time use.",
        "success": true,
        "image_paths": [
          "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\main_result\\inference_efficiency_barchart.png"
        ],
        "error": null,
        "usage": {
          "input_tokens": 3763,
          "output_tokens": 1537
        }
      }
    ],
    "motivation": {
      "success": true,
      "image_paths": [
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\motivation\\motivation_1_bfe91589.png",
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\motivation\\motivation_2_de4e210c.png",
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\motivation\\motivation_3_4bb29ecb.png",
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\motivation\\motivation_4_0b412769.png",
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\motivation\\motivation_5_f764bbdc.png"
      ],
      "prompts_used": [
        "A professional, academic motivation figure for a NeurIPS-style paper on Online Generic Event Boundary Detection (On-GEBD), illustrating the core motivation by contrasting offline vs. online processing and highlighting the inspiration from Event Segmentation Theory (EST). The figure is a clean, intuitive visualization with two main horizontal panels side-by-side for clear comparison, using a minimalistic style with sharp lines, ample white space, and high contrast for easy understanding. Overall layout: top title \"Motivation for Online Generic Event Boundary Detection\" in bold Arial font, size 14; subtitles in Arial font, size 12; all labels in Arial font, size 10, black text for readability.\n\nLeft panel (50% width, labeled \"Offline GEBD: Full Video Context\"): Depict a horizontal timeline of a video stream with 10 sequential abstract frames (simple icons like a person jumping, then landing, then walking, representing action transitions). Show bidirectional arrows across the entire timeline indicating access to past, present, and future frames (blue arrows for past/future, green for present). Mark 3-4 event boundaries with vertical dashed red lines, and overlay a smooth curve below representing low-variance boundary detection signals. Emphasize global context with a semi-transparent blue overlay covering the whole timeline. Caption below: \"Offline methods use full past, present, and future context for accurate but non-real-time boundary detection.\"\n\nRight panel (50% width, labeled \"Online GEBD: Streaming, Causal Processing\"): Show the same video timeline but as a streaming sequence, with frames arriving from left to right; past frames in full color, current frame highlighted in green, future frames grayed out and inaccessible (with a lock icon). Use unidirectional arrows (orange for data flow) pointing rightward, indicating real-time processing without future access. Mark boundaries with vertical dashed red lines detected only up to the current frame, and overlay a jagged curve below representing prediction error spikes (in red) at boundaries, inspired by EST. Emphasize challenges with subtle transitions shown as faint frame changes and a question mark on future frames. Caption below: \"Online methods mimic human cognition (EST): predict based on past/present only, detecting boundaries via prediction errors in real-time, but face challenges like limited context and subtle transitions.\"\n\nBottom section (spanning both panels): A small conceptual diagram of EST with a stylized brain icon on the left, connected by an arrow to a prediction module showing \"Predicted Frame\" vs. \"Actual Frame\"; low error (green checkmark) inside an event segment, high error (red exclamation) at a boundary, leading to event segmentation. Use contrasting colors: blue tones for offline elements, orange/red tones for online challenges and errors, green for successful predictions. Ensure the figure is visually balanced, easy to interpret at a glance, with no clutter, suitable for black-and-white printing but enhanced with color for emphasis.",
        "A highly professional, academic-style motivation figure for a NeurIPS-level paper on Online Generic Event Boundary Detection (On-GEBD), illustrating the key challenges and contrasts between offline and online video segmentation. The figure is a clean, intuitive visualization divided into two main panels for clear comparison, using a simplistic and straight-on layout with high contrast to emphasize differences. \n\nTop panel: Labeled \"Offline GEBD (Traditional Approach)\" in bold Arial font. Depict a horizontal timeline of a video stream as a sequence of 10 rectangular frames (abstractly represented as colorful thumbnails showing a sports action like a gymnast performing flips, transitioning between moves). The entire timeline is fully visible and accessible, with bidirectional arrows (in blue) spanning past, present, and future frames, indicating global context usage. Highlight detected event boundaries as vertical red dashed lines at frames 3, 6, and 9, with labels like \"Boundary Detected Using Full Context\" in small Calibri font. Show a central processing box with inputs from all directions, symbolizing offline methods' reliance on future information for accurate segmentation.\n\nBottom panel: Labeled \"Online GEBD (Our Challenge)\" in bold Arial font. Depict the same video stream but as a streaming input from left to right, with only past and current frames visible (frames 1-5 shown, future frames 6-10 faded out or hidden behind a \"streaming barrier\" wall). Use unidirectional arrows (in green) pointing rightward to indicate causal, frame-by-frame processing without future access. Mark potential boundaries with question marks at frames 3 and 5, and show real-time decision icons (checkmarks or crosses) emerging sequentially, with a label \"Immediate Decisions with Limited Info\" in small Calibri font. Include a small inset icon of a human brain with a thought bubble saying \"Mimic Human Perception (EST): Predict and Detect Errors,\" using orange accents to highlight prediction discrepancy.\n\nOverall style: Minimalist vector illustration with clean lines, no clutter, and a white background for readability. Use contrasting colors—blue for offline elements, green for online elements, red for boundaries, and orange for EST-inspired highlights—to visually distinguish concepts. Ensure easy understanding through intuitive icons (e.g., clock for real-time, lock for no future access). Add subtle shadows for depth, and include a central title \"Motivation: Bridging Offline to Online Event Boundary Detection\" in large Arial font at the top. The figure is balanced, square-shaped, and optimized for paper insertion with high resolution and professional polish.",
        "A professional, clean, and intuitive motivation figure for an academic paper on Online Generic Event Boundary Detection (On-GEBD), styled as a high-quality illustration suitable for top conferences like NeurIPS or CVPR. The figure is divided into two main panels side-by-side for clear comparison, with a minimalist design using simple icons, arrows, and text labels to highlight contrasts and key concepts from Event Segmentation Theory (EST). Use a white background, sharp lines, and high contrast for readability. Employ Arial font for all text, with bold headings in size 14pt and body text in size 10pt. Color scheme: blue tones (e.g., deep blue for online elements) to represent real-time processing and human-like perception, red tones (e.g., crimson for offline elements) to represent full-context access, green for prediction matches, and orange for prediction errors or boundaries to emphasize differences.\n\nLeft panel: Illustrate the challenge of offline vs. online GEBD. Show a horizontal timeline of 10 simplified video frames (abstract rectangles with icons like a person running, jumping, landing to represent event segments in a sports video). Label the timeline \"Video Stream\" at the bottom. For offline GEBD, draw a large bidirectional arrow spanning the entire timeline in red, labeled \"Offline: Full Video Access (Past + Future Context)\" with icons of eyes looking forward and backward. Below it, show detected boundaries as vertical red dashed lines at key transition frames, with a caption \"Relies on global context, incompatible with real-time\". For online GEBD, draw a unidirectional arrow from left to right in blue, stopping at the current frame, labeled \"Online: Causal Processing (Only Past + Present)\" with an icon of a streaming play button. Show partial boundaries detected up to the current frame as blue dashed lines, with a caption \"Mimics human perception: sequential, real-time decisions without future info\". Add a subtle shadow effect to differentiate the two approaches, emphasizing the information gap with a red \"X\" over future frames for online.\n\nRight panel: Visualize the EST inspiration for On-GEBD. Depict a sequence of 5 frames in a row, with abstract representations (e.g., smooth motion in first three, sudden change in fourth). Above each frame, show a \"Predicted Frame\" icon (green checkmark for matches in ongoing event, orange exclamation for error at boundary). Draw a wavy line graph below the frames plotting \"Prediction Error\" over time, with low green levels during consistent events and a sharp orange peak at the boundary frame, labeled \"Boundary Detected via Significant Prediction Error\". Include icons of a brain for \"Human Cognition\" connected by a dotted line to the model, with text \"EST: Humans predict input; error signals event boundary\". Caption the panel \"Motivation: Principled online detection via prediction discrepancy, addressing offline limitations\".\n\nOverall layout: Panels separated by a thin gray vertical line, with a main title at the top \"Motivation for Online GEBD: Bridging Human Perception and Real-Time Video Understanding\" in bold Arial. Ensure the figure is balanced, visually engaging yet simple, with ample white space for clarity, and all elements proportionally sized for a square aspect ratio suitable for paper insertion.",
        "A highly detailed, professional academic motivation figure in a clean, minimalist illustrative style suitable for a NeurIPS or CVPR paper, visualizing the key challenges and motivation for Online Generic Event Boundary Detection (On-GEBD) compared to offline methods, based on Event Segmentation Theory (EST). The figure is divided into two main panels side-by-side for clear contrast: left panel illustrates \"Offline GEBD\" with full video access, right panel illustrates \"Online GEBD\" with streaming, causal processing.\n\nOverall layout: Horizontal composition on a white background, with a central dividing line; use Arial font for all text labels, keeping them concise and readable; employ a color scheme with blue tones for offline elements (representing global context), red tones for online elements (representing real-time constraints), and gray for video frames; highlight differences with contrasting shades and bold arrows; ensure intuitive visualization with simple icons and timelines for easy understanding.\n\nTop title: \"Motivation: Offline vs. Online Generic Event Boundary Detection\" in bold Arial font.\n\nLeft panel (Offline GEBD): Show a horizontal timeline of a video stream as a sequence of 10 rectangular frames (gray boxes labeled Frame 1 to Frame 10), with bidirectional arrows spanning the entire timeline in blue, indicating access to past, present, and future context; mark 3 event boundaries (vertical dashed lines in green) at frames 3, 6, and 9, each labeled \"Boundary\" with icons of smooth transitions (e.g., abstract stick figures changing actions like walking to jumping); above the timeline, a blue cloud icon representing \"Global Context Analysis\" with text \"Processes entire video at once, uses future info for accurate boundaries\"; below, pros/cons text: \"Accurate but high latency, not real-time\" in blue.\n\nRight panel (Online GEBD): Show the same video timeline, but with unidirectional arrows in red pointing rightward from left to right, indicating causal processing (only past and present); frames appear sequentially as if streaming, with a red \"streaming input\" arrow entering from the left; mark the same 3 boundaries, but with question marks on future frames to show uncertainty; include an EST-inspired element with a small brain icon predicting the next frame (e.g., a thought bubble showing predicted vs. actual frame, with a red \"Prediction Error\" spike at boundaries); above, a red clock icon representing \"Real-Time Processing\" with text \"Single-pass, frame-by-frame decisions, no future access – mimics human perception\"; below, pros/cons text: \"Low latency for real-time apps, but challenging due to limited info\" in red.\n\nBottom caption: \"Offline methods leverage full context for precise segmentation, while online methods must detect taxonomy-free event boundaries in real-time using only historical data, addressing the need for efficient video understanding in streaming scenarios.\" Ensure the figure is visually balanced, with high contrast for differences, abstract icons for concepts, and no clutter for academic clarity.",
        "A highly professional, clean, and intuitive motivation figure for a NeurIPS-style academic paper on Online Generic Event Boundary Detection (On-GEBD), illustrating the core problem motivation and conceptual contrast between offline and online GEBD methods, inspired by Event Segmentation Theory (EST). The figure is a horizontal split-panel layout with two main sections side-by-side for clear comparison, using a minimalist vector illustration style with sharp lines, subtle gradients, and high contrast for readability. Overall background is white with light gray borders; use Arial font for all text labels, sized appropriately for clarity (larger for titles, smaller for details). Employ contrasting colors: blue tones for offline elements (representing full context access), red tones for online elements (representing causal constraints), and green accents for human cognition/EST inspiration to highlight differences.\n\nLeft panel: Titled \"Offline GEBD (Traditional Approach)\" in bold Arial font at the top. Depict a horizontal video timeline as a straight blue arrow from left to right, labeled \"Video Stream\" with frame icons (simple rectangles) spaced evenly, showing past, present, and future frames accessible simultaneously via bidirectional dashed arrows connecting all frames. Above the timeline, illustrate a neural network icon (abstract box with layers) processing the entire video at once, outputting event boundaries as vertical dashed lines on the timeline. Below, add a small caption in Arial: \"Relies on global context, processes full video with past and future information, but incompatible with real-time streaming.\" Use blue shading for the timeline and network to emphasize non-causal, batch processing.\n\nRight panel: Titled \"Online GEBD (Proposed Challenge)\" in bold Arial font at the top. Mirror the timeline as a straight red arrow from left to right, but with a vertical \"Now\" line indicating the current frame, showing only past and present frames accessible (gray out future frames with a forbidden symbol). Illustrate sequential processing with a recurrent network icon (abstract loop with memory) advancing frame-by-frame, outputting immediate boundary decisions as red flags on the timeline. Incorporate EST inspiration: add a stylized human brain icon in green above the network, connected by a green arrow labeled \"Event Segmentation Theory (EST): Predict upcoming input, detect boundaries via prediction errors.\" Below, add a small caption in Arial: \"Mimics human online perception; processes causally without future access, enabling real-time detection but facing efficiency and subtlety challenges.\" Use red shading for the timeline and network to emphasize causal, streaming constraints.\n\nCenter: A vertical dividing line with contrasting icons – a clock for \"Real-Time Requirement\" on the online side and a batch folder for \"Full Video Access\" on the offline side – to visually highlight the key motivation gap. At the bottom, a unifying title in large bold Arial: \"Motivation: Bridging Offline to Online GEBD for Real-Time Video Understanding.\" Ensure the figure is balanced, not overcrowded, with ample white space, and elements scaled for easy comprehension, suitable for black-and-white printing while retaining color distinctions for emphasis."
      ],
      "usage": {},
      "generated_count": 5,
      "target_count": 5
    },
    "algorithm": {
      "success": true,
      "image_paths": [
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\algorithm\\algorithm_1_4fe904a3.png",
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\algorithm\\algorithm_2_2450eec7.png",
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\algorithm\\algorithm_3_1638069b.png",
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\algorithm\\algorithm_4_be2c6edf.png",
        "E:\\pycharm_project\\rewrite_agent\\backend\\app\\services\\workflows\\figure_generation_workflow_utils\\sample\\output\\c0ab150e-6665-44d2-ab5c-5070f1805f50\\generated_figures\\algorithm\\algorithm_5_edec0e67.png"
      ],
      "prompts_used": [
        "Create an abstract conceptual diagram illustrating the high-level architecture of ESTimator++ for Online Generic Event Boundary Detection, presented as a clean, minimalistic line drawing in a professional academic style suitable for top conferences like NeurIPS or CVPR. This is not a flowchart with decision symbols or process boxes, but an abstract framework diagram emphasizing conceptual modules, logical relationships, and data flows using abstract shapes and connections. Use a horizontal layout from left to right on a white background, with clear visual hierarchy through module grouping and color-coded arrows.\n\nOn the left, depict an abstract input module as a simple rectangular block labeled \"Input Frame Features (f_t)\" in black Arial font, connected by an orange arrow to the central module.\n\nThe central module is a large abstract block representing the \"Hierarchical Memory-augmented Transformer (HMT)\", divided into two sub-modules: an upper rounded rectangle for \"Main Event Memory (M_t)\" in blue text, and a lower one for \"Local Context Cache (C_t)\" in blue text. Show abstract internal connections with thin black lines, and an orange arrow looping within for \"Gated Update\" (labeled in small black text). From HMT, draw a green arrow to an abstract prediction node labeled \"Predicted Feature (\\hat{f}_t)\" in red text.\n\nTo the right of HMT, connect with an orange arrow to a module labeled \"Structured Multi-view Prediction Error (SMPE)\", shown as an abstract triangular shape with three internal compartments labeled \"Feature Reconstruction Error (E^r_t)\", \"Predictive Smoothness Error (E^s_t)\", and \"Memory Perturbation Error (E^m_t)\" in black Arial font. Use orange arrows for input flows into SMPE and a green arrow outputting the concatenated \"Error Vector (E_t)\".\n\nFurther right, connect with a green arrow to the \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" module, depicted as a layered abstract structure with three parallel bars for \"Multi-scale Conv (k1, k2, k3)\" in blue text, converging via thin black lines to a fusion node labeled \"Self-Attention Fusion\" in small black text, then to an output node labeled \"Boundary Probability (P_t^{boundary})\" in red text.\n\nAt the bottom, show an overarching abstract bracket or dashed enclosure spanning the entire diagram, labeled \"Structural Consistency Loss (SCL)\" in red Arial font, with green arrows indicating feedback loops for training (e.g., from SMPE and AMTD back to HMT).\n\nUse orange arrows for data input and processing flows (e.g., feature streams and updates), green arrows for feedback, output, and prediction paths (e.g., error signals and boundary decisions). Keep text in Arial font, with module names in bold black, variables in blue, and key outputs in red for emphasis. Ensure a simple, uncluttered design with straight lines, no shadows or 3D effects, and balanced spacing for readability in a paper figure.",
        "Create a high-level abstract concept diagram illustrating the architecture of ESTimator++, a framework for Online Generic Event Boundary Detection, in a clean, minimalist line art style suitable for a top-tier academic conference paper like NeurIPS or CVPR. This is not a flowchart; instead, use abstract rectangular modules and conceptual nodes to represent high-level components and their relationships, with logical data flows indicated by stylized arrows. Emphasize technical professionalism, simplicity, and clarity, avoiding any specific steps or flowchart symbols like diamonds or ovals—focus on modular architecture and conceptual connections.\n\nLayout: Arrange in a horizontal pipeline from left to right on a white background, with input on the left and output on the right. Start with an abstract node labeled \"Input Frame Features (f_t)\" as a simple box. Connect it to a central composite module labeled \"Hierarchical Memory-augmented Transformer (HMT)\" which includes sub-nodes for \"Main Event Memory (M)\" and \"Local Context Cache (C)\", shown as stacked abstract blocks with dashed lines indicating gated updates and cross-attention. From HMT, show an abstract connection to a node labeled \"Next-Frame Prediction (hat{f}_t)\", then to a module labeled \"Structured Multi-view Prediction Error (SMPE)\" depicted as a conceptual vector with three sub-components: \"Feature Reconstruction Error (E^r)\", \"Predictive Smoothness Error (E^s)\", and \"Memory Perturbation Error (E^m)\", represented as parallel abstract bars.\n\nContinue the flow to a module labeled \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" as an abstract layered structure with sub-layers for multi-scale convolutions (short, medium, long kernels) and self-attention fusion, ending in a probability output. Finally, connect to an output node labeled \"Boundary Probability (P_t^boundary)\". Overlay a curved abstract bracket encompassing the entire diagram, labeled \"Structural Consistency Loss (SCL) for End-to-End Training\" to indicate integration.\n\nVisual elements: Use thin black lines for module borders, with subtle shadows for depth. Employ orange arrows for data input and processing flows (e.g., from input to HMT and SMPE), and green arrows for feedback, output, or loop connections (e.g., memory updates in HMT and from SMPE to AMTD). Text labels in clear Arial font: black for main module names, blue for sub-components, and red for key equations or symbols like \"E_t = [E^r, E^s, E^m]\". Ensure visual hierarchy with orange indicating primary information flow and green for secondary cycles, creating a balanced, non-cluttered composition that fits a square aspect ratio for paper insertion. Keep the design abstract, conceptual, and professional, with no icons, colors only on arrows, and ample white space for readability.",
        "Create an abstract conceptual diagram illustrating the overall architecture of the ESTimator++ framework for Online Generic Event Boundary Detection, suitable for a top-tier academic conference paper like NeurIPS or CVPR. The diagram should be a high-level, minimalist line drawing in a clean, professional style with abstract modules represented as simple rectangles or rounded boxes, connected by straight arrows to show conceptual data and information flows, emphasizing logical relationships and high-level concepts rather than specific steps or flowchart symbols—explicitly avoid any flowchart elements like diamonds, ovals, or process boxes. Use a white background with subtle grid lines for alignment, ensuring the layout is horizontal and flows logically from left to right for clarity and readability.\n\nOn the left, depict an input module labeled \"Video Frame Stream\" as an abstract vertical stack of small rectangles representing frames (f_{t-Δ} to f_t), connected by an orange arrow to a central module labeled \"Hierarchical Memory-augmented Transformer (HMT)\" shown as a large abstract box divided into two sub-sections: an upper sub-box for \"Main Event Memory (M)\" (with a persistent icon like a stable cloud) and a lower sub-box for \"Local Context Cache (C)\" (with a queue-like abstract line). From HMT, draw an orange arrow downward to a small box labeled \"Next-Frame Prediction (ẑ_t, ˆf_t)\", indicating predictive output.\n\nFrom the prediction box, connect a green arrow to the right to a module labeled \"Structured Multi-view Prediction Error (SMPE)\" as an abstract triangular shape converging three inputs: label them as \"Feature Reconstruction Error (E^r)\", \"Predictive Smoothness Error (E^s)\", and \"Memory Perturbation Error (E^m)\", with the output as a vector E_t shown as a small bracketed array.\n\nFrom SMPE, draw a green arrow to the right to a module labeled \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" depicted as a layered abstract structure with three parallel bars representing multi-scale convolutions (short, medium, long kernels), fused into a single output box labeled \"Boundary Probability (P_t^{boundary})\" with a sigmoid icon.\n\nOverlay a dashed blue bounding box around the entire pipeline (HMT, SMPE, AMTD) labeled \"End-to-End Training with Structural Consistency Loss (SCL)\", with abstract icons for loss components (e.g., a focal loss symbol, smoothness curve, and clustering dots) connected via thin red lines to relevant modules to indicate supervision.\n\nUse orange arrows for data input and processing flows (e.g., frame to prediction stages), green arrows for feedback and output flows (e.g., error to discriminator stages), creating visual hierarchy; text labels in black Arial font for main modules, blue for inputs/outputs, and red for loss-related annotations; keep the design simple, balanced, and not overcrowded, with sufficient white space for academic presentation.",
        "Create an abstract conceptual diagram illustrating the overall architecture of the ESTimator++ framework for Online Generic Event Boundary Detection, suitable for a top-tier academic conference paper like NeurIPS or CVPR. The diagram should be a high-level, minimalist line-based concept graph emphasizing modular structure, logical data flows, and relationships between components, without any flowchart symbols such as decision diamonds or process boxes—use abstract rectangular or rounded modules with clean lines instead. Maintain a professional, technical aesthetic with a white background, clear visual hierarchy, and balanced composition that fills the frame without overcrowding.\n\nAt the top, show an input stream of video frame features labeled \"Frame Features f_t\" entering from the left as a horizontal timeline of small abstract icons (e.g., simple squares representing frames), connected by an orange arrow indicating data input flow.\n\nThe central module is the \"Hierarchical Memory-augmented Transformer (HMT-CEA)\", depicted as a large abstract block divided into two sub-modules: a smaller \"Local Context Cache C_t\" (FIFO queue of recent features, shown as a stack of layered rectangles) and a larger \"Main Event Memory M_t\" (gated update mechanism, represented as a central core with abstract neural layers). Use orange arrows to connect input f_t to both sub-modules, showing processing stages: an orange arrow from input to \"Projection h_t\", then to cache update, and a gated fusion to memory update. From HMT, output an anticipated feature \"\\hat{f}_t\" via a green arrow downward, representing feedback/output.\n\nBelow HMT, connect to the \"Structured Multi-view Prediction Error (SMPE)\" module as an abstract triangular or trapezoidal shape, taking inputs from h_t, z_t (from HMT), and previous states. Depict it computing a vector E_t with three abstract sub-components: \"Feature Reconstruction Error E^{(r)}_t\", \"Predictive Smoothness Error E^{(s)}_t\", and \"Memory Perturbation Error E^{(m)}_t\", shown as interconnected nodes. Use orange arrows for input processing and green arrows for the concatenated output E_t flowing rightward.\n\nTo the right of SMPE, place the \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" as a layered abstract structure with multi-scale convolutional paths (parallel lines of varying lengths for kernel sizes 3,7,15), fused via a self-attention node, leading to a final output \"Boundary Probability P_t^{boundary}\". Connect a sequence of E_t inputs from SMPE via orange arrows into AMTD, with green arrows indicating the multi-scale feature extraction, dynamic fusion, and output flow to a binary decision icon (e.g., a simple threshold gate).\n\nOverlay abstract connections for training: a dashed blue loop around the entire diagram labeled \"Structural Consistency Loss (SCL)\", with sub-labels for \"Focal Reconstruction Loss\", \"Intra-event Smoothness\", and \"Intra-event Clustering\" as small icons or text notes connected by red arrows to relevant modules (e.g., red arrow from SCL to HMT for prediction smoothness).\n\nUse a color scheme with orange arrows for data input and processing stages (e.g., feature flows and updates), green arrows for feedback, output, and loop paths (e.g., predictions and error signals), black text for main labels, blue for module names, and red for loss-related annotations. Employ Arial font for all text, keeping it concise and readable at various sizes. Ensure the diagram has a logical left-to-right flow for online processing, with abstract icons (e.g., wavy lines for temporal sequences, gear-like shapes for transformations) to enhance conceptual clarity, and subtle shadows or borders for depth without complexity. The overall layout should be symmetric and professional, evoking a high-level neural architecture overview.",
        "Create an abstract conceptual diagram of the ESTimator++ framework for Online Generic Event Boundary Detection, presented as a high-level architecture graph suitable for a NeurIPS or ICML paper. Use a clean, minimalistic line drawing style with abstract rectangular modules and simple geometric shapes to represent components, emphasizing conceptual relationships rather than specific steps—not a flowchart, avoid any process symbols like diamonds or rounded boxes. Layout the diagram horizontally from left to right on a white background, with a logical flow showing data progression through the system.\n\nOn the left, depict an abstract input module labeled \"Frame Features (f_t)\" as a simple vertical stack of small rectangles representing streaming video frames, connected by an orange arrow to the central module.\n\nIn the center, show the \"Hierarchical Memory-augmented Transformer (HMT)\" as a large abstract block divided into two sub-modules: an upper rounded rectangle labeled \"Main Event Memory (M_t)\" in blue text, and a lower rectangular queue labeled \"Local Context Cache (C_t)\" in black text; include internal abstract connections with green arrows indicating gated updates and cross-attention, and an orange arrow from input to HMT for state updates. From HMT, output an abstract prediction node labeled \"Predicted Feature (ˆf_t)\" with a green arrow pointing rightward, and another orange arrow to a comparison node labeled \"Actual Feature (h_t)\" for error computation.\n\nTo the right of HMT, depict the \"Structured Multi-view Prediction Error (SMPE)\" as an abstract triangular module computing a vector E_t, with three small embedded circles labeled \"E^(r)_t (Reconstruction)\", \"E^(s)_t (Smoothness)\", and \"E^(m)_t (Perturbation)\" in red text; connect it via orange arrows from the prediction and actual features, emphasizing multi-dimensional error extraction.\n\nFurther right, show the \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" as a layered abstract structure with three parallel bars representing multi-scale convolutions (labeled \"Short-range\", \"Medium-range\", \"Long-range\" in blue text), fused via a self-attention icon (small dot matrix), leading to an output node labeled \"Boundary Probability (P_t)\" with a green arrow pointing to a final abstract output shape labeled \"Online Boundary Detection\".\n\nIndicate training integration with a dashed green loop around the entire diagram labeled \"Structural Consistency Loss (SCL)\" in red text, connecting back to HMT and AMTD to show end-to-end optimization, without detailed equations.\n\nUse orange arrows for data input and processing flows (e.g., feature streams and error computations), green arrows for feedback, output, and cyclic updates (e.g., memory updates and predictions). Employ a professional color scheme with black text for main labels, blue for module names, red for error components, Arial font for all text, clear visual hierarchy through arrow thickness (thicker for primary flows), and subtle shading on modules for depth. Ensure the diagram is balanced, readable, and academically polished, with no clutter, fitting a single paper figure panel."
      ],
      "usage": {},
      "generated_count": 5,
      "target_count": 5
    }
  },
  "usage": {
    "input_tokens": 192834,
    "output_tokens": 36511
  }
}