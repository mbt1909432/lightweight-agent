\section{Ablation Studies}
\subsection{Ablation Study}
\label{sec:ablation}

We conduct a comprehensive ablation study on the Kinetics-GEBD validation set to validate the contribution of each component in \textbf{ESTimator++}. All experiments follow the implementation details in Section~\ref{sec:setup}.

\subsubsection{Impact of Proposed Components}
We start from a baseline model that uses a standard Transformer decoder for prediction and a single reconstruction error with a fixed heuristic threshold for detection. This baseline achieves an average F1 score of 0.715. We then sequentially integrate our proposed components, with results summarized in Table~\ref{tab:ablation_components}.

\begin{table}[htbp]
\centering
\caption{Ablation study on the contribution of each proposed component. Evaluated on Kinetics-GEBD validation set (Average F1). $\Delta$ denotes the improvement over the previous row.}
\label{tab:ablation_components}
\begin{tabular}{lcccccc}
\toprule
\textbf{Variant} & \textbf{HMT} & \textbf{SMPE} & \textbf{SCL} & \textbf{AMTD} & \textbf{Avg F1} & $\Delta$ \\
\midrule
Baseline (ESTimator Core) & & & & & 0.715 & -- \\
+ Hierarchical Memory (HMT) & \checkmark & & & & 0.733 & +0.018 \\
+ Structured Error (SMPE) & \checkmark & \checkmark & & & 0.754 & +0.021 \\
+ Consistency Loss (SCL) & \checkmark & \checkmark & \checkmark & & 0.769 & +0.015 \\
+ Adaptive Discriminator (AMTD) & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{0.778} & \textbf{+0.029} \\
\bottomrule
\end{tabular}
\end{table}

Integrating the \textbf{Hierarchical Memory-augmented Transformer (HMT)} improves performance by +1.8\%, demonstrating the importance of maintaining stable event context for consistent prediction. Replacing the single error with the \textbf{Structured Multi-view Prediction Error (SMPE)} yields a further gain of +2.1\%, confirming that a composite error signal provides richer boundary cues. Employing the \textbf{Structural Consistency Loss (SCL)} for training adds another +1.5\%, validating that enforcing intra-event smoothness and feature clustering shapes more discriminative representations. Finally, substituting the heuristic threshold with the learned \textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} provides the most substantial single-module improvement of +2.9\%, underscoring the advantage of a data-driven decision function. The full model integrating all components achieves the best performance of \textbf{0.778}, confirming their synergistic effect.

\subsubsection{Analysis of the Structured Multi-view Prediction Error (SMPE)}
We analyze the contribution of each error view within the SMPE by ablating them individually from the full model. Results are presented in Table~\ref{tab:ablation_smpe}. Removing the \textbf{Predictive Smoothness Error} causes the most significant drop (-1.7\% in Avg F1), highlighting its sensitivity to event transitions. Omitting the \textbf{Memory Perturbation Error} leads to a -1.2\% decrease, demonstrating the importance of tracking high-level event schema changes. Removing the primary \textbf{Feature Reconstruction Error} also degrades performance (-1.0\%). Using any single error view consistently underperforms compared to the full SMPE vector, validating our composite design.

\begin{table}[htbp]
\centering
\caption{Ablation on the components of the Structured Multi-view Prediction Error (SMPE). Evaluated on Kinetics-GEBD validation set (Average F1).}
\label{tab:ablation_smpe}
\begin{tabular}{lc}
\toprule
\textbf{Error Components Used} & \textbf{Avg F1} \\
\midrule
\textbf{Full SMPE} ($E^{(r)}, E^{(s)}, E^{(m)}$) & \textbf{0.778} \\
\hline
$E^{(s)}, E^{(m)}$ (w/o $E^{(r)}$) & 0.768 \\
$E^{(r)}, E^{(m)}$ (w/o $E^{(s)}$) & 0.761 \\
$E^{(r)}, E^{(s)}$ (w/o $E^{(m)}$) & 0.766 \\
\hline
Only $E^{(r)}$ & 0.756 \\
Only $E^{(s)}$ & 0.749 \\
Only $E^{(m)}$ & 0.738 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Design Choices for the Adaptive Discriminator (AMTD)}
We analyze key design choices in the AMTD module. Results are shown in Table~\ref{tab:ablation_amtd}. Replacing the AMTD with the original heuristic Online Boundary Detector (OBD) causes a performance drop to 0.748. Using a \textbf{single-scale} Temporal Convolutional Network (TCN) instead of the multi-scale architecture reduces Avg F1 to 0.771. Removing the \textbf{self-attention based dynamic fusion} and simply concatenating multi-scale features results in a score of 0.774. These ablations confirm that each aspect of the AMTD's design—its learned nature, multi-scale perception, and adaptive fusion—contributes to its effectiveness.

\begin{table}[htbp]
\centering
\caption{Ablation on the design of the Adaptive Multi-scale Temporal Discriminator (AMTD). Evaluated on Kinetics-GEBD validation set (Average F1).}
\label{tab:ablation_amtd}
\begin{tabular}{lc}
\toprule
\textbf{AMTD Variant} & \textbf{Avg F1} \\
\midrule
\textbf{Full AMTD (Ours)} & \textbf{0.778} \\
\hline
Replace with ESTimator's OBD & 0.748 \\
Single-scale TCN (kernel=7) & 0.771 \\
Multi-scale TCN w/o Self-Attention Fusion & 0.774 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Utility of the Structural Consistency Loss (SCL)}
We dissect the contribution of the two regularization terms in the SCL. As shown in Table~\ref{tab:ablation_loss}, training with only the \textbf{Focal Reconstruction Loss} yields a baseline score of 0.754. Adding the \textbf{Intra-event Smoothness Loss} improves performance to 0.766. Further incorporating the \textbf{Intra-event Clustering Loss} to form the full SCL pushes the score to 0.769. The full SCL, combining both regularizers, delivers the best result, confirming that jointly optimizing for local accuracy and global temporal coherence is an effective strategy.

\begin{table}[htbp]
\centering
\caption{Ablation on the components of the Structural Consistency Loss (SCL). The HMT and SMPE are used in all variants. Evaluated on Kinetics-GEBD validation set (Average F1).}
\label{tab:ablation_loss}
\begin{tabular}{lc}
\toprule
\textbf{Training Loss Variant} & \textbf{Avg F1} \\
\midrule
$\mathcal{L}_{Focal}$ only & 0.754 \\
$\mathcal{L}_{Focal} + \mathcal{L}_{Smooth}$ & 0.766 \\
$\mathcal{L}_{Focal} + \mathcal{L}_{Cluster}$ & 0.762 \\
\textbf{Full SCL} ($\mathcal{L}_{Focal} + \mathcal{L}_{Smooth} + \mathcal{L}_{Cluster}$) & \textbf{0.769} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Summary.} Our ablation studies provide compelling evidence for the necessity and effectiveness of every component in \textbf{ESTimator++}. The hierarchical memory (HMT) establishes a robust predictive foundation, the structured error (SMPE) extracts optimal boundary cues, the consistency loss (SCL) shapes the learning objective, and the adaptive discriminator (AMTD) makes precise online decisions. The full model's superior performance arises from their seamless integration.