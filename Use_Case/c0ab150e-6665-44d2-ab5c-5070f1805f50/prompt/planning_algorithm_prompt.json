{
  "stage": "planning_algorithm",
  "timestamp": "2026-01-17T06:35:04.309624+00:00",
  "figure_type": "algorithm",
  "variants": [
    {
      "request": {
        "messages": [
          {
            "role": "system",
            "content": "\n你是一位专业的学术图表设计专家。你的任务是将论文内容描述转换为适合 AI 图像生成的详细提示词。\n\n你的职责：\n1. 分析输入的论文内容描述\n2. 根据所需的图表类型（算法图或动机图）调整描述风格\n3. 生成详细且专业的图像生成提示词，确保生成的图像符合顶级学术会议（NeurIPS、ICML、ICLR、CVPR、AAAI 等）的发表标准\n\n图表类型规范：\n- **算法图**：用于展示算法流程、模型架构、方法框架等。必须强调：\n  * **抽象性和概念性**：不要绘制成具体的流程图，而应使用抽象概念图、架构图或框架图\n  * 清晰的结构和模块关系（用抽象模块表示，而非具体步骤）\n  * 逻辑流和数据流（用抽象箭头和连接表示，而非流程图符号）\n  * 技术性和专业性\n  * 简洁的视觉设计，避免过于具体的操作步骤\n  * 强调高层次概念和架构，而非详细的执行流程\n  \n- **动机图**：用于展示问题动机、方法对比、概念解释等。必须强调：\n  * 直观的可视化效果\n  * 突出的对比和差异\n  * 易于理解和可视化\n  * 概念的可理解性\n\n提示词生成要求：\n1. 使用英文生成提示词（AI 图像生成模型对英文的理解更好）\n2. 描述应详细且具体，包括视觉元素、布局、样式等\n3. 强调学术论文的专业性和简洁性\n4. 避免过于复杂的细节，保持图表清晰可读且相对饱满，必要时再复杂你自己判断\n5. **颜色和样式要求**（根据图表类型区分）：\n   \n   **对于算法图**：\n   * **整体风格**：采用简洁、极简主义的线条图风格，使用清晰的线条和抽象图形元素\n   * **颜色方案**：使用橙色（orange）和绿色（green）箭头来区分不同的流程阶段和循环路径，文本主要使用黑色、蓝色和红色作为主色调\n   * **视觉层次**：通过颜色区分信息流（如橙色表示数据输入/处理阶段，绿色表示反馈/输出阶段），形成清晰的视觉层次\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   * **箭头和连接**：使用不同颜色的箭头（橙色、绿色）来明确表示信息流向和循环关系\n   其他你自行判断\n   \n   **对于动机图**：\n   * **整体风格**：采用简洁、直观的图风格，使用清晰的视觉元素突出对比和差异\n   * **颜色方案**：使用对比鲜明的颜色来区分不同方法或概念\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   其他你自行判断\n   \n6. 确保图表适合在论文中展示\n7. **对于算法图**：必须强调抽象性和概念性，明确说明\"不是流程图\"和\"抽象概念图\"，避免使用流程图符号（如决策菱形、处理框等），应使用抽象模块和概念连接\n\n输出格式：\n- 仅输出优化后的提示词文本，不要包含任何解释或额外内容\n- 不要用代码块包裹\n- 提示词应是一个完整的描述，可以直接用于图像生成\n"
          },
          {
            "role": "user",
            "content": "Please convert the following paper content description into a detailed prompt suitable for AI image generation.\n\n[Required Figure Type]\nalgorithm\n\n[Paper Content Description]\n\\documentclass{article}\n\\usepackage{amsmath,amssymb}\n\\usepackage{booktabs}\n\n\\begin{document}\n\n\\title{ESTimator++: Structural Consistency Learning for Online Generic Event Boundary Detection}\n\\maketitle\n\\begin{abstract}\nOnline Generic Event Boundary Detection (On-GEBD) is essential for real-time video understanding, yet remains challenging due to the requirement for efficient, single-pass processing. While Event Segmentation Theory (EST) provides a cognitive foundation, its computational implementation in prior online methods such as ESTimator is limited. This paper presents \\textbf{ESTimator++}, a significantly enhanced framework that advances the original through a more systematic instantiation of EST. Our key contributions are threefold: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation; (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer boundary signals; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} which replaces heuristic thresholding with a learned decision function. These components are integrated via a novel \\textbf{Structural Consistency Loss (SCL)}, enabling end-to-end optimization. Extensive experiments on Kinetics-GEBD and TAPOS benchmarks demonstrate that ESTimator++ achieves state-of-the-art performance for On-GEBD, with average F1 scores of \\textbf{0.778} and \\textbf{0.569}, respectively, outperforming all existing online methods and competing favorably with many offline models. Ablation studies confirm the contribution of each component, with the AMTD providing the most significant single-module gain (+2.9\\%). Remarkably, this is achieved while maintaining high efficiency at \\textbf{8.2 ms per frame}. Our work demonstrates that a principled, integrated design substantially advances the performance of online event segmentation.\n\\end{abstract}\n\n\\section{Introduction}\nThe proliferation of long-form video content on platforms like YouTube and Netflix has created a pressing need for advanced video understanding systems. While tasks such as action recognition are well-established, they predominantly focus on short, trimmed clips with predefined labels. In contrast, the analysis of untrimmed, long-form videos containing complex narratives remains a formidable challenge. Generic Event Boundary Detection (GEBD) tackles this issue by aiming to segment videos into taxonomy-free, semantically coherent units, thereby emulating the human ability to parse continuous visual streams.\n\nCurrent GEBD frameworks operate in an \\textit{offline} manner. They process an entire video segment in one pass, leveraging both past and future context to identify all boundaries simultaneously. This paradigm stands in stark contrast to human perception, which segments events \\textit{online}—sequentially and causally, relying solely on past and present information without any access to future content. To bridge this fundamental gap and more faithfully replicate human cognitive processes, we introduce a novel and more demanding task: \\textbf{Online Generic Event Boundary Detection (On-GEBD)}. In this setting, a model must process a video stream and make immediate, frame-by-frame decisions regarding boundary occurrences, using only historical and current data.\n\nOn-GEBD inherits the inherent challenges of offline GEBD, such as detecting subtle and taxonomy-free transitions, but introduces additional complexities due to its strictly causal and information-limited nature. Existing offline GEBD methods, which often depend on global temporal context through mechanisms like similarity matrices or multi-frame comparisons, are fundamentally incompatible with the online setting. Likewise, naïvely adapting models designed for other online action understanding tasks proves ineffective, as they are not tailored for the fine-grained detection of generic boundaries.\n\nTo address these unique challenges, we propose \\textbf{ESTimator++}, an enhanced online framework inspired by Event Segmentation Theory (EST) from cognitive science. EST suggests that humans continuously predict upcoming visual input based on an ongoing event model and perceive a boundary when a significant prediction error arises. Our framework operationalizes this principle through three core innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer} that maintains a stable representation of the current event for robust future anticipation; (2) a \\textbf{Structured Multi-view Prediction Error} mechanism that derives a richer and more stable discrepancy signal; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator}, a lightweight network that replaces heuristic thresholding with a learned, context-aware decision function. These components are integrated under a novel \\textbf{Structural Consistency Loss} that jointly enforces prediction accuracy and temporal coherence.\n\nExtensive experiments on the Kinetics-GEBD and TAPOS benchmarks demonstrate that \\textbf{ESTimator++} sets a new state-of-the-art for On-GEBD. It significantly surpasses all adapted online baselines and, remarkably, achieves performance competitive with many sophisticated offline methods, despite operating under a strict causal constraint. Ablation studies validate the contribution of each proposed component. The remainder of this paper is organized as follows: we review related work, detail our methodology, present comprehensive experimental results, and provide concluding remarks.\n\n\\section{Related Work}\n\\subsection{Generic Event Boundary Detection}\nGeneric Event Boundary Detection (GEBD) aims to identify boundaries that align with human perceptual segmentation within untrimmed videos, operating without a predefined taxonomy of events. This task differs from traditional video understanding tasks such as Temporal Action Localization (TAL) and Action Recognition, as GEBD handles continuous, open-ended semantics. Recent advancements in offline GEBD typically process entire videos or extensive temporal contexts. Representative methods include UBoCo, which leverages temporal similarity matrices with contrastive learning; DDM-Net, which applies progressive attention on multi-level dense difference maps; and CoSeg, which introduces a reconstruction-based objective inspired by cognitive modeling. While these offline methods have shown progress, they are not designed for streaming video. Our work adapts GEBD to the \\textit{online} setting, requiring instantaneous decisions without access to future frames.\n\n\\subsection{Video Understanding}\nVideo understanding encompasses tasks such as Temporal Action Detection (TAD), Temporal Action Localization (TAL), and Video Instance Segmentation (VIS). State-of-the-art approaches often employ transformer architectures, memory mechanisms, and end-to-end frameworks. A common limitation is their reliance on a closed set of predefined action classes, which restricts applicability in open-world scenarios. Although open-set formulations for TAL exist, they focus on localizing and classifying known actions. In contrast, GEBD detects boundaries for taxonomy-free events, providing a more general approach to video structure understanding.\n\n\\subsection{Online Video Understanding}\nOnline video understanding methods process data in a streaming fashion, sharing core challenges with Online GEBD (On-GEBD). Relevant tasks include Online Action Detection (OAD), Online Temporal Action Localization (On-TAL), and Online Detection of Action Start (ODAS). For OAD, recent works such as LSTR, TeSTra, and MiniROAD demonstrate the efficacy of transformers and recurrent units. For On-TAL, methods like CAG-QIL and OAT have been proposed. While ODAS shares an online nature with On-GEBD, it is designed for a fixed set of action categories. Conversely, On-GEBD must detect subtle, categorical changes between taxonomy-free events, necessitating a distinct methodological approach from conventional online action-centric models.\n\n\\section{Method}\nOur enhanced framework, \\textbf{ESTimator++}, extends the original ESTimator by providing a more systematic computational implementation of Event Segmentation Theory (EST). While retaining the core principle of using prediction discrepancy for online boundary detection, ESTimator++ introduces three key innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation, (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer and more stable boundary cues, and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} that replaces heuristic thresholding with a learned, context-sensitive decision function. These components are unified during training via a novel \\textbf{Structural Consistency Loss (SCL)}, which enforces both local prediction accuracy and global temporal coherence.\n\n\\subsection{Hierarchical Memory-augmented Transformer for Consistent Event Anticipation (HMT-CEA)}\nThe original Consistent Event Anticipation (CEA) employs a standard transformer decoder with a limited look-back window, which may fail to maintain a persistent representation of an ongoing event's gist. Inspired by human cognition—which uses working memory for immediate context and long-term memory for schemas—we propose the HMT-CEA. It maintains two complementary memory structures: a dynamic \\textbf{Main Event Memory ($\\mathbf{M}$)} that captures the abstract, persistent essence of the current event, and a \\textbf{Local Context Cache ($\\mathbf{C}$)} that holds fine-grained recent features.\n\n\\subsubsection{Architecture and State Update}\nGiven a stream of frame features $\\mathbf{f}_t \\in \\mathbb{R}^D$, we first project them into a memory-compatible space: $\\mathbf{h}_t = \\text{Linear}(\\mathbf{f}_t)$. The HMT-CEA operates recurrently, maintaining states $(\\mathbf{M}_t, \\mathbf{C}_t)$.\n\n\\begin{enumerate}\n    \\item \\textbf{Local Context Update}: The cache $\\mathbf{C}_t$ is a FIFO queue of the last $L$ projected features: $\\mathbf{C}_t = [\\mathbf{h}_{t-L}, ..., \\mathbf{h}_{t-1}]$.\n\n    \\item \\textbf{Main Memory Gated Update}: The core memory $\\mathbf{M}_t \\in \\mathbb{R}^{d_m}$ is updated via a gated mechanism using the new observation $\\mathbf{h}_t$ and previous context.\n    \\[\n    \\mathbf{g}_t = \\sigma(\\mathbf{W}_g [\\mathbf{M}_{t-1}; \\bar{\\mathbf{C}}_t; \\mathbf{h}_t] + \\mathbf{b}_g)\n    \\]\n    \\[\n    \\tilde{\\mathbf{M}}_t = \\text{TransformerBlock}(\\mathbf{M}_{t-1}, \\mathbf{C}_t, \\mathbf{h}_t)\n    \\]\n    \\[\n    \\mathbf{M}_t = \\mathbf{g}_t \\odot \\tilde{\\mathbf{M}}_t + (1 - \\mathbf{g}_t) \\odot \\mathbf{M}_{t-1}\n    \\]\n    where $\\bar{\\mathbf{C}}_t$ is the mean-pooled cache, $\\sigma$ is the sigmoid function, $\\odot$ denotes element-wise multiplication, and $\\text{TransformerBlock}$ is a lightweight cross-attention module that allows the memory to attend to the local context and new input.\n\n    \\item \\textbf{Next-Frame Prediction}: The anticipated feature for frame $t$ is generated by fusing information from both memory levels:\n    \\[\n    \\mathbf{z}_t = \\text{Decoder}( \\mathbf{M}_{t-1}, \\mathbf{C}_{t-1} )\n    \\]\n    \\[\n    \\hat{\\mathbf{f}}_t = \\text{Linear}(\\mathbf{z}_t)\n    \\]\n    Here, $\\text{Decoder}$ is a small autoregressive transformer that uses $\\mathbf{M}_{t-1}$ as a persistent query and $\\mathbf{C}_{t-1}$ as keys and values, ensuring causality.\n\\end{enumerate}\n\nThis architecture ensures predictions are grounded in both the immediate past ($\\mathbf{C}$) and a stable, evolving representation of the current event ($\\mathbf{M}$).\n\n\\subsection{Structured Multi-view Prediction Error (SMPE)}\nInstead of a single scalar error, we define a multi-dimensional error vector $\\mathbf{E}_t \\in \\mathbb{R}^3$ that captures different facets of prediction inconsistency, yielding a more robust boundary detection signal.\n\n\\begin{enumerate}\n    \\item \\textbf{Feature Reconstruction Error ($E^{(r)}_t$)}: The primary discrepancy, analogous to the original $\\epsilon_t$, but computed in the projected space for stability.\n    \\[\n    E^{(r)}_t = 1 - \\frac{\\mathbf{h}_t \\cdot \\mathbf{z}_t}{\\|\\mathbf{h}_t\\| \\|\\mathbf{z}_t\\|}\n    \\]\n\n    \\item \\textbf{Predictive Smoothness Error ($E^{(s)}_t$)}: Measures inconsistency between consecutive predictions. Within a consistent event, predictions should vary smoothly.\n    \\[\n    E^{(s)}_t = \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|_2 - \\beta \\cdot \\|\\mathbf{h}_t - \\mathbf{h}_{t-1}\\|_2\n    \\]\n    where $\\beta$ is a scaling factor. This term becomes large only when predictions are jagged despite smooth actual changes.\n\n    \\item \\textbf{Memory Perturbation Error ($E^{(m)}_t$)}: Quantifies the impact of the new frame on the main event memory, reflecting a potential high-level schema violation.\n    \\[\n    E^{(m)}_t = \\|\\mathbf{M}_t - \\mathbf{M}_{t-1}\\|_1\n    \\]\n\\end{enumerate}\n\nThe final SMPE vector is the concatenation: $\\mathbf{E}_t = [E^{(r)}_t, E^{(s)}_t, E^{(m)}_t]^\\top$. This structured error is more informative and less noisy than a scalar counterpart.\n\n\\subsection{Training with Structural Consistency Loss (SCL)}\nThe HMT-CEA is trained by minimizing a novel \\textbf{Structural Consistency Loss (SCL)}, which jointly optimizes prediction accuracy and the temporal structural properties implied by EST.\n\n\\subsubsection{Boundary-Aware Focal Reconstruction Loss}\nWe adapt binary cross-entropy to address class imbalance and hard examples using the primary error $E^{(r)}_t$:\n\\[\n\\mathcal{L}_{Focal}(E^{(r)}_t, y_t) = - \\alpha (1 - p_t)^\\gamma y_t \\log(p_t) - (1 - \\alpha) p_t^\\gamma (1 - y_t) \\log(1 - p_t)\n\\]\nwhere $p_t = \\text{sigmoid}(-\\lambda \\cdot E^{(r)}_t + b)$, $\\lambda$ and $b$ are learnable parameters, $y_t$ is the boundary label, $\\alpha$ balances class frequency, and $\\gamma$ focuses on hard examples.\n\n\\subsubsection{Temporal Coherence Regularization}\nThis unsupervised loss enforces two structural properties:\n\\begin{itemize}\n    \\item \\textbf{Intra-event Prediction Smoothness}: Within non-boundary segments ($y_t=0$), predictions should be smooth:\n    \\[\n    \\mathcal{L}_{Smooth} = \\frac{1}{|\\mathcal{N}|} \\sum_{t \\in \\mathcal{N}} \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|^2_2\n    \\]\n    where $\\mathcal{N}$ is the set of non-boundary frames.\n\n    \\item \\textbf{Intra-event Feature Clustering}: All predictions within a ground-truth event segment should cluster in embedding space:\n    \\[\n    \\mathcal{L}_{Cluster} = \\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{|I_s|} \\sum_{i \\in I_s} \\|\\mathbf{z}_i - \\boldsymbol{\\mu}_s\\|^2_2\n    \\]\n    where $S$ is the number of events in a clip, $I_s$ is the set of frame indices in event $s$, and $\\boldsymbol{\\mu}_s$ is the mean of $\\mathbf{z}_i$ for $i \\in I_s$.\n\\end{itemize}\n\nThe total SCL is: $\\mathcal{L}_{SCL} = \\mathcal{L}_{Focal} + \\lambda_s \\mathcal{L}_{Smooth} + \\lambda_c \\mathcal{L}_{Cluster}$.\n\n\\subsection{Adaptive Multi-scale Temporal Discriminator (AMTD)}\nThe AMTD replaces the heuristic online boundary detector (OBD) with a lightweight temporal convolutional network (TCN) that learns to detect boundaries from the sequence of SMPE vectors $\\{\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t\\}$.\n\n\\subsubsection{Architecture}\nThe AMTD, denoted $\\mathcal{D}_\\phi$, comprises:\n\\begin{enumerate}\n    \\item \\textbf{Multi-scale Feature Extraction}: Parallel 1D convolutions with kernel sizes $k_1, k_2, k_3$ capture short-, medium-, and long-range temporal patterns in $\\mathbf{E}_t$:\n    \\[\n    \\mathbf{s}^{(i)}_t = \\text{ReLU}(\\text{Conv1D}_{k_i}([\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t]))\n    \\]\n\n    \\item \\textbf{Dynamic Fusion}: Multi-scale features are concatenated and weighted adaptively via self-attention:\n    \\[\n    \\mathbf{S}_t = [\\mathbf{s}^{(1)}_t, \\mathbf{s}^{(2)}_t, \\mathbf{s}^{(3)}_t], \\quad \\mathbf{A}_t = \\text{SelfAttention}(\\mathbf{S}_t)\n    \\]\n\n    \\item \\textbf{Boundary Probability Estimation}: The attended feature produces the final online boundary probability:\n    \\[\n    P_t^{boundary} = \\sigma(\\text{MLP}(\\text{Pool}(\\mathbf{A}_t)))\n    \\]\n    A boundary is declared if $P_t^{boundary} > 0.5$.\n\\end{enumerate}\n\n\\subsubsection{Online Operation and Training}\nDuring online inference, AMTD slides over the streaming SMPE vectors using a FIFO buffer of size $\\Delta+1$. It is trained jointly with the HMT-CEA via SCL, where binary labels $y_t$ supervise $P_t^{boundary}$ through a standard cross-entropy loss, forming a fully differentiable, end-to-end optimizable system for online granular event boundary detection (On-GEBD).\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Setup}\n\\label{sec:setup}\n\n\\subsubsection{Benchmark Datasets.}\nWe evaluate our method on two established benchmarks for generic event boundary detection (GEBD). The \\textbf{Kinetics-GEBD} dataset is derived from the Kinetics-400 dataset, comprising approximately 60K videos. Each video is annotated with taxonomy-free event boundaries, containing about five events on average. The dataset is officially split into training (18,794 videos), validation (18,813 videos), and test (17,725 videos) sets. As test set annotations are withheld, we report results on the validation set, consistent with prior work. For cross-validation in certain analyses, we further split the training data into 80\\% for training and 20\\% for validation. The \\textbf{TAPOS} dataset consists of Olympic sports videos originally annotated for action localization. Following the standard adaptation for GEBD, we repurpose it by discarding action labels and treating sub-action boundaries as generic event boundaries. This results in 13,094 training and 1,790 validation action instances.\n\n\\subsubsection{Evaluation Metric.}\nWe adopt the standard Relative Distance (Rel.Dis) metric for GEBD. For a predicted boundary timestamp and its corresponding ground truth, the relative distance is computed as their temporal offset divided by the duration of the union of the predicted and ground-truth event instances. Following common practice, when multiple consecutive frames are predicted as boundaries, the central frame is taken as the final timestamp. Performance is evaluated under ten Rel.Dis thresholds from 0.05 to 0.5 with a step size of 0.05. A prediction is considered correct if its Rel.Dis is below a given threshold. We report the F1 score at each threshold and the average F1 score across all thresholds.\n\n\\subsubsection{Baselines.}\nEstablishing baselines for Online GEBD (On-GEBD) is non-trivial as standard offline GEBD methods are not designed for streaming input. Therefore, we adapt state-of-the-art models from related online video understanding tasks. From Online Action Detection, we adapt TeSTra, OadTR, and MiniROAD. From Online Temporal Action Localization, we adapt Sim-On. For a fair comparison, we modify only the final layer of each model to perform binary classification (boundary vs. non-boundary), denoted with the suffix '-BC'. We also compare against a dedicated online GEBD method, \\textbf{ESTimator}. Furthermore, to contextualize our online performance, we include comparisons with leading offline GEBD methods.\n\n\\subsubsection{Implementation Details.}\nOur proposed method, \\textbf{ESTimator++}, is implemented using standard video features for fair comparison. Videos are sampled at 24 FPS for Kinetics-GEBD and 6 FPS for TAPOS. We use 2048-dimensional features extracted by an ImageNet-pretrained ResNet-50 encoder. The Hierarchical Memory-augmented Transformer (HMT) employs a main memory with dimension $d_m=512$ and a local context cache of size $L=16$. The Adaptive Multi-scale Temporal Discriminator (AMTD) uses temporal convolutional kernels of sizes $[3, 7, 15]$. The model is trained end-to-end for 30 epochs using the AdamW optimizer with a learning rate of $1\\times10^{-4}$, a batch size of 512, and loss weights $\\lambda_s=0.1$ and $\\lambda_c=0.05$.\n\n\\subsection{Main Results}\n\n\\subsubsection{Comparison with Online Baselines}\nTable~\\ref{tab:online_k400} and Table~\\ref{tab:online_tapos} present the online detection performance on the Kinetics-GEBD and TAPOS validation sets, respectively.\n\nOn Kinetics-GEBD (Table~\\ref{tab:online_k400}), \\textbf{ESTimator++} sets a new state-of-the-art, outperforming all online baselines across all Rel.Dis thresholds. Traditional online models like TeSTra-BC and Sim-On-BC show limited effectiveness for this task. While MiniROAD-BC and the prior dedicated method ESTimator perform better, our method achieves a superior average F1 score of \\textbf{0.778}. This represents an improvement of +3.0\\% over ESTimator and +9.7\\% over MiniROAD-BC, demonstrating the efficacy of our proposed architectural components.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the Kinetics-GEBD validation set. 'BC' denotes a binary classifier head. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_k400}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.438 & 0.488 & 0.521 & 0.545 & 0.564 & 0.580 & 0.593 & 0.604 & 0.614 & 0.622 & 0.557 \\\\\nSim-On-BC & 0.461 & 0.534 & 0.579 & 0.610 & 0.633 & 0.651 & 0.664 & 0.675 & 0.685 & 0.692 & 0.618 \\\\\nOadTR-BC & 0.474 & 0.512 & 0.535 & 0.552 & 0.565 & 0.575 & 0.583 & 0.590 & 0.596 & 0.601 & 0.558 \\\\\nMiniROAD-BC & 0.569 & 0.622 & 0.649 & 0.675 & 0.691 & 0.704 & 0.714 & 0.722 & 0.729 & 0.735 & 0.681 \\\\\nESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.642} & \\textbf{0.712} & \\textbf{0.751} & \\textbf{0.775} & \\textbf{0.790} & \\textbf{0.802} & \\textbf{0.810} & \\textbf{0.816} & \\textbf{0.821} & \\textbf{0.825} & \\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nA similar trend is observed on TAPOS (Table~\\ref{tab:online_tapos}). \\textbf{ESTimator++} achieves the best average F1 score of \\textbf{0.569}, outperforming ESTimator by +2.2\\% and MiniROAD-BC by +4.1\\%. The advantage is particularly notable at stricter thresholds (e.g., 0.05, 0.1), indicating more precise boundary localization. The consistent superiority across both datasets underscores the robustness and generalizability of our approach.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the TAPOS validation set. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_tapos}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.364 & 0.417 & 0.452 & 0.478 & 0.496 & 0.511 & 0.523 & 0.533 & 0.542 & 0.550 & 0.487 \\\\\nSim-On-BC & 0.225 & 0.269 & 0.303 & 0.329 & 0.350 & 0.367 & 0.381 & 0.394 & 0.405 & 0.415 & 0.344 \\\\\nOadTR-BC & 0.263 & 0.319 & 0.361 & 0.394 & 0.422 & 0.445 & 0.465 & 0.483 & 0.497 & 0.510 & 0.416 \\\\\nMiniROAD-BC & 0.422 & 0.472 & 0.502 & 0.522 & 0.537 & 0.549 & 0.558 & 0.566 & 0.572 & 0.578 & 0.528 \\\\\nESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.418} & \\textbf{0.484} & \\textbf{0.531} & \\textbf{0.566} & \\textbf{0.592} & \\textbf{0.613} & \\textbf{0.629} & \\textbf{0.642} & \\textbf{0.653} & \\textbf{0.663} & \\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Comparison with Offline Methods}\nWe further compare our online method against powerful offline models that utilize the full video context. Results are shown in Table~\\ref{tab:offline_k400} for Kinetics-GEBD and Table~\\ref{tab:offline_tapos} for TAPOS.\n\nRemarkably, on Kinetics-GEBD, \\textbf{ESTimator++} with an average F1 of \\textbf{0.778} not only leads all online methods but also competes favorably with state-of-the-art offline approaches. It surpasses all offline supervised methods except for PC and the unsupervised CoSeg. It is important to note that PC and CoSeg benefit from full-video processing and, in CoSeg's case, cross-video co-segmentation. The fact that our \\textit{online} method achieves performance comparable to sophisticated offline models like TCN highlights the efficacy of its design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on Kinetics-GEBD. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_k400}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & BMN & 0.186 & 0.204 & 0.213 & 0.220 & 0.226 & 0.230 & 0.233 & 0.237 & 0.239 & 0.241 & 0.223 \\\\\nOffline & Supervised & BMN-StartEnd & 0.491 & 0.589 & 0.627 & 0.648 & 0.660 & 0.668 & 0.674 & 0.678 & 0.681 & 0.683 & 0.640 \\\\\nOffline & Supervised & TCN-TAPOS & 0.464 & 0.560 & 0.602 & 0.628 & 0.645 & 0.659 & 0.669 & 0.676 & 0.682 & 0.687 & 0.627 \\\\\nOffline & Supervised & TCN & 0.588 & 0.657 & 0.679 & 0.691 & 0.698 & 0.703 & 0.706 & 0.708 & 0.710 & 0.712 & 0.685 \\\\\nOffline & Supervised & \\textbf{PC} & 0.625 & \\textbf{0.758} & \\textbf{0.804} & \\textbf{0.829} & \\textbf{0.844} & \\textbf{0.853} & \\textbf{0.859} & \\textbf{0.864} & \\textbf{0.867} & \\textbf{0.870} & \\textbf{0.817} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.275 & 0.300 & 0.312 & 0.319 & 0.324 & 0.327 & 0.330 & 0.332 & 0.334 & 0.335 & 0.318 \\\\\nOffline & Unsupervised & PA-Random & 0.336 & 0.435 & 0.484 & 0.512 & 0.529 & 0.541 & 0.548 & 0.554 & 0.558 & 0.561 & 0.506 \\\\\nOffline & Unsupervised & PA & 0.396 & 0.488 & 0.520 & 0.534 & 0.544 & 0.550 & 0.555 & 0.558 & 0.561 & 0.564 & 0.527 \\\\\nOffline & Unsupervised & \\underline{CoSeg} & \\textbf{0.656} & 0.758 & 0.783 & 0.794 & 0.799 & 0.803 & 0.804 & 0.806 & 0.807 & 0.809 & 0.782 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.712} & $\\dagger$\\textbf{0.751} & $\\dagger$\\textbf{0.775} & $\\dagger$\\textbf{0.790} & $\\dagger$\\textbf{0.802} & $\\dagger$\\textbf{0.810} & $\\dagger$\\textbf{0.816} & $\\dagger$\\textbf{0.821} & $\\dagger$\\textbf{0.825} & $\\dagger$\\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\nOn the TAPOS dataset (Table~\\ref{tab:offline_tapos}), \\textbf{ESTimator++} again demonstrates strong competitiveness, achieving an average F1 of \\textbf{0.569}. It outperforms all offline supervised baselines except PC, and shows superior performance compared to the unsupervised PA method, especially at stricter thresholds. This consistent ability to rival many offline models underscores a key advantage: \\textbf{ESTimator++} delivers high-quality boundary detection with minimal latency, making it suitable for real-time applications.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on TAPOS. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_tapos}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & ISBA & 0.106 & 0.170 & 0.227 & 0.265 & 0.298 & 0.326 & 0.348 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & TCN & 0.237 & 0.312 & 0.331 & 0.339 & 0.342 & 0.344 & 0.347 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & CTM & 0.244 & 0.312 & 0.336 & 0.351 & 0.361 & 0.369 & 0.374 & 0.381 & 0.383 & 0.385 & 0.350 \\\\\nOffline & Supervised & TransParser & 0.289 & 0.381 & 0.435 & 0.475 & 0.500 & 0.514 & 0.527 & 0.534 & 0.540 & 0.545 & 0.474 \\\\\nOffline & Supervised & \\textbf{PC} & \\textbf{0.522} & \\textbf{0.595} & \\textbf{0.628} & \\textbf{0.646} & \\textbf{0.659} & \\textbf{0.665} & \\textbf{0.671} & \\textbf{0.676} & \\textbf{0.679} & \\textbf{0.683} & \\textbf{0.642} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.035 & 0.045 & 0.047 & 0.051 & 0.053 & 0.054 & 0.055 & 0.056 & 0.057 & 0.058 & 0.051 \\\\\nOffline & Unsupervised & PA-Random & 0.158 & 0.233 & 0.273 & 0.310 & 0.331 & 0.347 & 0.357 & 0.369 & 0.376 & 0.384 & 0.314 \\\\\nOffline & Unsupervised & \\underline{PA} & 0.360 & 0.459 & 0.507 & 0.543 & 0.567 & 0.579 & 0.592 & 0.601 & 0.609 & 0.615 & 0.543 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.418} & $\\dagger$\\textbf{0.484} & $\\dagger$\\textbf{0.531} & $\\dagger$\\textbf{0.566} & $\\dagger$\\textbf{0.592} & $\\dagger$\\textbf{0.613} & $\\dagger$\\textbf{0.629} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.653} & $\\dagger$\\textbf{0.663} & $\\dagger$\\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\n\\subsection{Model Efficiency}\nWhile \\textbf{ESTimator++} introduces more sophisticated components, its design prioritizes online efficiency. We profile the average inference time per frame on a single NVIDIA V100 GPU. As shown in Table~\\ref{tab:efficiency}, \\textbf{ESTimator++} processes frames at \\textbf{8.2 ms (∼122 FPS)}, which is significantly faster than the real-time requirement of 24 FPS for the Kinetics dataset. Although it is understandably slower than the simpler ESTimator and some lightweight baselines like TeSTra-BC, its runtime is still an order of magnitude lower than the total latency of offline methods, which must process entire clips. This demonstrates that \\textbf{ESTimator++} successfully balances state-of-the-art accuracy with practical online inference speed.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison of inference efficiency (lower time per frame is better). Offline methods process full clips; their reported time is the total latency per clip.}\n\\label{tab:efficiency}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Method} & \\textbf{Time per Frame (ms)} \\\\\n\\midrule\n\\textbf{Online Methods} \\\\\nTeSTra-BC & 2.1 \\\\\nMiniROAD-BC & 5.7 \\\\\nESTimator & 6.5 \\\\\n\\textbf{ESTimator++ (Ours)} & \\textbf{8.2} \\\\\n\\midrule\n\\textbf{Offline Methods (Full Clip)} \\\\\nTCN & 1520* \\\\\nPC & 1850* \\\\\n\\bottomrule\n\\multicolumn{2}{l}{*Total processing time for a 10-second clip, not per frame.}\n\\end{tabular}\n\\end{table}\n\n\\subsection{Qualitative Analysis}\nQualitative comparisons reveal that predictions from \\textbf{ESTimator++} exhibit sharper and more confident peaks at genuine event boundaries while showing greater suppression of spurious fluctuations within event segments, compared to both the TeSTra-BC baseline and its predecessor ESTimator. This can be attributed to the Structured Multi-view Prediction Error (SMPE) providing a cleaner signal and the Adaptive Multi-scale Temporal Discriminator (AMTD) learning to distinguish true boundary patterns from noise. Furthermore, in sequences with gradual transitions, \\textbf{ESTimator++} shows improved temporal localization, likely due to the multi-scale receptive field of the AMTD. These observations align with its superior quantitative performance.\n\n\\section{Ablation Studies}\n\\subsection{Ablation Study}\n\\label{sec:ablation}\n\nWe conduct a comprehensive ablation study on the Kinetics-GEBD validation set to validate the contribution of each component in \\textbf{ESTimator++}. All experiments follow the implementation details in Section~\\ref{sec:setup}.\n\n\\subsubsection{Impact of Proposed Components}\nWe start from a baseline model that uses a standard Transformer decoder for prediction and a single reconstruction error with a fixed heuristic threshold for detection. This baseline achieves an average F1 score of 0.715. We then sequentially integrate our proposed components, with results summarized in Table~\\ref{tab:ablation_components}.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation study on the contribution of each proposed component. Evaluated on Kinetics-GEBD validation set (Average F1). $\\Delta$ denotes the improvement over the previous row.}\n\\label{tab:ablation_components}\n\\begin{tabular}{lcccccc}\n\\toprule\n\\textbf{Variant} & \\textbf{HMT} & \\textbf{SMPE} & \\textbf{SCL} & \\textbf{AMTD} & \\textbf{Avg F1} & $\\Delta$ \\\\\n\\midrule\nBaseline (ESTimator Core) & & & & & 0.715 & -- \\\\\n+ Hierarchical Memory (HMT) & \\checkmark & & & & 0.733 & +0.018 \\\\\n+ Structured Error (SMPE) & \\checkmark & \\checkmark & & & 0.754 & +0.021 \\\\\n+ Consistency Loss (SCL) & \\checkmark & \\checkmark & \\checkmark & & 0.769 & +0.015 \\\\\n+ Adaptive Discriminator (AMTD) & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\textbf{0.778} & \\textbf{+0.029} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nIntegrating the \\textbf{Hierarchical Memory-augmented Transformer (HMT)} improves performance by +1.8\\%, demonstrating the importance of maintaining stable event context for consistent prediction. Replacing the single error with the \\textbf{Structured Multi-view Prediction Error (SMPE)} yields a further gain of +2.1\\%, confirming that a composite error signal provides richer boundary cues. Employing the \\textbf{Structural Consistency Loss (SCL)} for training adds another +1.5\\%, validating that enforcing intra-event smoothness and feature clustering shapes more discriminative representations. Finally, substituting the heuristic threshold with the learned \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} provides the most substantial single-module improvement of +2.9\\%, underscoring the advantage of a data-driven decision function. The full model integrating all components achieves the best performance of \\textbf{0.778}, confirming their synergistic effect.\n\n\\subsubsection{Analysis of the Structured Multi-view Prediction Error (SMPE)}\nWe analyze the contribution of each error view within the SMPE by ablating them individually from the full model. Results are presented in Table~\\ref{tab:ablation_smpe}. Removing the \\textbf{Predictive Smoothness Error} causes the most significant drop (-1.7\\% in Avg F1), highlighting its sensitivity to event transitions. Omitting the \\textbf{Memory Perturbation Error} leads to a -1.2\\% decrease, demonstrating the importance of tracking high-level event schema changes. Removing the primary \\textbf{Feature Reconstruction Error} also degrades performance (-1.0\\%). Using any single error view consistently underperforms compared to the full SMPE vector, validating our composite design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structured Multi-view Prediction Error (SMPE). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_smpe}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Error Components Used} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full SMPE} ($E^{(r)}, E^{(s)}, E^{(m)}$) & \\textbf{0.778} \\\\\n\\hline\n$E^{(s)}, E^{(m)}$ (w/o $E^{(r)}$) & 0.768 \\\\\n$E^{(r)}, E^{(m)}$ (w/o $E^{(s)}$) & 0.761 \\\\\n$E^{(r)}, E^{(s)}$ (w/o $E^{(m)}$) & 0.766 \\\\\n\\hline\nOnly $E^{(r)}$ & 0.756 \\\\\nOnly $E^{(s)}$ & 0.749 \\\\\nOnly $E^{(m)}$ & 0.738 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Design Choices for the Adaptive Discriminator (AMTD)}\nWe analyze key design choices in the AMTD module. Results are shown in Table~\\ref{tab:ablation_amtd}. Replacing the AMTD with the original heuristic Online Boundary Detector (OBD) causes a performance drop to 0.748. Using a \\textbf{single-scale} Temporal Convolutional Network (TCN) instead of the multi-scale architecture reduces Avg F1 to 0.771. Removing the \\textbf{self-attention based dynamic fusion} and simply concatenating multi-scale features results in a score of 0.774. These ablations confirm that each aspect of the AMTD's design—its learned nature, multi-scale perception, and adaptive fusion—contributes to its effectiveness.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the design of the Adaptive Multi-scale Temporal Discriminator (AMTD). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_amtd}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{AMTD Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full AMTD (Ours)} & \\textbf{0.778} \\\\\n\\hline\nReplace with ESTimator's OBD & 0.748 \\\\\nSingle-scale TCN (kernel=7) & 0.771 \\\\\nMulti-scale TCN w/o Self-Attention Fusion & 0.774 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Utility of the Structural Consistency Loss (SCL)}\nWe dissect the contribution of the two regularization terms in the SCL. As shown in Table~\\ref{tab:ablation_loss}, training with only the \\textbf{Focal Reconstruction Loss} yields a baseline score of 0.754. Adding the \\textbf{Intra-event Smoothness Loss} improves performance to 0.766. Further incorporating the \\textbf{Intra-event Clustering Loss} to form the full SCL pushes the score to 0.769. The full SCL, combining both regularizers, delivers the best result, confirming that jointly optimizing for local accuracy and global temporal coherence is an effective strategy.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structural Consistency Loss (SCL). The HMT and SMPE are used in all variants. Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_loss}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Training Loss Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n$\\mathcal{L}_{Focal}$ only & 0.754 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth}$ & 0.766 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Cluster}$ & 0.762 \\\\\n\\textbf{Full SCL} ($\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth} + \\mathcal{L}_{Cluster}$) & \\textbf{0.769} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\textbf{Summary.} Our ablation studies provide compelling evidence for the necessity and effectiveness of every component in \\textbf{ESTimator++}. The hierarchical memory (HMT) establishes a robust predictive foundation, the structured error (SMPE) extracts optimal boundary cues, the consistency loss (SCL) shapes the learning objective, and the adaptive discriminator (AMTD) makes precise online decisions. The full model's superior performance arises from their seamless integration.\n\n\\section{Evaluation}\n\\subsection{Training Dynamics and Convergence Analysis}\nTo gain deeper insights into the optimization behavior of \\textbf{ESTimator++}, we analyze its training dynamics. The training and validation loss curves over 30 epochs on the Kinetics-GEBD dataset show that \\textbf{ESTimator++} converges more smoothly and stably to a lower final loss compared to its predecessor \\textbf{ESTimator} and the strong baseline \\textbf{MiniROAD-BC}. This indicates that the Structural Consistency Loss (SCL) provides a more stable gradient signal. Furthermore, the gap between training and validation loss for \\textbf{ESTimator++} remains consistently narrow, suggesting strong generalization and reduced overfitting, a benefit likely attributable to the regularizing effects of $\\mathcal{L}_{Smooth}$ and $\\mathcal{L}_{Cluster}$. In contrast, the baseline models exhibit more pronounced fluctuations and a larger generalization gap.\n\n\\subsection{In-depth Case Study}\nWe conduct a systematic case study to examine \\textbf{ESTimator++}'s performance across diverse scenarios. We categorize sequences from the Kinetics-GEBD validation set into groups such as \\textit{‘fast-paced action transitions’}, \\textit{‘gradual scene changes’}, and \\textit{‘static periods with subtle motions’}. For each category, we compare the SMPE signal and final boundary predictions from \\textbf{ESTimator++} against those from \\textbf{ESTimator} and \\textbf{TeSTra-BC}. In fast-paced sequences, \\textbf{ESTimator++}'s AMTD demonstrates superior ability to isolate closely spaced peaks, leading to precise detection of sequential boundaries. For gradual transitions, the multi-view SMPE provides a rising error signal over several frames, which the multi-scale AMTD effectively integrates to produce a confident prediction. During static periods, \\textbf{ESTimator++} exhibits significantly lower spurious activation due to the intra-event smoothness constraint. These observations concretely explain the quantitative improvements shown in the main results.\n\n\\section{Conclusion}\nThis paper addresses the task of Online Generic Event Boundary Detection (On-GEBD). We propose \\textbf{ESTimator++}, an enhanced framework that provides a deeper computational implementation of the Event Segmentation Theory (EST). It integrates three key innovations: a Hierarchical Memory-augmented Transformer (HMT) for robust event anticipation, a Structured Multi-view Prediction Error (SMPE) for richer boundary cues, and an Adaptive Multi-scale Temporal Discriminator (AMTD) for learning boundary decisions. These components are trained end-to-end using a novel Structural Consistency Loss (SCL).\n\nExtensive experiments demonstrate the effectiveness of our method. On the Kinetics-GEBD dataset, \\textbf{ESTimator++} achieves a state-of-the-art average F1 score of \\textbf{0.778}, significantly outperforming all existing online baselines, including its predecessor ESTimator (0.748). Similarly, on the TAPOS dataset, it attains a leading average F1 score of \\textbf{0.569}. Notably, our online method performs competitively against many powerful offline models, securing the third-highest overall ranking on both benchmarks. Ablation studies confirm the contributions of each component, with AMTD yielding the most substantial gain as a single module. Furthermore, \\textbf{ESTimator++} maintains a practical inference time of 8.2ms per frame, making it suitable for real-time processing.\n\nIn summary, \\textbf{ESTimator++} establishes a new state-of-the-art for On-GEBD by effectively translating cognitive principles into a synergistic neural architecture. It delivers highly accurate boundary detection with low latency, paving the way for more sophisticated real-time video understanding systems. Future work could explore extending this framework to more complex, long-form video streams and multi-modal inputs.\n\n\\end{document}\n\nPlease generate a professional image generation prompt according to the figure type."
          }
        ]
      },
      "response": {
        "raw_response": "Create an abstract conceptual diagram illustrating the high-level architecture of ESTimator++ for Online Generic Event Boundary Detection, presented as a clean, minimalistic line drawing in a professional academic style suitable for top conferences like NeurIPS or CVPR. This is not a flowchart with decision symbols or process boxes, but an abstract framework diagram emphasizing conceptual modules, logical relationships, and data flows using abstract shapes and connections. Use a horizontal layout from left to right on a white background, with clear visual hierarchy through module grouping and color-coded arrows.\n\nOn the left, depict an abstract input module as a simple rectangular block labeled \"Input Frame Features (f_t)\" in black Arial font, connected by an orange arrow to the central module.\n\nThe central module is a large abstract block representing the \"Hierarchical Memory-augmented Transformer (HMT)\", divided into two sub-modules: an upper rounded rectangle for \"Main Event Memory (M_t)\" in blue text, and a lower one for \"Local Context Cache (C_t)\" in blue text. Show abstract internal connections with thin black lines, and an orange arrow looping within for \"Gated Update\" (labeled in small black text). From HMT, draw a green arrow to an abstract prediction node labeled \"Predicted Feature (\\hat{f}_t)\" in red text.\n\nTo the right of HMT, connect with an orange arrow to a module labeled \"Structured Multi-view Prediction Error (SMPE)\", shown as an abstract triangular shape with three internal compartments labeled \"Feature Reconstruction Error (E^r_t)\", \"Predictive Smoothness Error (E^s_t)\", and \"Memory Perturbation Error (E^m_t)\" in black Arial font. Use orange arrows for input flows into SMPE and a green arrow outputting the concatenated \"Error Vector (E_t)\".\n\nFurther right, connect with a green arrow to the \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" module, depicted as a layered abstract structure with three parallel bars for \"Multi-scale Conv (k1, k2, k3)\" in blue text, converging via thin black lines to a fusion node labeled \"Self-Attention Fusion\" in small black text, then to an output node labeled \"Boundary Probability (P_t^{boundary})\" in red text.\n\nAt the bottom, show an overarching abstract bracket or dashed enclosure spanning the entire diagram, labeled \"Structural Consistency Loss (SCL)\" in red Arial font, with green arrows indicating feedback loops for training (e.g., from SMPE and AMTD back to HMT).\n\nUse orange arrows for data input and processing flows (e.g., feature streams and updates), green arrows for feedback, output, and prediction paths (e.g., error signals and boundary decisions). Keep text in Arial font, with module names in bold black, variables in blue, and key outputs in red for emphasis. Ensure a simple, uncluttered design with straight lines, no shadows or 3D effects, and balanced spacing for readability in a paper figure.",
        "usage": {
          "input_tokens": 12274,
          "output_tokens": 991
        },
        "raw_reasoning_content": null
      }
    },
    {
      "request": {
        "messages": [
          {
            "role": "system",
            "content": "\n你是一位专业的学术图表设计专家。你的任务是将论文内容描述转换为适合 AI 图像生成的详细提示词。\n\n你的职责：\n1. 分析输入的论文内容描述\n2. 根据所需的图表类型（算法图或动机图）调整描述风格\n3. 生成详细且专业的图像生成提示词，确保生成的图像符合顶级学术会议（NeurIPS、ICML、ICLR、CVPR、AAAI 等）的发表标准\n\n图表类型规范：\n- **算法图**：用于展示算法流程、模型架构、方法框架等。必须强调：\n  * **抽象性和概念性**：不要绘制成具体的流程图，而应使用抽象概念图、架构图或框架图\n  * 清晰的结构和模块关系（用抽象模块表示，而非具体步骤）\n  * 逻辑流和数据流（用抽象箭头和连接表示，而非流程图符号）\n  * 技术性和专业性\n  * 简洁的视觉设计，避免过于具体的操作步骤\n  * 强调高层次概念和架构，而非详细的执行流程\n  \n- **动机图**：用于展示问题动机、方法对比、概念解释等。必须强调：\n  * 直观的可视化效果\n  * 突出的对比和差异\n  * 易于理解和可视化\n  * 概念的可理解性\n\n提示词生成要求：\n1. 使用英文生成提示词（AI 图像生成模型对英文的理解更好）\n2. 描述应详细且具体，包括视觉元素、布局、样式等\n3. 强调学术论文的专业性和简洁性\n4. 避免过于复杂的细节，保持图表清晰可读且相对饱满，必要时再复杂你自己判断\n5. **颜色和样式要求**（根据图表类型区分）：\n   \n   **对于算法图**：\n   * **整体风格**：采用简洁、极简主义的线条图风格，使用清晰的线条和抽象图形元素\n   * **颜色方案**：使用橙色（orange）和绿色（green）箭头来区分不同的流程阶段和循环路径，文本主要使用黑色、蓝色和红色作为主色调\n   * **视觉层次**：通过颜色区分信息流（如橙色表示数据输入/处理阶段，绿色表示反馈/输出阶段），形成清晰的视觉层次\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   * **箭头和连接**：使用不同颜色的箭头（橙色、绿色）来明确表示信息流向和循环关系\n   其他你自行判断\n   \n   **对于动机图**：\n   * **整体风格**：采用简洁、直观的图风格，使用清晰的视觉元素突出对比和差异\n   * **颜色方案**：使用对比鲜明的颜色来区分不同方法或概念\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   其他你自行判断\n   \n6. 确保图表适合在论文中展示\n7. **对于算法图**：必须强调抽象性和概念性，明确说明\"不是流程图\"和\"抽象概念图\"，避免使用流程图符号（如决策菱形、处理框等），应使用抽象模块和概念连接\n\n输出格式：\n- 仅输出优化后的提示词文本，不要包含任何解释或额外内容\n- 不要用代码块包裹\n- 提示词应是一个完整的描述，可以直接用于图像生成\n"
          },
          {
            "role": "user",
            "content": "Please convert the following paper content description into a detailed prompt suitable for AI image generation.\n\n[Required Figure Type]\nalgorithm\n\n[Paper Content Description]\n\\documentclass{article}\n\\usepackage{amsmath,amssymb}\n\\usepackage{booktabs}\n\n\\begin{document}\n\n\\title{ESTimator++: Structural Consistency Learning for Online Generic Event Boundary Detection}\n\\maketitle\n\\begin{abstract}\nOnline Generic Event Boundary Detection (On-GEBD) is essential for real-time video understanding, yet remains challenging due to the requirement for efficient, single-pass processing. While Event Segmentation Theory (EST) provides a cognitive foundation, its computational implementation in prior online methods such as ESTimator is limited. This paper presents \\textbf{ESTimator++}, a significantly enhanced framework that advances the original through a more systematic instantiation of EST. Our key contributions are threefold: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation; (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer boundary signals; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} which replaces heuristic thresholding with a learned decision function. These components are integrated via a novel \\textbf{Structural Consistency Loss (SCL)}, enabling end-to-end optimization. Extensive experiments on Kinetics-GEBD and TAPOS benchmarks demonstrate that ESTimator++ achieves state-of-the-art performance for On-GEBD, with average F1 scores of \\textbf{0.778} and \\textbf{0.569}, respectively, outperforming all existing online methods and competing favorably with many offline models. Ablation studies confirm the contribution of each component, with the AMTD providing the most significant single-module gain (+2.9\\%). Remarkably, this is achieved while maintaining high efficiency at \\textbf{8.2 ms per frame}. Our work demonstrates that a principled, integrated design substantially advances the performance of online event segmentation.\n\\end{abstract}\n\n\\section{Introduction}\nThe proliferation of long-form video content on platforms like YouTube and Netflix has created a pressing need for advanced video understanding systems. While tasks such as action recognition are well-established, they predominantly focus on short, trimmed clips with predefined labels. In contrast, the analysis of untrimmed, long-form videos containing complex narratives remains a formidable challenge. Generic Event Boundary Detection (GEBD) tackles this issue by aiming to segment videos into taxonomy-free, semantically coherent units, thereby emulating the human ability to parse continuous visual streams.\n\nCurrent GEBD frameworks operate in an \\textit{offline} manner. They process an entire video segment in one pass, leveraging both past and future context to identify all boundaries simultaneously. This paradigm stands in stark contrast to human perception, which segments events \\textit{online}—sequentially and causally, relying solely on past and present information without any access to future content. To bridge this fundamental gap and more faithfully replicate human cognitive processes, we introduce a novel and more demanding task: \\textbf{Online Generic Event Boundary Detection (On-GEBD)}. In this setting, a model must process a video stream and make immediate, frame-by-frame decisions regarding boundary occurrences, using only historical and current data.\n\nOn-GEBD inherits the inherent challenges of offline GEBD, such as detecting subtle and taxonomy-free transitions, but introduces additional complexities due to its strictly causal and information-limited nature. Existing offline GEBD methods, which often depend on global temporal context through mechanisms like similarity matrices or multi-frame comparisons, are fundamentally incompatible with the online setting. Likewise, naïvely adapting models designed for other online action understanding tasks proves ineffective, as they are not tailored for the fine-grained detection of generic boundaries.\n\nTo address these unique challenges, we propose \\textbf{ESTimator++}, an enhanced online framework inspired by Event Segmentation Theory (EST) from cognitive science. EST suggests that humans continuously predict upcoming visual input based on an ongoing event model and perceive a boundary when a significant prediction error arises. Our framework operationalizes this principle through three core innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer} that maintains a stable representation of the current event for robust future anticipation; (2) a \\textbf{Structured Multi-view Prediction Error} mechanism that derives a richer and more stable discrepancy signal; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator}, a lightweight network that replaces heuristic thresholding with a learned, context-aware decision function. These components are integrated under a novel \\textbf{Structural Consistency Loss} that jointly enforces prediction accuracy and temporal coherence.\n\nExtensive experiments on the Kinetics-GEBD and TAPOS benchmarks demonstrate that \\textbf{ESTimator++} sets a new state-of-the-art for On-GEBD. It significantly surpasses all adapted online baselines and, remarkably, achieves performance competitive with many sophisticated offline methods, despite operating under a strict causal constraint. Ablation studies validate the contribution of each proposed component. The remainder of this paper is organized as follows: we review related work, detail our methodology, present comprehensive experimental results, and provide concluding remarks.\n\n\\section{Related Work}\n\\subsection{Generic Event Boundary Detection}\nGeneric Event Boundary Detection (GEBD) aims to identify boundaries that align with human perceptual segmentation within untrimmed videos, operating without a predefined taxonomy of events. This task differs from traditional video understanding tasks such as Temporal Action Localization (TAL) and Action Recognition, as GEBD handles continuous, open-ended semantics. Recent advancements in offline GEBD typically process entire videos or extensive temporal contexts. Representative methods include UBoCo, which leverages temporal similarity matrices with contrastive learning; DDM-Net, which applies progressive attention on multi-level dense difference maps; and CoSeg, which introduces a reconstruction-based objective inspired by cognitive modeling. While these offline methods have shown progress, they are not designed for streaming video. Our work adapts GEBD to the \\textit{online} setting, requiring instantaneous decisions without access to future frames.\n\n\\subsection{Video Understanding}\nVideo understanding encompasses tasks such as Temporal Action Detection (TAD), Temporal Action Localization (TAL), and Video Instance Segmentation (VIS). State-of-the-art approaches often employ transformer architectures, memory mechanisms, and end-to-end frameworks. A common limitation is their reliance on a closed set of predefined action classes, which restricts applicability in open-world scenarios. Although open-set formulations for TAL exist, they focus on localizing and classifying known actions. In contrast, GEBD detects boundaries for taxonomy-free events, providing a more general approach to video structure understanding.\n\n\\subsection{Online Video Understanding}\nOnline video understanding methods process data in a streaming fashion, sharing core challenges with Online GEBD (On-GEBD). Relevant tasks include Online Action Detection (OAD), Online Temporal Action Localization (On-TAL), and Online Detection of Action Start (ODAS). For OAD, recent works such as LSTR, TeSTra, and MiniROAD demonstrate the efficacy of transformers and recurrent units. For On-TAL, methods like CAG-QIL and OAT have been proposed. While ODAS shares an online nature with On-GEBD, it is designed for a fixed set of action categories. Conversely, On-GEBD must detect subtle, categorical changes between taxonomy-free events, necessitating a distinct methodological approach from conventional online action-centric models.\n\n\\section{Method}\nOur enhanced framework, \\textbf{ESTimator++}, extends the original ESTimator by providing a more systematic computational implementation of Event Segmentation Theory (EST). While retaining the core principle of using prediction discrepancy for online boundary detection, ESTimator++ introduces three key innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation, (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer and more stable boundary cues, and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} that replaces heuristic thresholding with a learned, context-sensitive decision function. These components are unified during training via a novel \\textbf{Structural Consistency Loss (SCL)}, which enforces both local prediction accuracy and global temporal coherence.\n\n\\subsection{Hierarchical Memory-augmented Transformer for Consistent Event Anticipation (HMT-CEA)}\nThe original Consistent Event Anticipation (CEA) employs a standard transformer decoder with a limited look-back window, which may fail to maintain a persistent representation of an ongoing event's gist. Inspired by human cognition—which uses working memory for immediate context and long-term memory for schemas—we propose the HMT-CEA. It maintains two complementary memory structures: a dynamic \\textbf{Main Event Memory ($\\mathbf{M}$)} that captures the abstract, persistent essence of the current event, and a \\textbf{Local Context Cache ($\\mathbf{C}$)} that holds fine-grained recent features.\n\n\\subsubsection{Architecture and State Update}\nGiven a stream of frame features $\\mathbf{f}_t \\in \\mathbb{R}^D$, we first project them into a memory-compatible space: $\\mathbf{h}_t = \\text{Linear}(\\mathbf{f}_t)$. The HMT-CEA operates recurrently, maintaining states $(\\mathbf{M}_t, \\mathbf{C}_t)$.\n\n\\begin{enumerate}\n    \\item \\textbf{Local Context Update}: The cache $\\mathbf{C}_t$ is a FIFO queue of the last $L$ projected features: $\\mathbf{C}_t = [\\mathbf{h}_{t-L}, ..., \\mathbf{h}_{t-1}]$.\n\n    \\item \\textbf{Main Memory Gated Update}: The core memory $\\mathbf{M}_t \\in \\mathbb{R}^{d_m}$ is updated via a gated mechanism using the new observation $\\mathbf{h}_t$ and previous context.\n    \\[\n    \\mathbf{g}_t = \\sigma(\\mathbf{W}_g [\\mathbf{M}_{t-1}; \\bar{\\mathbf{C}}_t; \\mathbf{h}_t] + \\mathbf{b}_g)\n    \\]\n    \\[\n    \\tilde{\\mathbf{M}}_t = \\text{TransformerBlock}(\\mathbf{M}_{t-1}, \\mathbf{C}_t, \\mathbf{h}_t)\n    \\]\n    \\[\n    \\mathbf{M}_t = \\mathbf{g}_t \\odot \\tilde{\\mathbf{M}}_t + (1 - \\mathbf{g}_t) \\odot \\mathbf{M}_{t-1}\n    \\]\n    where $\\bar{\\mathbf{C}}_t$ is the mean-pooled cache, $\\sigma$ is the sigmoid function, $\\odot$ denotes element-wise multiplication, and $\\text{TransformerBlock}$ is a lightweight cross-attention module that allows the memory to attend to the local context and new input.\n\n    \\item \\textbf{Next-Frame Prediction}: The anticipated feature for frame $t$ is generated by fusing information from both memory levels:\n    \\[\n    \\mathbf{z}_t = \\text{Decoder}( \\mathbf{M}_{t-1}, \\mathbf{C}_{t-1} )\n    \\]\n    \\[\n    \\hat{\\mathbf{f}}_t = \\text{Linear}(\\mathbf{z}_t)\n    \\]\n    Here, $\\text{Decoder}$ is a small autoregressive transformer that uses $\\mathbf{M}_{t-1}$ as a persistent query and $\\mathbf{C}_{t-1}$ as keys and values, ensuring causality.\n\\end{enumerate}\n\nThis architecture ensures predictions are grounded in both the immediate past ($\\mathbf{C}$) and a stable, evolving representation of the current event ($\\mathbf{M}$).\n\n\\subsection{Structured Multi-view Prediction Error (SMPE)}\nInstead of a single scalar error, we define a multi-dimensional error vector $\\mathbf{E}_t \\in \\mathbb{R}^3$ that captures different facets of prediction inconsistency, yielding a more robust boundary detection signal.\n\n\\begin{enumerate}\n    \\item \\textbf{Feature Reconstruction Error ($E^{(r)}_t$)}: The primary discrepancy, analogous to the original $\\epsilon_t$, but computed in the projected space for stability.\n    \\[\n    E^{(r)}_t = 1 - \\frac{\\mathbf{h}_t \\cdot \\mathbf{z}_t}{\\|\\mathbf{h}_t\\| \\|\\mathbf{z}_t\\|}\n    \\]\n\n    \\item \\textbf{Predictive Smoothness Error ($E^{(s)}_t$)}: Measures inconsistency between consecutive predictions. Within a consistent event, predictions should vary smoothly.\n    \\[\n    E^{(s)}_t = \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|_2 - \\beta \\cdot \\|\\mathbf{h}_t - \\mathbf{h}_{t-1}\\|_2\n    \\]\n    where $\\beta$ is a scaling factor. This term becomes large only when predictions are jagged despite smooth actual changes.\n\n    \\item \\textbf{Memory Perturbation Error ($E^{(m)}_t$)}: Quantifies the impact of the new frame on the main event memory, reflecting a potential high-level schema violation.\n    \\[\n    E^{(m)}_t = \\|\\mathbf{M}_t - \\mathbf{M}_{t-1}\\|_1\n    \\]\n\\end{enumerate}\n\nThe final SMPE vector is the concatenation: $\\mathbf{E}_t = [E^{(r)}_t, E^{(s)}_t, E^{(m)}_t]^\\top$. This structured error is more informative and less noisy than a scalar counterpart.\n\n\\subsection{Training with Structural Consistency Loss (SCL)}\nThe HMT-CEA is trained by minimizing a novel \\textbf{Structural Consistency Loss (SCL)}, which jointly optimizes prediction accuracy and the temporal structural properties implied by EST.\n\n\\subsubsection{Boundary-Aware Focal Reconstruction Loss}\nWe adapt binary cross-entropy to address class imbalance and hard examples using the primary error $E^{(r)}_t$:\n\\[\n\\mathcal{L}_{Focal}(E^{(r)}_t, y_t) = - \\alpha (1 - p_t)^\\gamma y_t \\log(p_t) - (1 - \\alpha) p_t^\\gamma (1 - y_t) \\log(1 - p_t)\n\\]\nwhere $p_t = \\text{sigmoid}(-\\lambda \\cdot E^{(r)}_t + b)$, $\\lambda$ and $b$ are learnable parameters, $y_t$ is the boundary label, $\\alpha$ balances class frequency, and $\\gamma$ focuses on hard examples.\n\n\\subsubsection{Temporal Coherence Regularization}\nThis unsupervised loss enforces two structural properties:\n\\begin{itemize}\n    \\item \\textbf{Intra-event Prediction Smoothness}: Within non-boundary segments ($y_t=0$), predictions should be smooth:\n    \\[\n    \\mathcal{L}_{Smooth} = \\frac{1}{|\\mathcal{N}|} \\sum_{t \\in \\mathcal{N}} \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|^2_2\n    \\]\n    where $\\mathcal{N}$ is the set of non-boundary frames.\n\n    \\item \\textbf{Intra-event Feature Clustering}: All predictions within a ground-truth event segment should cluster in embedding space:\n    \\[\n    \\mathcal{L}_{Cluster} = \\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{|I_s|} \\sum_{i \\in I_s} \\|\\mathbf{z}_i - \\boldsymbol{\\mu}_s\\|^2_2\n    \\]\n    where $S$ is the number of events in a clip, $I_s$ is the set of frame indices in event $s$, and $\\boldsymbol{\\mu}_s$ is the mean of $\\mathbf{z}_i$ for $i \\in I_s$.\n\\end{itemize}\n\nThe total SCL is: $\\mathcal{L}_{SCL} = \\mathcal{L}_{Focal} + \\lambda_s \\mathcal{L}_{Smooth} + \\lambda_c \\mathcal{L}_{Cluster}$.\n\n\\subsection{Adaptive Multi-scale Temporal Discriminator (AMTD)}\nThe AMTD replaces the heuristic online boundary detector (OBD) with a lightweight temporal convolutional network (TCN) that learns to detect boundaries from the sequence of SMPE vectors $\\{\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t\\}$.\n\n\\subsubsection{Architecture}\nThe AMTD, denoted $\\mathcal{D}_\\phi$, comprises:\n\\begin{enumerate}\n    \\item \\textbf{Multi-scale Feature Extraction}: Parallel 1D convolutions with kernel sizes $k_1, k_2, k_3$ capture short-, medium-, and long-range temporal patterns in $\\mathbf{E}_t$:\n    \\[\n    \\mathbf{s}^{(i)}_t = \\text{ReLU}(\\text{Conv1D}_{k_i}([\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t]))\n    \\]\n\n    \\item \\textbf{Dynamic Fusion}: Multi-scale features are concatenated and weighted adaptively via self-attention:\n    \\[\n    \\mathbf{S}_t = [\\mathbf{s}^{(1)}_t, \\mathbf{s}^{(2)}_t, \\mathbf{s}^{(3)}_t], \\quad \\mathbf{A}_t = \\text{SelfAttention}(\\mathbf{S}_t)\n    \\]\n\n    \\item \\textbf{Boundary Probability Estimation}: The attended feature produces the final online boundary probability:\n    \\[\n    P_t^{boundary} = \\sigma(\\text{MLP}(\\text{Pool}(\\mathbf{A}_t)))\n    \\]\n    A boundary is declared if $P_t^{boundary} > 0.5$.\n\\end{enumerate}\n\n\\subsubsection{Online Operation and Training}\nDuring online inference, AMTD slides over the streaming SMPE vectors using a FIFO buffer of size $\\Delta+1$. It is trained jointly with the HMT-CEA via SCL, where binary labels $y_t$ supervise $P_t^{boundary}$ through a standard cross-entropy loss, forming a fully differentiable, end-to-end optimizable system for online granular event boundary detection (On-GEBD).\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Setup}\n\\label{sec:setup}\n\n\\subsubsection{Benchmark Datasets.}\nWe evaluate our method on two established benchmarks for generic event boundary detection (GEBD). The \\textbf{Kinetics-GEBD} dataset is derived from the Kinetics-400 dataset, comprising approximately 60K videos. Each video is annotated with taxonomy-free event boundaries, containing about five events on average. The dataset is officially split into training (18,794 videos), validation (18,813 videos), and test (17,725 videos) sets. As test set annotations are withheld, we report results on the validation set, consistent with prior work. For cross-validation in certain analyses, we further split the training data into 80\\% for training and 20\\% for validation. The \\textbf{TAPOS} dataset consists of Olympic sports videos originally annotated for action localization. Following the standard adaptation for GEBD, we repurpose it by discarding action labels and treating sub-action boundaries as generic event boundaries. This results in 13,094 training and 1,790 validation action instances.\n\n\\subsubsection{Evaluation Metric.}\nWe adopt the standard Relative Distance (Rel.Dis) metric for GEBD. For a predicted boundary timestamp and its corresponding ground truth, the relative distance is computed as their temporal offset divided by the duration of the union of the predicted and ground-truth event instances. Following common practice, when multiple consecutive frames are predicted as boundaries, the central frame is taken as the final timestamp. Performance is evaluated under ten Rel.Dis thresholds from 0.05 to 0.5 with a step size of 0.05. A prediction is considered correct if its Rel.Dis is below a given threshold. We report the F1 score at each threshold and the average F1 score across all thresholds.\n\n\\subsubsection{Baselines.}\nEstablishing baselines for Online GEBD (On-GEBD) is non-trivial as standard offline GEBD methods are not designed for streaming input. Therefore, we adapt state-of-the-art models from related online video understanding tasks. From Online Action Detection, we adapt TeSTra, OadTR, and MiniROAD. From Online Temporal Action Localization, we adapt Sim-On. For a fair comparison, we modify only the final layer of each model to perform binary classification (boundary vs. non-boundary), denoted with the suffix '-BC'. We also compare against a dedicated online GEBD method, \\textbf{ESTimator}. Furthermore, to contextualize our online performance, we include comparisons with leading offline GEBD methods.\n\n\\subsubsection{Implementation Details.}\nOur proposed method, \\textbf{ESTimator++}, is implemented using standard video features for fair comparison. Videos are sampled at 24 FPS for Kinetics-GEBD and 6 FPS for TAPOS. We use 2048-dimensional features extracted by an ImageNet-pretrained ResNet-50 encoder. The Hierarchical Memory-augmented Transformer (HMT) employs a main memory with dimension $d_m=512$ and a local context cache of size $L=16$. The Adaptive Multi-scale Temporal Discriminator (AMTD) uses temporal convolutional kernels of sizes $[3, 7, 15]$. The model is trained end-to-end for 30 epochs using the AdamW optimizer with a learning rate of $1\\times10^{-4}$, a batch size of 512, and loss weights $\\lambda_s=0.1$ and $\\lambda_c=0.05$.\n\n\\subsection{Main Results}\n\n\\subsubsection{Comparison with Online Baselines}\nTable~\\ref{tab:online_k400} and Table~\\ref{tab:online_tapos} present the online detection performance on the Kinetics-GEBD and TAPOS validation sets, respectively.\n\nOn Kinetics-GEBD (Table~\\ref{tab:online_k400}), \\textbf{ESTimator++} sets a new state-of-the-art, outperforming all online baselines across all Rel.Dis thresholds. Traditional online models like TeSTra-BC and Sim-On-BC show limited effectiveness for this task. While MiniROAD-BC and the prior dedicated method ESTimator perform better, our method achieves a superior average F1 score of \\textbf{0.778}. This represents an improvement of +3.0\\% over ESTimator and +9.7\\% over MiniROAD-BC, demonstrating the efficacy of our proposed architectural components.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the Kinetics-GEBD validation set. 'BC' denotes a binary classifier head. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_k400}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.438 & 0.488 & 0.521 & 0.545 & 0.564 & 0.580 & 0.593 & 0.604 & 0.614 & 0.622 & 0.557 \\\\\nSim-On-BC & 0.461 & 0.534 & 0.579 & 0.610 & 0.633 & 0.651 & 0.664 & 0.675 & 0.685 & 0.692 & 0.618 \\\\\nOadTR-BC & 0.474 & 0.512 & 0.535 & 0.552 & 0.565 & 0.575 & 0.583 & 0.590 & 0.596 & 0.601 & 0.558 \\\\\nMiniROAD-BC & 0.569 & 0.622 & 0.649 & 0.675 & 0.691 & 0.704 & 0.714 & 0.722 & 0.729 & 0.735 & 0.681 \\\\\nESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.642} & \\textbf{0.712} & \\textbf{0.751} & \\textbf{0.775} & \\textbf{0.790} & \\textbf{0.802} & \\textbf{0.810} & \\textbf{0.816} & \\textbf{0.821} & \\textbf{0.825} & \\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nA similar trend is observed on TAPOS (Table~\\ref{tab:online_tapos}). \\textbf{ESTimator++} achieves the best average F1 score of \\textbf{0.569}, outperforming ESTimator by +2.2\\% and MiniROAD-BC by +4.1\\%. The advantage is particularly notable at stricter thresholds (e.g., 0.05, 0.1), indicating more precise boundary localization. The consistent superiority across both datasets underscores the robustness and generalizability of our approach.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the TAPOS validation set. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_tapos}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.364 & 0.417 & 0.452 & 0.478 & 0.496 & 0.511 & 0.523 & 0.533 & 0.542 & 0.550 & 0.487 \\\\\nSim-On-BC & 0.225 & 0.269 & 0.303 & 0.329 & 0.350 & 0.367 & 0.381 & 0.394 & 0.405 & 0.415 & 0.344 \\\\\nOadTR-BC & 0.263 & 0.319 & 0.361 & 0.394 & 0.422 & 0.445 & 0.465 & 0.483 & 0.497 & 0.510 & 0.416 \\\\\nMiniROAD-BC & 0.422 & 0.472 & 0.502 & 0.522 & 0.537 & 0.549 & 0.558 & 0.566 & 0.572 & 0.578 & 0.528 \\\\\nESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.418} & \\textbf{0.484} & \\textbf{0.531} & \\textbf{0.566} & \\textbf{0.592} & \\textbf{0.613} & \\textbf{0.629} & \\textbf{0.642} & \\textbf{0.653} & \\textbf{0.663} & \\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Comparison with Offline Methods}\nWe further compare our online method against powerful offline models that utilize the full video context. Results are shown in Table~\\ref{tab:offline_k400} for Kinetics-GEBD and Table~\\ref{tab:offline_tapos} for TAPOS.\n\nRemarkably, on Kinetics-GEBD, \\textbf{ESTimator++} with an average F1 of \\textbf{0.778} not only leads all online methods but also competes favorably with state-of-the-art offline approaches. It surpasses all offline supervised methods except for PC and the unsupervised CoSeg. It is important to note that PC and CoSeg benefit from full-video processing and, in CoSeg's case, cross-video co-segmentation. The fact that our \\textit{online} method achieves performance comparable to sophisticated offline models like TCN highlights the efficacy of its design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on Kinetics-GEBD. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_k400}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & BMN & 0.186 & 0.204 & 0.213 & 0.220 & 0.226 & 0.230 & 0.233 & 0.237 & 0.239 & 0.241 & 0.223 \\\\\nOffline & Supervised & BMN-StartEnd & 0.491 & 0.589 & 0.627 & 0.648 & 0.660 & 0.668 & 0.674 & 0.678 & 0.681 & 0.683 & 0.640 \\\\\nOffline & Supervised & TCN-TAPOS & 0.464 & 0.560 & 0.602 & 0.628 & 0.645 & 0.659 & 0.669 & 0.676 & 0.682 & 0.687 & 0.627 \\\\\nOffline & Supervised & TCN & 0.588 & 0.657 & 0.679 & 0.691 & 0.698 & 0.703 & 0.706 & 0.708 & 0.710 & 0.712 & 0.685 \\\\\nOffline & Supervised & \\textbf{PC} & 0.625 & \\textbf{0.758} & \\textbf{0.804} & \\textbf{0.829} & \\textbf{0.844} & \\textbf{0.853} & \\textbf{0.859} & \\textbf{0.864} & \\textbf{0.867} & \\textbf{0.870} & \\textbf{0.817} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.275 & 0.300 & 0.312 & 0.319 & 0.324 & 0.327 & 0.330 & 0.332 & 0.334 & 0.335 & 0.318 \\\\\nOffline & Unsupervised & PA-Random & 0.336 & 0.435 & 0.484 & 0.512 & 0.529 & 0.541 & 0.548 & 0.554 & 0.558 & 0.561 & 0.506 \\\\\nOffline & Unsupervised & PA & 0.396 & 0.488 & 0.520 & 0.534 & 0.544 & 0.550 & 0.555 & 0.558 & 0.561 & 0.564 & 0.527 \\\\\nOffline & Unsupervised & \\underline{CoSeg} & \\textbf{0.656} & 0.758 & 0.783 & 0.794 & 0.799 & 0.803 & 0.804 & 0.806 & 0.807 & 0.809 & 0.782 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.712} & $\\dagger$\\textbf{0.751} & $\\dagger$\\textbf{0.775} & $\\dagger$\\textbf{0.790} & $\\dagger$\\textbf{0.802} & $\\dagger$\\textbf{0.810} & $\\dagger$\\textbf{0.816} & $\\dagger$\\textbf{0.821} & $\\dagger$\\textbf{0.825} & $\\dagger$\\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\nOn the TAPOS dataset (Table~\\ref{tab:offline_tapos}), \\textbf{ESTimator++} again demonstrates strong competitiveness, achieving an average F1 of \\textbf{0.569}. It outperforms all offline supervised baselines except PC, and shows superior performance compared to the unsupervised PA method, especially at stricter thresholds. This consistent ability to rival many offline models underscores a key advantage: \\textbf{ESTimator++} delivers high-quality boundary detection with minimal latency, making it suitable for real-time applications.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on TAPOS. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_tapos}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & ISBA & 0.106 & 0.170 & 0.227 & 0.265 & 0.298 & 0.326 & 0.348 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & TCN & 0.237 & 0.312 & 0.331 & 0.339 & 0.342 & 0.344 & 0.347 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & CTM & 0.244 & 0.312 & 0.336 & 0.351 & 0.361 & 0.369 & 0.374 & 0.381 & 0.383 & 0.385 & 0.350 \\\\\nOffline & Supervised & TransParser & 0.289 & 0.381 & 0.435 & 0.475 & 0.500 & 0.514 & 0.527 & 0.534 & 0.540 & 0.545 & 0.474 \\\\\nOffline & Supervised & \\textbf{PC} & \\textbf{0.522} & \\textbf{0.595} & \\textbf{0.628} & \\textbf{0.646} & \\textbf{0.659} & \\textbf{0.665} & \\textbf{0.671} & \\textbf{0.676} & \\textbf{0.679} & \\textbf{0.683} & \\textbf{0.642} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.035 & 0.045 & 0.047 & 0.051 & 0.053 & 0.054 & 0.055 & 0.056 & 0.057 & 0.058 & 0.051 \\\\\nOffline & Unsupervised & PA-Random & 0.158 & 0.233 & 0.273 & 0.310 & 0.331 & 0.347 & 0.357 & 0.369 & 0.376 & 0.384 & 0.314 \\\\\nOffline & Unsupervised & \\underline{PA} & 0.360 & 0.459 & 0.507 & 0.543 & 0.567 & 0.579 & 0.592 & 0.601 & 0.609 & 0.615 & 0.543 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.418} & $\\dagger$\\textbf{0.484} & $\\dagger$\\textbf{0.531} & $\\dagger$\\textbf{0.566} & $\\dagger$\\textbf{0.592} & $\\dagger$\\textbf{0.613} & $\\dagger$\\textbf{0.629} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.653} & $\\dagger$\\textbf{0.663} & $\\dagger$\\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\n\\subsection{Model Efficiency}\nWhile \\textbf{ESTimator++} introduces more sophisticated components, its design prioritizes online efficiency. We profile the average inference time per frame on a single NVIDIA V100 GPU. As shown in Table~\\ref{tab:efficiency}, \\textbf{ESTimator++} processes frames at \\textbf{8.2 ms (∼122 FPS)}, which is significantly faster than the real-time requirement of 24 FPS for the Kinetics dataset. Although it is understandably slower than the simpler ESTimator and some lightweight baselines like TeSTra-BC, its runtime is still an order of magnitude lower than the total latency of offline methods, which must process entire clips. This demonstrates that \\textbf{ESTimator++} successfully balances state-of-the-art accuracy with practical online inference speed.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison of inference efficiency (lower time per frame is better). Offline methods process full clips; their reported time is the total latency per clip.}\n\\label{tab:efficiency}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Method} & \\textbf{Time per Frame (ms)} \\\\\n\\midrule\n\\textbf{Online Methods} \\\\\nTeSTra-BC & 2.1 \\\\\nMiniROAD-BC & 5.7 \\\\\nESTimator & 6.5 \\\\\n\\textbf{ESTimator++ (Ours)} & \\textbf{8.2} \\\\\n\\midrule\n\\textbf{Offline Methods (Full Clip)} \\\\\nTCN & 1520* \\\\\nPC & 1850* \\\\\n\\bottomrule\n\\multicolumn{2}{l}{*Total processing time for a 10-second clip, not per frame.}\n\\end{tabular}\n\\end{table}\n\n\\subsection{Qualitative Analysis}\nQualitative comparisons reveal that predictions from \\textbf{ESTimator++} exhibit sharper and more confident peaks at genuine event boundaries while showing greater suppression of spurious fluctuations within event segments, compared to both the TeSTra-BC baseline and its predecessor ESTimator. This can be attributed to the Structured Multi-view Prediction Error (SMPE) providing a cleaner signal and the Adaptive Multi-scale Temporal Discriminator (AMTD) learning to distinguish true boundary patterns from noise. Furthermore, in sequences with gradual transitions, \\textbf{ESTimator++} shows improved temporal localization, likely due to the multi-scale receptive field of the AMTD. These observations align with its superior quantitative performance.\n\n\\section{Ablation Studies}\n\\subsection{Ablation Study}\n\\label{sec:ablation}\n\nWe conduct a comprehensive ablation study on the Kinetics-GEBD validation set to validate the contribution of each component in \\textbf{ESTimator++}. All experiments follow the implementation details in Section~\\ref{sec:setup}.\n\n\\subsubsection{Impact of Proposed Components}\nWe start from a baseline model that uses a standard Transformer decoder for prediction and a single reconstruction error with a fixed heuristic threshold for detection. This baseline achieves an average F1 score of 0.715. We then sequentially integrate our proposed components, with results summarized in Table~\\ref{tab:ablation_components}.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation study on the contribution of each proposed component. Evaluated on Kinetics-GEBD validation set (Average F1). $\\Delta$ denotes the improvement over the previous row.}\n\\label{tab:ablation_components}\n\\begin{tabular}{lcccccc}\n\\toprule\n\\textbf{Variant} & \\textbf{HMT} & \\textbf{SMPE} & \\textbf{SCL} & \\textbf{AMTD} & \\textbf{Avg F1} & $\\Delta$ \\\\\n\\midrule\nBaseline (ESTimator Core) & & & & & 0.715 & -- \\\\\n+ Hierarchical Memory (HMT) & \\checkmark & & & & 0.733 & +0.018 \\\\\n+ Structured Error (SMPE) & \\checkmark & \\checkmark & & & 0.754 & +0.021 \\\\\n+ Consistency Loss (SCL) & \\checkmark & \\checkmark & \\checkmark & & 0.769 & +0.015 \\\\\n+ Adaptive Discriminator (AMTD) & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\textbf{0.778} & \\textbf{+0.029} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nIntegrating the \\textbf{Hierarchical Memory-augmented Transformer (HMT)} improves performance by +1.8\\%, demonstrating the importance of maintaining stable event context for consistent prediction. Replacing the single error with the \\textbf{Structured Multi-view Prediction Error (SMPE)} yields a further gain of +2.1\\%, confirming that a composite error signal provides richer boundary cues. Employing the \\textbf{Structural Consistency Loss (SCL)} for training adds another +1.5\\%, validating that enforcing intra-event smoothness and feature clustering shapes more discriminative representations. Finally, substituting the heuristic threshold with the learned \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} provides the most substantial single-module improvement of +2.9\\%, underscoring the advantage of a data-driven decision function. The full model integrating all components achieves the best performance of \\textbf{0.778}, confirming their synergistic effect.\n\n\\subsubsection{Analysis of the Structured Multi-view Prediction Error (SMPE)}\nWe analyze the contribution of each error view within the SMPE by ablating them individually from the full model. Results are presented in Table~\\ref{tab:ablation_smpe}. Removing the \\textbf{Predictive Smoothness Error} causes the most significant drop (-1.7\\% in Avg F1), highlighting its sensitivity to event transitions. Omitting the \\textbf{Memory Perturbation Error} leads to a -1.2\\% decrease, demonstrating the importance of tracking high-level event schema changes. Removing the primary \\textbf{Feature Reconstruction Error} also degrades performance (-1.0\\%). Using any single error view consistently underperforms compared to the full SMPE vector, validating our composite design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structured Multi-view Prediction Error (SMPE). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_smpe}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Error Components Used} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full SMPE} ($E^{(r)}, E^{(s)}, E^{(m)}$) & \\textbf{0.778} \\\\\n\\hline\n$E^{(s)}, E^{(m)}$ (w/o $E^{(r)}$) & 0.768 \\\\\n$E^{(r)}, E^{(m)}$ (w/o $E^{(s)}$) & 0.761 \\\\\n$E^{(r)}, E^{(s)}$ (w/o $E^{(m)}$) & 0.766 \\\\\n\\hline\nOnly $E^{(r)}$ & 0.756 \\\\\nOnly $E^{(s)}$ & 0.749 \\\\\nOnly $E^{(m)}$ & 0.738 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Design Choices for the Adaptive Discriminator (AMTD)}\nWe analyze key design choices in the AMTD module. Results are shown in Table~\\ref{tab:ablation_amtd}. Replacing the AMTD with the original heuristic Online Boundary Detector (OBD) causes a performance drop to 0.748. Using a \\textbf{single-scale} Temporal Convolutional Network (TCN) instead of the multi-scale architecture reduces Avg F1 to 0.771. Removing the \\textbf{self-attention based dynamic fusion} and simply concatenating multi-scale features results in a score of 0.774. These ablations confirm that each aspect of the AMTD's design—its learned nature, multi-scale perception, and adaptive fusion—contributes to its effectiveness.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the design of the Adaptive Multi-scale Temporal Discriminator (AMTD). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_amtd}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{AMTD Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full AMTD (Ours)} & \\textbf{0.778} \\\\\n\\hline\nReplace with ESTimator's OBD & 0.748 \\\\\nSingle-scale TCN (kernel=7) & 0.771 \\\\\nMulti-scale TCN w/o Self-Attention Fusion & 0.774 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Utility of the Structural Consistency Loss (SCL)}\nWe dissect the contribution of the two regularization terms in the SCL. As shown in Table~\\ref{tab:ablation_loss}, training with only the \\textbf{Focal Reconstruction Loss} yields a baseline score of 0.754. Adding the \\textbf{Intra-event Smoothness Loss} improves performance to 0.766. Further incorporating the \\textbf{Intra-event Clustering Loss} to form the full SCL pushes the score to 0.769. The full SCL, combining both regularizers, delivers the best result, confirming that jointly optimizing for local accuracy and global temporal coherence is an effective strategy.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structural Consistency Loss (SCL). The HMT and SMPE are used in all variants. Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_loss}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Training Loss Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n$\\mathcal{L}_{Focal}$ only & 0.754 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth}$ & 0.766 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Cluster}$ & 0.762 \\\\\n\\textbf{Full SCL} ($\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth} + \\mathcal{L}_{Cluster}$) & \\textbf{0.769} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\textbf{Summary.} Our ablation studies provide compelling evidence for the necessity and effectiveness of every component in \\textbf{ESTimator++}. The hierarchical memory (HMT) establishes a robust predictive foundation, the structured error (SMPE) extracts optimal boundary cues, the consistency loss (SCL) shapes the learning objective, and the adaptive discriminator (AMTD) makes precise online decisions. The full model's superior performance arises from their seamless integration.\n\n\\section{Evaluation}\n\\subsection{Training Dynamics and Convergence Analysis}\nTo gain deeper insights into the optimization behavior of \\textbf{ESTimator++}, we analyze its training dynamics. The training and validation loss curves over 30 epochs on the Kinetics-GEBD dataset show that \\textbf{ESTimator++} converges more smoothly and stably to a lower final loss compared to its predecessor \\textbf{ESTimator} and the strong baseline \\textbf{MiniROAD-BC}. This indicates that the Structural Consistency Loss (SCL) provides a more stable gradient signal. Furthermore, the gap between training and validation loss for \\textbf{ESTimator++} remains consistently narrow, suggesting strong generalization and reduced overfitting, a benefit likely attributable to the regularizing effects of $\\mathcal{L}_{Smooth}$ and $\\mathcal{L}_{Cluster}$. In contrast, the baseline models exhibit more pronounced fluctuations and a larger generalization gap.\n\n\\subsection{In-depth Case Study}\nWe conduct a systematic case study to examine \\textbf{ESTimator++}'s performance across diverse scenarios. We categorize sequences from the Kinetics-GEBD validation set into groups such as \\textit{‘fast-paced action transitions’}, \\textit{‘gradual scene changes’}, and \\textit{‘static periods with subtle motions’}. For each category, we compare the SMPE signal and final boundary predictions from \\textbf{ESTimator++} against those from \\textbf{ESTimator} and \\textbf{TeSTra-BC}. In fast-paced sequences, \\textbf{ESTimator++}'s AMTD demonstrates superior ability to isolate closely spaced peaks, leading to precise detection of sequential boundaries. For gradual transitions, the multi-view SMPE provides a rising error signal over several frames, which the multi-scale AMTD effectively integrates to produce a confident prediction. During static periods, \\textbf{ESTimator++} exhibits significantly lower spurious activation due to the intra-event smoothness constraint. These observations concretely explain the quantitative improvements shown in the main results.\n\n\\section{Conclusion}\nThis paper addresses the task of Online Generic Event Boundary Detection (On-GEBD). We propose \\textbf{ESTimator++}, an enhanced framework that provides a deeper computational implementation of the Event Segmentation Theory (EST). It integrates three key innovations: a Hierarchical Memory-augmented Transformer (HMT) for robust event anticipation, a Structured Multi-view Prediction Error (SMPE) for richer boundary cues, and an Adaptive Multi-scale Temporal Discriminator (AMTD) for learning boundary decisions. These components are trained end-to-end using a novel Structural Consistency Loss (SCL).\n\nExtensive experiments demonstrate the effectiveness of our method. On the Kinetics-GEBD dataset, \\textbf{ESTimator++} achieves a state-of-the-art average F1 score of \\textbf{0.778}, significantly outperforming all existing online baselines, including its predecessor ESTimator (0.748). Similarly, on the TAPOS dataset, it attains a leading average F1 score of \\textbf{0.569}. Notably, our online method performs competitively against many powerful offline models, securing the third-highest overall ranking on both benchmarks. Ablation studies confirm the contributions of each component, with AMTD yielding the most substantial gain as a single module. Furthermore, \\textbf{ESTimator++} maintains a practical inference time of 8.2ms per frame, making it suitable for real-time processing.\n\nIn summary, \\textbf{ESTimator++} establishes a new state-of-the-art for On-GEBD by effectively translating cognitive principles into a synergistic neural architecture. It delivers highly accurate boundary detection with low latency, paving the way for more sophisticated real-time video understanding systems. Future work could explore extending this framework to more complex, long-form video streams and multi-modal inputs.\n\n\\end{document}\n\nPlease generate a professional image generation prompt according to the figure type."
          }
        ]
      },
      "response": {
        "raw_response": "Create a high-level abstract concept diagram illustrating the architecture of ESTimator++, a framework for Online Generic Event Boundary Detection, in a clean, minimalist line art style suitable for a top-tier academic conference paper like NeurIPS or CVPR. This is not a flowchart; instead, use abstract rectangular modules and conceptual nodes to represent high-level components and their relationships, with logical data flows indicated by stylized arrows. Emphasize technical professionalism, simplicity, and clarity, avoiding any specific steps or flowchart symbols like diamonds or ovals—focus on modular architecture and conceptual connections.\n\nLayout: Arrange in a horizontal pipeline from left to right on a white background, with input on the left and output on the right. Start with an abstract node labeled \"Input Frame Features (f_t)\" as a simple box. Connect it to a central composite module labeled \"Hierarchical Memory-augmented Transformer (HMT)\" which includes sub-nodes for \"Main Event Memory (M)\" and \"Local Context Cache (C)\", shown as stacked abstract blocks with dashed lines indicating gated updates and cross-attention. From HMT, show an abstract connection to a node labeled \"Next-Frame Prediction (hat{f}_t)\", then to a module labeled \"Structured Multi-view Prediction Error (SMPE)\" depicted as a conceptual vector with three sub-components: \"Feature Reconstruction Error (E^r)\", \"Predictive Smoothness Error (E^s)\", and \"Memory Perturbation Error (E^m)\", represented as parallel abstract bars.\n\nContinue the flow to a module labeled \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" as an abstract layered structure with sub-layers for multi-scale convolutions (short, medium, long kernels) and self-attention fusion, ending in a probability output. Finally, connect to an output node labeled \"Boundary Probability (P_t^boundary)\". Overlay a curved abstract bracket encompassing the entire diagram, labeled \"Structural Consistency Loss (SCL) for End-to-End Training\" to indicate integration.\n\nVisual elements: Use thin black lines for module borders, with subtle shadows for depth. Employ orange arrows for data input and processing flows (e.g., from input to HMT and SMPE), and green arrows for feedback, output, or loop connections (e.g., memory updates in HMT and from SMPE to AMTD). Text labels in clear Arial font: black for main module names, blue for sub-components, and red for key equations or symbols like \"E_t = [E^r, E^s, E^m]\". Ensure visual hierarchy with orange indicating primary information flow and green for secondary cycles, creating a balanced, non-cluttered composition that fits a square aspect ratio for paper insertion. Keep the design abstract, conceptual, and professional, with no icons, colors only on arrows, and ample white space for readability.",
        "usage": {
          "input_tokens": 12274,
          "output_tokens": 835
        },
        "raw_reasoning_content": null
      }
    },
    {
      "request": {
        "messages": [
          {
            "role": "system",
            "content": "\n你是一位专业的学术图表设计专家。你的任务是将论文内容描述转换为适合 AI 图像生成的详细提示词。\n\n你的职责：\n1. 分析输入的论文内容描述\n2. 根据所需的图表类型（算法图或动机图）调整描述风格\n3. 生成详细且专业的图像生成提示词，确保生成的图像符合顶级学术会议（NeurIPS、ICML、ICLR、CVPR、AAAI 等）的发表标准\n\n图表类型规范：\n- **算法图**：用于展示算法流程、模型架构、方法框架等。必须强调：\n  * **抽象性和概念性**：不要绘制成具体的流程图，而应使用抽象概念图、架构图或框架图\n  * 清晰的结构和模块关系（用抽象模块表示，而非具体步骤）\n  * 逻辑流和数据流（用抽象箭头和连接表示，而非流程图符号）\n  * 技术性和专业性\n  * 简洁的视觉设计，避免过于具体的操作步骤\n  * 强调高层次概念和架构，而非详细的执行流程\n  \n- **动机图**：用于展示问题动机、方法对比、概念解释等。必须强调：\n  * 直观的可视化效果\n  * 突出的对比和差异\n  * 易于理解和可视化\n  * 概念的可理解性\n\n提示词生成要求：\n1. 使用英文生成提示词（AI 图像生成模型对英文的理解更好）\n2. 描述应详细且具体，包括视觉元素、布局、样式等\n3. 强调学术论文的专业性和简洁性\n4. 避免过于复杂的细节，保持图表清晰可读且相对饱满，必要时再复杂你自己判断\n5. **颜色和样式要求**（根据图表类型区分）：\n   \n   **对于算法图**：\n   * **整体风格**：采用简洁、极简主义的线条图风格，使用清晰的线条和抽象图形元素\n   * **颜色方案**：使用橙色（orange）和绿色（green）箭头来区分不同的流程阶段和循环路径，文本主要使用黑色、蓝色和红色作为主色调\n   * **视觉层次**：通过颜色区分信息流（如橙色表示数据输入/处理阶段，绿色表示反馈/输出阶段），形成清晰的视觉层次\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   * **箭头和连接**：使用不同颜色的箭头（橙色、绿色）来明确表示信息流向和循环关系\n   其他你自行判断\n   \n   **对于动机图**：\n   * **整体风格**：采用简洁、直观的图风格，使用清晰的视觉元素突出对比和差异\n   * **颜色方案**：使用对比鲜明的颜色来区分不同方法或概念\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   其他你自行判断\n   \n6. 确保图表适合在论文中展示\n7. **对于算法图**：必须强调抽象性和概念性，明确说明\"不是流程图\"和\"抽象概念图\"，避免使用流程图符号（如决策菱形、处理框等），应使用抽象模块和概念连接\n\n输出格式：\n- 仅输出优化后的提示词文本，不要包含任何解释或额外内容\n- 不要用代码块包裹\n- 提示词应是一个完整的描述，可以直接用于图像生成\n"
          },
          {
            "role": "user",
            "content": "Please convert the following paper content description into a detailed prompt suitable for AI image generation.\n\n[Required Figure Type]\nalgorithm\n\n[Paper Content Description]\n\\documentclass{article}\n\\usepackage{amsmath,amssymb}\n\\usepackage{booktabs}\n\n\\begin{document}\n\n\\title{ESTimator++: Structural Consistency Learning for Online Generic Event Boundary Detection}\n\\maketitle\n\\begin{abstract}\nOnline Generic Event Boundary Detection (On-GEBD) is essential for real-time video understanding, yet remains challenging due to the requirement for efficient, single-pass processing. While Event Segmentation Theory (EST) provides a cognitive foundation, its computational implementation in prior online methods such as ESTimator is limited. This paper presents \\textbf{ESTimator++}, a significantly enhanced framework that advances the original through a more systematic instantiation of EST. Our key contributions are threefold: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation; (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer boundary signals; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} which replaces heuristic thresholding with a learned decision function. These components are integrated via a novel \\textbf{Structural Consistency Loss (SCL)}, enabling end-to-end optimization. Extensive experiments on Kinetics-GEBD and TAPOS benchmarks demonstrate that ESTimator++ achieves state-of-the-art performance for On-GEBD, with average F1 scores of \\textbf{0.778} and \\textbf{0.569}, respectively, outperforming all existing online methods and competing favorably with many offline models. Ablation studies confirm the contribution of each component, with the AMTD providing the most significant single-module gain (+2.9\\%). Remarkably, this is achieved while maintaining high efficiency at \\textbf{8.2 ms per frame}. Our work demonstrates that a principled, integrated design substantially advances the performance of online event segmentation.\n\\end{abstract}\n\n\\section{Introduction}\nThe proliferation of long-form video content on platforms like YouTube and Netflix has created a pressing need for advanced video understanding systems. While tasks such as action recognition are well-established, they predominantly focus on short, trimmed clips with predefined labels. In contrast, the analysis of untrimmed, long-form videos containing complex narratives remains a formidable challenge. Generic Event Boundary Detection (GEBD) tackles this issue by aiming to segment videos into taxonomy-free, semantically coherent units, thereby emulating the human ability to parse continuous visual streams.\n\nCurrent GEBD frameworks operate in an \\textit{offline} manner. They process an entire video segment in one pass, leveraging both past and future context to identify all boundaries simultaneously. This paradigm stands in stark contrast to human perception, which segments events \\textit{online}—sequentially and causally, relying solely on past and present information without any access to future content. To bridge this fundamental gap and more faithfully replicate human cognitive processes, we introduce a novel and more demanding task: \\textbf{Online Generic Event Boundary Detection (On-GEBD)}. In this setting, a model must process a video stream and make immediate, frame-by-frame decisions regarding boundary occurrences, using only historical and current data.\n\nOn-GEBD inherits the inherent challenges of offline GEBD, such as detecting subtle and taxonomy-free transitions, but introduces additional complexities due to its strictly causal and information-limited nature. Existing offline GEBD methods, which often depend on global temporal context through mechanisms like similarity matrices or multi-frame comparisons, are fundamentally incompatible with the online setting. Likewise, naïvely adapting models designed for other online action understanding tasks proves ineffective, as they are not tailored for the fine-grained detection of generic boundaries.\n\nTo address these unique challenges, we propose \\textbf{ESTimator++}, an enhanced online framework inspired by Event Segmentation Theory (EST) from cognitive science. EST suggests that humans continuously predict upcoming visual input based on an ongoing event model and perceive a boundary when a significant prediction error arises. Our framework operationalizes this principle through three core innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer} that maintains a stable representation of the current event for robust future anticipation; (2) a \\textbf{Structured Multi-view Prediction Error} mechanism that derives a richer and more stable discrepancy signal; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator}, a lightweight network that replaces heuristic thresholding with a learned, context-aware decision function. These components are integrated under a novel \\textbf{Structural Consistency Loss} that jointly enforces prediction accuracy and temporal coherence.\n\nExtensive experiments on the Kinetics-GEBD and TAPOS benchmarks demonstrate that \\textbf{ESTimator++} sets a new state-of-the-art for On-GEBD. It significantly surpasses all adapted online baselines and, remarkably, achieves performance competitive with many sophisticated offline methods, despite operating under a strict causal constraint. Ablation studies validate the contribution of each proposed component. The remainder of this paper is organized as follows: we review related work, detail our methodology, present comprehensive experimental results, and provide concluding remarks.\n\n\\section{Related Work}\n\\subsection{Generic Event Boundary Detection}\nGeneric Event Boundary Detection (GEBD) aims to identify boundaries that align with human perceptual segmentation within untrimmed videos, operating without a predefined taxonomy of events. This task differs from traditional video understanding tasks such as Temporal Action Localization (TAL) and Action Recognition, as GEBD handles continuous, open-ended semantics. Recent advancements in offline GEBD typically process entire videos or extensive temporal contexts. Representative methods include UBoCo, which leverages temporal similarity matrices with contrastive learning; DDM-Net, which applies progressive attention on multi-level dense difference maps; and CoSeg, which introduces a reconstruction-based objective inspired by cognitive modeling. While these offline methods have shown progress, they are not designed for streaming video. Our work adapts GEBD to the \\textit{online} setting, requiring instantaneous decisions without access to future frames.\n\n\\subsection{Video Understanding}\nVideo understanding encompasses tasks such as Temporal Action Detection (TAD), Temporal Action Localization (TAL), and Video Instance Segmentation (VIS). State-of-the-art approaches often employ transformer architectures, memory mechanisms, and end-to-end frameworks. A common limitation is their reliance on a closed set of predefined action classes, which restricts applicability in open-world scenarios. Although open-set formulations for TAL exist, they focus on localizing and classifying known actions. In contrast, GEBD detects boundaries for taxonomy-free events, providing a more general approach to video structure understanding.\n\n\\subsection{Online Video Understanding}\nOnline video understanding methods process data in a streaming fashion, sharing core challenges with Online GEBD (On-GEBD). Relevant tasks include Online Action Detection (OAD), Online Temporal Action Localization (On-TAL), and Online Detection of Action Start (ODAS). For OAD, recent works such as LSTR, TeSTra, and MiniROAD demonstrate the efficacy of transformers and recurrent units. For On-TAL, methods like CAG-QIL and OAT have been proposed. While ODAS shares an online nature with On-GEBD, it is designed for a fixed set of action categories. Conversely, On-GEBD must detect subtle, categorical changes between taxonomy-free events, necessitating a distinct methodological approach from conventional online action-centric models.\n\n\\section{Method}\nOur enhanced framework, \\textbf{ESTimator++}, extends the original ESTimator by providing a more systematic computational implementation of Event Segmentation Theory (EST). While retaining the core principle of using prediction discrepancy for online boundary detection, ESTimator++ introduces three key innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation, (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer and more stable boundary cues, and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} that replaces heuristic thresholding with a learned, context-sensitive decision function. These components are unified during training via a novel \\textbf{Structural Consistency Loss (SCL)}, which enforces both local prediction accuracy and global temporal coherence.\n\n\\subsection{Hierarchical Memory-augmented Transformer for Consistent Event Anticipation (HMT-CEA)}\nThe original Consistent Event Anticipation (CEA) employs a standard transformer decoder with a limited look-back window, which may fail to maintain a persistent representation of an ongoing event's gist. Inspired by human cognition—which uses working memory for immediate context and long-term memory for schemas—we propose the HMT-CEA. It maintains two complementary memory structures: a dynamic \\textbf{Main Event Memory ($\\mathbf{M}$)} that captures the abstract, persistent essence of the current event, and a \\textbf{Local Context Cache ($\\mathbf{C}$)} that holds fine-grained recent features.\n\n\\subsubsection{Architecture and State Update}\nGiven a stream of frame features $\\mathbf{f}_t \\in \\mathbb{R}^D$, we first project them into a memory-compatible space: $\\mathbf{h}_t = \\text{Linear}(\\mathbf{f}_t)$. The HMT-CEA operates recurrently, maintaining states $(\\mathbf{M}_t, \\mathbf{C}_t)$.\n\n\\begin{enumerate}\n    \\item \\textbf{Local Context Update}: The cache $\\mathbf{C}_t$ is a FIFO queue of the last $L$ projected features: $\\mathbf{C}_t = [\\mathbf{h}_{t-L}, ..., \\mathbf{h}_{t-1}]$.\n\n    \\item \\textbf{Main Memory Gated Update}: The core memory $\\mathbf{M}_t \\in \\mathbb{R}^{d_m}$ is updated via a gated mechanism using the new observation $\\mathbf{h}_t$ and previous context.\n    \\[\n    \\mathbf{g}_t = \\sigma(\\mathbf{W}_g [\\mathbf{M}_{t-1}; \\bar{\\mathbf{C}}_t; \\mathbf{h}_t] + \\mathbf{b}_g)\n    \\]\n    \\[\n    \\tilde{\\mathbf{M}}_t = \\text{TransformerBlock}(\\mathbf{M}_{t-1}, \\mathbf{C}_t, \\mathbf{h}_t)\n    \\]\n    \\[\n    \\mathbf{M}_t = \\mathbf{g}_t \\odot \\tilde{\\mathbf{M}}_t + (1 - \\mathbf{g}_t) \\odot \\mathbf{M}_{t-1}\n    \\]\n    where $\\bar{\\mathbf{C}}_t$ is the mean-pooled cache, $\\sigma$ is the sigmoid function, $\\odot$ denotes element-wise multiplication, and $\\text{TransformerBlock}$ is a lightweight cross-attention module that allows the memory to attend to the local context and new input.\n\n    \\item \\textbf{Next-Frame Prediction}: The anticipated feature for frame $t$ is generated by fusing information from both memory levels:\n    \\[\n    \\mathbf{z}_t = \\text{Decoder}( \\mathbf{M}_{t-1}, \\mathbf{C}_{t-1} )\n    \\]\n    \\[\n    \\hat{\\mathbf{f}}_t = \\text{Linear}(\\mathbf{z}_t)\n    \\]\n    Here, $\\text{Decoder}$ is a small autoregressive transformer that uses $\\mathbf{M}_{t-1}$ as a persistent query and $\\mathbf{C}_{t-1}$ as keys and values, ensuring causality.\n\\end{enumerate}\n\nThis architecture ensures predictions are grounded in both the immediate past ($\\mathbf{C}$) and a stable, evolving representation of the current event ($\\mathbf{M}$).\n\n\\subsection{Structured Multi-view Prediction Error (SMPE)}\nInstead of a single scalar error, we define a multi-dimensional error vector $\\mathbf{E}_t \\in \\mathbb{R}^3$ that captures different facets of prediction inconsistency, yielding a more robust boundary detection signal.\n\n\\begin{enumerate}\n    \\item \\textbf{Feature Reconstruction Error ($E^{(r)}_t$)}: The primary discrepancy, analogous to the original $\\epsilon_t$, but computed in the projected space for stability.\n    \\[\n    E^{(r)}_t = 1 - \\frac{\\mathbf{h}_t \\cdot \\mathbf{z}_t}{\\|\\mathbf{h}_t\\| \\|\\mathbf{z}_t\\|}\n    \\]\n\n    \\item \\textbf{Predictive Smoothness Error ($E^{(s)}_t$)}: Measures inconsistency between consecutive predictions. Within a consistent event, predictions should vary smoothly.\n    \\[\n    E^{(s)}_t = \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|_2 - \\beta \\cdot \\|\\mathbf{h}_t - \\mathbf{h}_{t-1}\\|_2\n    \\]\n    where $\\beta$ is a scaling factor. This term becomes large only when predictions are jagged despite smooth actual changes.\n\n    \\item \\textbf{Memory Perturbation Error ($E^{(m)}_t$)}: Quantifies the impact of the new frame on the main event memory, reflecting a potential high-level schema violation.\n    \\[\n    E^{(m)}_t = \\|\\mathbf{M}_t - \\mathbf{M}_{t-1}\\|_1\n    \\]\n\\end{enumerate}\n\nThe final SMPE vector is the concatenation: $\\mathbf{E}_t = [E^{(r)}_t, E^{(s)}_t, E^{(m)}_t]^\\top$. This structured error is more informative and less noisy than a scalar counterpart.\n\n\\subsection{Training with Structural Consistency Loss (SCL)}\nThe HMT-CEA is trained by minimizing a novel \\textbf{Structural Consistency Loss (SCL)}, which jointly optimizes prediction accuracy and the temporal structural properties implied by EST.\n\n\\subsubsection{Boundary-Aware Focal Reconstruction Loss}\nWe adapt binary cross-entropy to address class imbalance and hard examples using the primary error $E^{(r)}_t$:\n\\[\n\\mathcal{L}_{Focal}(E^{(r)}_t, y_t) = - \\alpha (1 - p_t)^\\gamma y_t \\log(p_t) - (1 - \\alpha) p_t^\\gamma (1 - y_t) \\log(1 - p_t)\n\\]\nwhere $p_t = \\text{sigmoid}(-\\lambda \\cdot E^{(r)}_t + b)$, $\\lambda$ and $b$ are learnable parameters, $y_t$ is the boundary label, $\\alpha$ balances class frequency, and $\\gamma$ focuses on hard examples.\n\n\\subsubsection{Temporal Coherence Regularization}\nThis unsupervised loss enforces two structural properties:\n\\begin{itemize}\n    \\item \\textbf{Intra-event Prediction Smoothness}: Within non-boundary segments ($y_t=0$), predictions should be smooth:\n    \\[\n    \\mathcal{L}_{Smooth} = \\frac{1}{|\\mathcal{N}|} \\sum_{t \\in \\mathcal{N}} \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|^2_2\n    \\]\n    where $\\mathcal{N}$ is the set of non-boundary frames.\n\n    \\item \\textbf{Intra-event Feature Clustering}: All predictions within a ground-truth event segment should cluster in embedding space:\n    \\[\n    \\mathcal{L}_{Cluster} = \\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{|I_s|} \\sum_{i \\in I_s} \\|\\mathbf{z}_i - \\boldsymbol{\\mu}_s\\|^2_2\n    \\]\n    where $S$ is the number of events in a clip, $I_s$ is the set of frame indices in event $s$, and $\\boldsymbol{\\mu}_s$ is the mean of $\\mathbf{z}_i$ for $i \\in I_s$.\n\\end{itemize}\n\nThe total SCL is: $\\mathcal{L}_{SCL} = \\mathcal{L}_{Focal} + \\lambda_s \\mathcal{L}_{Smooth} + \\lambda_c \\mathcal{L}_{Cluster}$.\n\n\\subsection{Adaptive Multi-scale Temporal Discriminator (AMTD)}\nThe AMTD replaces the heuristic online boundary detector (OBD) with a lightweight temporal convolutional network (TCN) that learns to detect boundaries from the sequence of SMPE vectors $\\{\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t\\}$.\n\n\\subsubsection{Architecture}\nThe AMTD, denoted $\\mathcal{D}_\\phi$, comprises:\n\\begin{enumerate}\n    \\item \\textbf{Multi-scale Feature Extraction}: Parallel 1D convolutions with kernel sizes $k_1, k_2, k_3$ capture short-, medium-, and long-range temporal patterns in $\\mathbf{E}_t$:\n    \\[\n    \\mathbf{s}^{(i)}_t = \\text{ReLU}(\\text{Conv1D}_{k_i}([\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t]))\n    \\]\n\n    \\item \\textbf{Dynamic Fusion}: Multi-scale features are concatenated and weighted adaptively via self-attention:\n    \\[\n    \\mathbf{S}_t = [\\mathbf{s}^{(1)}_t, \\mathbf{s}^{(2)}_t, \\mathbf{s}^{(3)}_t], \\quad \\mathbf{A}_t = \\text{SelfAttention}(\\mathbf{S}_t)\n    \\]\n\n    \\item \\textbf{Boundary Probability Estimation}: The attended feature produces the final online boundary probability:\n    \\[\n    P_t^{boundary} = \\sigma(\\text{MLP}(\\text{Pool}(\\mathbf{A}_t)))\n    \\]\n    A boundary is declared if $P_t^{boundary} > 0.5$.\n\\end{enumerate}\n\n\\subsubsection{Online Operation and Training}\nDuring online inference, AMTD slides over the streaming SMPE vectors using a FIFO buffer of size $\\Delta+1$. It is trained jointly with the HMT-CEA via SCL, where binary labels $y_t$ supervise $P_t^{boundary}$ through a standard cross-entropy loss, forming a fully differentiable, end-to-end optimizable system for online granular event boundary detection (On-GEBD).\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Setup}\n\\label{sec:setup}\n\n\\subsubsection{Benchmark Datasets.}\nWe evaluate our method on two established benchmarks for generic event boundary detection (GEBD). The \\textbf{Kinetics-GEBD} dataset is derived from the Kinetics-400 dataset, comprising approximately 60K videos. Each video is annotated with taxonomy-free event boundaries, containing about five events on average. The dataset is officially split into training (18,794 videos), validation (18,813 videos), and test (17,725 videos) sets. As test set annotations are withheld, we report results on the validation set, consistent with prior work. For cross-validation in certain analyses, we further split the training data into 80\\% for training and 20\\% for validation. The \\textbf{TAPOS} dataset consists of Olympic sports videos originally annotated for action localization. Following the standard adaptation for GEBD, we repurpose it by discarding action labels and treating sub-action boundaries as generic event boundaries. This results in 13,094 training and 1,790 validation action instances.\n\n\\subsubsection{Evaluation Metric.}\nWe adopt the standard Relative Distance (Rel.Dis) metric for GEBD. For a predicted boundary timestamp and its corresponding ground truth, the relative distance is computed as their temporal offset divided by the duration of the union of the predicted and ground-truth event instances. Following common practice, when multiple consecutive frames are predicted as boundaries, the central frame is taken as the final timestamp. Performance is evaluated under ten Rel.Dis thresholds from 0.05 to 0.5 with a step size of 0.05. A prediction is considered correct if its Rel.Dis is below a given threshold. We report the F1 score at each threshold and the average F1 score across all thresholds.\n\n\\subsubsection{Baselines.}\nEstablishing baselines for Online GEBD (On-GEBD) is non-trivial as standard offline GEBD methods are not designed for streaming input. Therefore, we adapt state-of-the-art models from related online video understanding tasks. From Online Action Detection, we adapt TeSTra, OadTR, and MiniROAD. From Online Temporal Action Localization, we adapt Sim-On. For a fair comparison, we modify only the final layer of each model to perform binary classification (boundary vs. non-boundary), denoted with the suffix '-BC'. We also compare against a dedicated online GEBD method, \\textbf{ESTimator}. Furthermore, to contextualize our online performance, we include comparisons with leading offline GEBD methods.\n\n\\subsubsection{Implementation Details.}\nOur proposed method, \\textbf{ESTimator++}, is implemented using standard video features for fair comparison. Videos are sampled at 24 FPS for Kinetics-GEBD and 6 FPS for TAPOS. We use 2048-dimensional features extracted by an ImageNet-pretrained ResNet-50 encoder. The Hierarchical Memory-augmented Transformer (HMT) employs a main memory with dimension $d_m=512$ and a local context cache of size $L=16$. The Adaptive Multi-scale Temporal Discriminator (AMTD) uses temporal convolutional kernels of sizes $[3, 7, 15]$. The model is trained end-to-end for 30 epochs using the AdamW optimizer with a learning rate of $1\\times10^{-4}$, a batch size of 512, and loss weights $\\lambda_s=0.1$ and $\\lambda_c=0.05$.\n\n\\subsection{Main Results}\n\n\\subsubsection{Comparison with Online Baselines}\nTable~\\ref{tab:online_k400} and Table~\\ref{tab:online_tapos} present the online detection performance on the Kinetics-GEBD and TAPOS validation sets, respectively.\n\nOn Kinetics-GEBD (Table~\\ref{tab:online_k400}), \\textbf{ESTimator++} sets a new state-of-the-art, outperforming all online baselines across all Rel.Dis thresholds. Traditional online models like TeSTra-BC and Sim-On-BC show limited effectiveness for this task. While MiniROAD-BC and the prior dedicated method ESTimator perform better, our method achieves a superior average F1 score of \\textbf{0.778}. This represents an improvement of +3.0\\% over ESTimator and +9.7\\% over MiniROAD-BC, demonstrating the efficacy of our proposed architectural components.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the Kinetics-GEBD validation set. 'BC' denotes a binary classifier head. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_k400}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.438 & 0.488 & 0.521 & 0.545 & 0.564 & 0.580 & 0.593 & 0.604 & 0.614 & 0.622 & 0.557 \\\\\nSim-On-BC & 0.461 & 0.534 & 0.579 & 0.610 & 0.633 & 0.651 & 0.664 & 0.675 & 0.685 & 0.692 & 0.618 \\\\\nOadTR-BC & 0.474 & 0.512 & 0.535 & 0.552 & 0.565 & 0.575 & 0.583 & 0.590 & 0.596 & 0.601 & 0.558 \\\\\nMiniROAD-BC & 0.569 & 0.622 & 0.649 & 0.675 & 0.691 & 0.704 & 0.714 & 0.722 & 0.729 & 0.735 & 0.681 \\\\\nESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.642} & \\textbf{0.712} & \\textbf{0.751} & \\textbf{0.775} & \\textbf{0.790} & \\textbf{0.802} & \\textbf{0.810} & \\textbf{0.816} & \\textbf{0.821} & \\textbf{0.825} & \\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nA similar trend is observed on TAPOS (Table~\\ref{tab:online_tapos}). \\textbf{ESTimator++} achieves the best average F1 score of \\textbf{0.569}, outperforming ESTimator by +2.2\\% and MiniROAD-BC by +4.1\\%. The advantage is particularly notable at stricter thresholds (e.g., 0.05, 0.1), indicating more precise boundary localization. The consistent superiority across both datasets underscores the robustness and generalizability of our approach.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the TAPOS validation set. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_tapos}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.364 & 0.417 & 0.452 & 0.478 & 0.496 & 0.511 & 0.523 & 0.533 & 0.542 & 0.550 & 0.487 \\\\\nSim-On-BC & 0.225 & 0.269 & 0.303 & 0.329 & 0.350 & 0.367 & 0.381 & 0.394 & 0.405 & 0.415 & 0.344 \\\\\nOadTR-BC & 0.263 & 0.319 & 0.361 & 0.394 & 0.422 & 0.445 & 0.465 & 0.483 & 0.497 & 0.510 & 0.416 \\\\\nMiniROAD-BC & 0.422 & 0.472 & 0.502 & 0.522 & 0.537 & 0.549 & 0.558 & 0.566 & 0.572 & 0.578 & 0.528 \\\\\nESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.418} & \\textbf{0.484} & \\textbf{0.531} & \\textbf{0.566} & \\textbf{0.592} & \\textbf{0.613} & \\textbf{0.629} & \\textbf{0.642} & \\textbf{0.653} & \\textbf{0.663} & \\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Comparison with Offline Methods}\nWe further compare our online method against powerful offline models that utilize the full video context. Results are shown in Table~\\ref{tab:offline_k400} for Kinetics-GEBD and Table~\\ref{tab:offline_tapos} for TAPOS.\n\nRemarkably, on Kinetics-GEBD, \\textbf{ESTimator++} with an average F1 of \\textbf{0.778} not only leads all online methods but also competes favorably with state-of-the-art offline approaches. It surpasses all offline supervised methods except for PC and the unsupervised CoSeg. It is important to note that PC and CoSeg benefit from full-video processing and, in CoSeg's case, cross-video co-segmentation. The fact that our \\textit{online} method achieves performance comparable to sophisticated offline models like TCN highlights the efficacy of its design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on Kinetics-GEBD. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_k400}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & BMN & 0.186 & 0.204 & 0.213 & 0.220 & 0.226 & 0.230 & 0.233 & 0.237 & 0.239 & 0.241 & 0.223 \\\\\nOffline & Supervised & BMN-StartEnd & 0.491 & 0.589 & 0.627 & 0.648 & 0.660 & 0.668 & 0.674 & 0.678 & 0.681 & 0.683 & 0.640 \\\\\nOffline & Supervised & TCN-TAPOS & 0.464 & 0.560 & 0.602 & 0.628 & 0.645 & 0.659 & 0.669 & 0.676 & 0.682 & 0.687 & 0.627 \\\\\nOffline & Supervised & TCN & 0.588 & 0.657 & 0.679 & 0.691 & 0.698 & 0.703 & 0.706 & 0.708 & 0.710 & 0.712 & 0.685 \\\\\nOffline & Supervised & \\textbf{PC} & 0.625 & \\textbf{0.758} & \\textbf{0.804} & \\textbf{0.829} & \\textbf{0.844} & \\textbf{0.853} & \\textbf{0.859} & \\textbf{0.864} & \\textbf{0.867} & \\textbf{0.870} & \\textbf{0.817} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.275 & 0.300 & 0.312 & 0.319 & 0.324 & 0.327 & 0.330 & 0.332 & 0.334 & 0.335 & 0.318 \\\\\nOffline & Unsupervised & PA-Random & 0.336 & 0.435 & 0.484 & 0.512 & 0.529 & 0.541 & 0.548 & 0.554 & 0.558 & 0.561 & 0.506 \\\\\nOffline & Unsupervised & PA & 0.396 & 0.488 & 0.520 & 0.534 & 0.544 & 0.550 & 0.555 & 0.558 & 0.561 & 0.564 & 0.527 \\\\\nOffline & Unsupervised & \\underline{CoSeg} & \\textbf{0.656} & 0.758 & 0.783 & 0.794 & 0.799 & 0.803 & 0.804 & 0.806 & 0.807 & 0.809 & 0.782 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.712} & $\\dagger$\\textbf{0.751} & $\\dagger$\\textbf{0.775} & $\\dagger$\\textbf{0.790} & $\\dagger$\\textbf{0.802} & $\\dagger$\\textbf{0.810} & $\\dagger$\\textbf{0.816} & $\\dagger$\\textbf{0.821} & $\\dagger$\\textbf{0.825} & $\\dagger$\\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\nOn the TAPOS dataset (Table~\\ref{tab:offline_tapos}), \\textbf{ESTimator++} again demonstrates strong competitiveness, achieving an average F1 of \\textbf{0.569}. It outperforms all offline supervised baselines except PC, and shows superior performance compared to the unsupervised PA method, especially at stricter thresholds. This consistent ability to rival many offline models underscores a key advantage: \\textbf{ESTimator++} delivers high-quality boundary detection with minimal latency, making it suitable for real-time applications.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on TAPOS. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_tapos}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & ISBA & 0.106 & 0.170 & 0.227 & 0.265 & 0.298 & 0.326 & 0.348 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & TCN & 0.237 & 0.312 & 0.331 & 0.339 & 0.342 & 0.344 & 0.347 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & CTM & 0.244 & 0.312 & 0.336 & 0.351 & 0.361 & 0.369 & 0.374 & 0.381 & 0.383 & 0.385 & 0.350 \\\\\nOffline & Supervised & TransParser & 0.289 & 0.381 & 0.435 & 0.475 & 0.500 & 0.514 & 0.527 & 0.534 & 0.540 & 0.545 & 0.474 \\\\\nOffline & Supervised & \\textbf{PC} & \\textbf{0.522} & \\textbf{0.595} & \\textbf{0.628} & \\textbf{0.646} & \\textbf{0.659} & \\textbf{0.665} & \\textbf{0.671} & \\textbf{0.676} & \\textbf{0.679} & \\textbf{0.683} & \\textbf{0.642} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.035 & 0.045 & 0.047 & 0.051 & 0.053 & 0.054 & 0.055 & 0.056 & 0.057 & 0.058 & 0.051 \\\\\nOffline & Unsupervised & PA-Random & 0.158 & 0.233 & 0.273 & 0.310 & 0.331 & 0.347 & 0.357 & 0.369 & 0.376 & 0.384 & 0.314 \\\\\nOffline & Unsupervised & \\underline{PA} & 0.360 & 0.459 & 0.507 & 0.543 & 0.567 & 0.579 & 0.592 & 0.601 & 0.609 & 0.615 & 0.543 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.418} & $\\dagger$\\textbf{0.484} & $\\dagger$\\textbf{0.531} & $\\dagger$\\textbf{0.566} & $\\dagger$\\textbf{0.592} & $\\dagger$\\textbf{0.613} & $\\dagger$\\textbf{0.629} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.653} & $\\dagger$\\textbf{0.663} & $\\dagger$\\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\n\\subsection{Model Efficiency}\nWhile \\textbf{ESTimator++} introduces more sophisticated components, its design prioritizes online efficiency. We profile the average inference time per frame on a single NVIDIA V100 GPU. As shown in Table~\\ref{tab:efficiency}, \\textbf{ESTimator++} processes frames at \\textbf{8.2 ms (∼122 FPS)}, which is significantly faster than the real-time requirement of 24 FPS for the Kinetics dataset. Although it is understandably slower than the simpler ESTimator and some lightweight baselines like TeSTra-BC, its runtime is still an order of magnitude lower than the total latency of offline methods, which must process entire clips. This demonstrates that \\textbf{ESTimator++} successfully balances state-of-the-art accuracy with practical online inference speed.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison of inference efficiency (lower time per frame is better). Offline methods process full clips; their reported time is the total latency per clip.}\n\\label{tab:efficiency}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Method} & \\textbf{Time per Frame (ms)} \\\\\n\\midrule\n\\textbf{Online Methods} \\\\\nTeSTra-BC & 2.1 \\\\\nMiniROAD-BC & 5.7 \\\\\nESTimator & 6.5 \\\\\n\\textbf{ESTimator++ (Ours)} & \\textbf{8.2} \\\\\n\\midrule\n\\textbf{Offline Methods (Full Clip)} \\\\\nTCN & 1520* \\\\\nPC & 1850* \\\\\n\\bottomrule\n\\multicolumn{2}{l}{*Total processing time for a 10-second clip, not per frame.}\n\\end{tabular}\n\\end{table}\n\n\\subsection{Qualitative Analysis}\nQualitative comparisons reveal that predictions from \\textbf{ESTimator++} exhibit sharper and more confident peaks at genuine event boundaries while showing greater suppression of spurious fluctuations within event segments, compared to both the TeSTra-BC baseline and its predecessor ESTimator. This can be attributed to the Structured Multi-view Prediction Error (SMPE) providing a cleaner signal and the Adaptive Multi-scale Temporal Discriminator (AMTD) learning to distinguish true boundary patterns from noise. Furthermore, in sequences with gradual transitions, \\textbf{ESTimator++} shows improved temporal localization, likely due to the multi-scale receptive field of the AMTD. These observations align with its superior quantitative performance.\n\n\\section{Ablation Studies}\n\\subsection{Ablation Study}\n\\label{sec:ablation}\n\nWe conduct a comprehensive ablation study on the Kinetics-GEBD validation set to validate the contribution of each component in \\textbf{ESTimator++}. All experiments follow the implementation details in Section~\\ref{sec:setup}.\n\n\\subsubsection{Impact of Proposed Components}\nWe start from a baseline model that uses a standard Transformer decoder for prediction and a single reconstruction error with a fixed heuristic threshold for detection. This baseline achieves an average F1 score of 0.715. We then sequentially integrate our proposed components, with results summarized in Table~\\ref{tab:ablation_components}.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation study on the contribution of each proposed component. Evaluated on Kinetics-GEBD validation set (Average F1). $\\Delta$ denotes the improvement over the previous row.}\n\\label{tab:ablation_components}\n\\begin{tabular}{lcccccc}\n\\toprule\n\\textbf{Variant} & \\textbf{HMT} & \\textbf{SMPE} & \\textbf{SCL} & \\textbf{AMTD} & \\textbf{Avg F1} & $\\Delta$ \\\\\n\\midrule\nBaseline (ESTimator Core) & & & & & 0.715 & -- \\\\\n+ Hierarchical Memory (HMT) & \\checkmark & & & & 0.733 & +0.018 \\\\\n+ Structured Error (SMPE) & \\checkmark & \\checkmark & & & 0.754 & +0.021 \\\\\n+ Consistency Loss (SCL) & \\checkmark & \\checkmark & \\checkmark & & 0.769 & +0.015 \\\\\n+ Adaptive Discriminator (AMTD) & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\textbf{0.778} & \\textbf{+0.029} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nIntegrating the \\textbf{Hierarchical Memory-augmented Transformer (HMT)} improves performance by +1.8\\%, demonstrating the importance of maintaining stable event context for consistent prediction. Replacing the single error with the \\textbf{Structured Multi-view Prediction Error (SMPE)} yields a further gain of +2.1\\%, confirming that a composite error signal provides richer boundary cues. Employing the \\textbf{Structural Consistency Loss (SCL)} for training adds another +1.5\\%, validating that enforcing intra-event smoothness and feature clustering shapes more discriminative representations. Finally, substituting the heuristic threshold with the learned \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} provides the most substantial single-module improvement of +2.9\\%, underscoring the advantage of a data-driven decision function. The full model integrating all components achieves the best performance of \\textbf{0.778}, confirming their synergistic effect.\n\n\\subsubsection{Analysis of the Structured Multi-view Prediction Error (SMPE)}\nWe analyze the contribution of each error view within the SMPE by ablating them individually from the full model. Results are presented in Table~\\ref{tab:ablation_smpe}. Removing the \\textbf{Predictive Smoothness Error} causes the most significant drop (-1.7\\% in Avg F1), highlighting its sensitivity to event transitions. Omitting the \\textbf{Memory Perturbation Error} leads to a -1.2\\% decrease, demonstrating the importance of tracking high-level event schema changes. Removing the primary \\textbf{Feature Reconstruction Error} also degrades performance (-1.0\\%). Using any single error view consistently underperforms compared to the full SMPE vector, validating our composite design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structured Multi-view Prediction Error (SMPE). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_smpe}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Error Components Used} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full SMPE} ($E^{(r)}, E^{(s)}, E^{(m)}$) & \\textbf{0.778} \\\\\n\\hline\n$E^{(s)}, E^{(m)}$ (w/o $E^{(r)}$) & 0.768 \\\\\n$E^{(r)}, E^{(m)}$ (w/o $E^{(s)}$) & 0.761 \\\\\n$E^{(r)}, E^{(s)}$ (w/o $E^{(m)}$) & 0.766 \\\\\n\\hline\nOnly $E^{(r)}$ & 0.756 \\\\\nOnly $E^{(s)}$ & 0.749 \\\\\nOnly $E^{(m)}$ & 0.738 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Design Choices for the Adaptive Discriminator (AMTD)}\nWe analyze key design choices in the AMTD module. Results are shown in Table~\\ref{tab:ablation_amtd}. Replacing the AMTD with the original heuristic Online Boundary Detector (OBD) causes a performance drop to 0.748. Using a \\textbf{single-scale} Temporal Convolutional Network (TCN) instead of the multi-scale architecture reduces Avg F1 to 0.771. Removing the \\textbf{self-attention based dynamic fusion} and simply concatenating multi-scale features results in a score of 0.774. These ablations confirm that each aspect of the AMTD's design—its learned nature, multi-scale perception, and adaptive fusion—contributes to its effectiveness.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the design of the Adaptive Multi-scale Temporal Discriminator (AMTD). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_amtd}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{AMTD Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full AMTD (Ours)} & \\textbf{0.778} \\\\\n\\hline\nReplace with ESTimator's OBD & 0.748 \\\\\nSingle-scale TCN (kernel=7) & 0.771 \\\\\nMulti-scale TCN w/o Self-Attention Fusion & 0.774 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Utility of the Structural Consistency Loss (SCL)}\nWe dissect the contribution of the two regularization terms in the SCL. As shown in Table~\\ref{tab:ablation_loss}, training with only the \\textbf{Focal Reconstruction Loss} yields a baseline score of 0.754. Adding the \\textbf{Intra-event Smoothness Loss} improves performance to 0.766. Further incorporating the \\textbf{Intra-event Clustering Loss} to form the full SCL pushes the score to 0.769. The full SCL, combining both regularizers, delivers the best result, confirming that jointly optimizing for local accuracy and global temporal coherence is an effective strategy.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structural Consistency Loss (SCL). The HMT and SMPE are used in all variants. Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_loss}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Training Loss Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n$\\mathcal{L}_{Focal}$ only & 0.754 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth}$ & 0.766 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Cluster}$ & 0.762 \\\\\n\\textbf{Full SCL} ($\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth} + \\mathcal{L}_{Cluster}$) & \\textbf{0.769} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\textbf{Summary.} Our ablation studies provide compelling evidence for the necessity and effectiveness of every component in \\textbf{ESTimator++}. The hierarchical memory (HMT) establishes a robust predictive foundation, the structured error (SMPE) extracts optimal boundary cues, the consistency loss (SCL) shapes the learning objective, and the adaptive discriminator (AMTD) makes precise online decisions. The full model's superior performance arises from their seamless integration.\n\n\\section{Evaluation}\n\\subsection{Training Dynamics and Convergence Analysis}\nTo gain deeper insights into the optimization behavior of \\textbf{ESTimator++}, we analyze its training dynamics. The training and validation loss curves over 30 epochs on the Kinetics-GEBD dataset show that \\textbf{ESTimator++} converges more smoothly and stably to a lower final loss compared to its predecessor \\textbf{ESTimator} and the strong baseline \\textbf{MiniROAD-BC}. This indicates that the Structural Consistency Loss (SCL) provides a more stable gradient signal. Furthermore, the gap between training and validation loss for \\textbf{ESTimator++} remains consistently narrow, suggesting strong generalization and reduced overfitting, a benefit likely attributable to the regularizing effects of $\\mathcal{L}_{Smooth}$ and $\\mathcal{L}_{Cluster}$. In contrast, the baseline models exhibit more pronounced fluctuations and a larger generalization gap.\n\n\\subsection{In-depth Case Study}\nWe conduct a systematic case study to examine \\textbf{ESTimator++}'s performance across diverse scenarios. We categorize sequences from the Kinetics-GEBD validation set into groups such as \\textit{‘fast-paced action transitions’}, \\textit{‘gradual scene changes’}, and \\textit{‘static periods with subtle motions’}. For each category, we compare the SMPE signal and final boundary predictions from \\textbf{ESTimator++} against those from \\textbf{ESTimator} and \\textbf{TeSTra-BC}. In fast-paced sequences, \\textbf{ESTimator++}'s AMTD demonstrates superior ability to isolate closely spaced peaks, leading to precise detection of sequential boundaries. For gradual transitions, the multi-view SMPE provides a rising error signal over several frames, which the multi-scale AMTD effectively integrates to produce a confident prediction. During static periods, \\textbf{ESTimator++} exhibits significantly lower spurious activation due to the intra-event smoothness constraint. These observations concretely explain the quantitative improvements shown in the main results.\n\n\\section{Conclusion}\nThis paper addresses the task of Online Generic Event Boundary Detection (On-GEBD). We propose \\textbf{ESTimator++}, an enhanced framework that provides a deeper computational implementation of the Event Segmentation Theory (EST). It integrates three key innovations: a Hierarchical Memory-augmented Transformer (HMT) for robust event anticipation, a Structured Multi-view Prediction Error (SMPE) for richer boundary cues, and an Adaptive Multi-scale Temporal Discriminator (AMTD) for learning boundary decisions. These components are trained end-to-end using a novel Structural Consistency Loss (SCL).\n\nExtensive experiments demonstrate the effectiveness of our method. On the Kinetics-GEBD dataset, \\textbf{ESTimator++} achieves a state-of-the-art average F1 score of \\textbf{0.778}, significantly outperforming all existing online baselines, including its predecessor ESTimator (0.748). Similarly, on the TAPOS dataset, it attains a leading average F1 score of \\textbf{0.569}. Notably, our online method performs competitively against many powerful offline models, securing the third-highest overall ranking on both benchmarks. Ablation studies confirm the contributions of each component, with AMTD yielding the most substantial gain as a single module. Furthermore, \\textbf{ESTimator++} maintains a practical inference time of 8.2ms per frame, making it suitable for real-time processing.\n\nIn summary, \\textbf{ESTimator++} establishes a new state-of-the-art for On-GEBD by effectively translating cognitive principles into a synergistic neural architecture. It delivers highly accurate boundary detection with low latency, paving the way for more sophisticated real-time video understanding systems. Future work could explore extending this framework to more complex, long-form video streams and multi-modal inputs.\n\n\\end{document}\n\nPlease generate a professional image generation prompt according to the figure type."
          }
        ]
      },
      "response": {
        "raw_response": "Create an abstract conceptual diagram illustrating the overall architecture of the ESTimator++ framework for Online Generic Event Boundary Detection, suitable for a top-tier academic conference paper like NeurIPS or CVPR. The diagram should be a high-level, minimalist line drawing in a clean, professional style with abstract modules represented as simple rectangles or rounded boxes, connected by straight arrows to show conceptual data and information flows, emphasizing logical relationships and high-level concepts rather than specific steps or flowchart symbols—explicitly avoid any flowchart elements like diamonds, ovals, or process boxes. Use a white background with subtle grid lines for alignment, ensuring the layout is horizontal and flows logically from left to right for clarity and readability.\n\nOn the left, depict an input module labeled \"Video Frame Stream\" as an abstract vertical stack of small rectangles representing frames (f_{t-Δ} to f_t), connected by an orange arrow to a central module labeled \"Hierarchical Memory-augmented Transformer (HMT)\" shown as a large abstract box divided into two sub-sections: an upper sub-box for \"Main Event Memory (M)\" (with a persistent icon like a stable cloud) and a lower sub-box for \"Local Context Cache (C)\" (with a queue-like abstract line). From HMT, draw an orange arrow downward to a small box labeled \"Next-Frame Prediction (ẑ_t, ˆf_t)\", indicating predictive output.\n\nFrom the prediction box, connect a green arrow to the right to a module labeled \"Structured Multi-view Prediction Error (SMPE)\" as an abstract triangular shape converging three inputs: label them as \"Feature Reconstruction Error (E^r)\", \"Predictive Smoothness Error (E^s)\", and \"Memory Perturbation Error (E^m)\", with the output as a vector E_t shown as a small bracketed array.\n\nFrom SMPE, draw a green arrow to the right to a module labeled \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" depicted as a layered abstract structure with three parallel bars representing multi-scale convolutions (short, medium, long kernels), fused into a single output box labeled \"Boundary Probability (P_t^{boundary})\" with a sigmoid icon.\n\nOverlay a dashed blue bounding box around the entire pipeline (HMT, SMPE, AMTD) labeled \"End-to-End Training with Structural Consistency Loss (SCL)\", with abstract icons for loss components (e.g., a focal loss symbol, smoothness curve, and clustering dots) connected via thin red lines to relevant modules to indicate supervision.\n\nUse orange arrows for data input and processing flows (e.g., frame to prediction stages), green arrows for feedback and output flows (e.g., error to discriminator stages), creating visual hierarchy; text labels in black Arial font for main modules, blue for inputs/outputs, and red for loss-related annotations; keep the design simple, balanced, and not overcrowded, with sufficient white space for academic presentation.",
        "usage": {
          "input_tokens": 12274,
          "output_tokens": 930
        },
        "raw_reasoning_content": null
      }
    },
    {
      "request": {
        "messages": [
          {
            "role": "system",
            "content": "\n你是一位专业的学术图表设计专家。你的任务是将论文内容描述转换为适合 AI 图像生成的详细提示词。\n\n你的职责：\n1. 分析输入的论文内容描述\n2. 根据所需的图表类型（算法图或动机图）调整描述风格\n3. 生成详细且专业的图像生成提示词，确保生成的图像符合顶级学术会议（NeurIPS、ICML、ICLR、CVPR、AAAI 等）的发表标准\n\n图表类型规范：\n- **算法图**：用于展示算法流程、模型架构、方法框架等。必须强调：\n  * **抽象性和概念性**：不要绘制成具体的流程图，而应使用抽象概念图、架构图或框架图\n  * 清晰的结构和模块关系（用抽象模块表示，而非具体步骤）\n  * 逻辑流和数据流（用抽象箭头和连接表示，而非流程图符号）\n  * 技术性和专业性\n  * 简洁的视觉设计，避免过于具体的操作步骤\n  * 强调高层次概念和架构，而非详细的执行流程\n  \n- **动机图**：用于展示问题动机、方法对比、概念解释等。必须强调：\n  * 直观的可视化效果\n  * 突出的对比和差异\n  * 易于理解和可视化\n  * 概念的可理解性\n\n提示词生成要求：\n1. 使用英文生成提示词（AI 图像生成模型对英文的理解更好）\n2. 描述应详细且具体，包括视觉元素、布局、样式等\n3. 强调学术论文的专业性和简洁性\n4. 避免过于复杂的细节，保持图表清晰可读且相对饱满，必要时再复杂你自己判断\n5. **颜色和样式要求**（根据图表类型区分）：\n   \n   **对于算法图**：\n   * **整体风格**：采用简洁、极简主义的线条图风格，使用清晰的线条和抽象图形元素\n   * **颜色方案**：使用橙色（orange）和绿色（green）箭头来区分不同的流程阶段和循环路径，文本主要使用黑色、蓝色和红色作为主色调\n   * **视觉层次**：通过颜色区分信息流（如橙色表示数据输入/处理阶段，绿色表示反馈/输出阶段），形成清晰的视觉层次\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   * **箭头和连接**：使用不同颜色的箭头（橙色、绿色）来明确表示信息流向和循环关系\n   其他你自行判断\n   \n   **对于动机图**：\n   * **整体风格**：采用简洁、直观的图风格，使用清晰的视觉元素突出对比和差异\n   * **颜色方案**：使用对比鲜明的颜色来区分不同方法或概念\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   其他你自行判断\n   \n6. 确保图表适合在论文中展示\n7. **对于算法图**：必须强调抽象性和概念性，明确说明\"不是流程图\"和\"抽象概念图\"，避免使用流程图符号（如决策菱形、处理框等），应使用抽象模块和概念连接\n\n输出格式：\n- 仅输出优化后的提示词文本，不要包含任何解释或额外内容\n- 不要用代码块包裹\n- 提示词应是一个完整的描述，可以直接用于图像生成\n"
          },
          {
            "role": "user",
            "content": "Please convert the following paper content description into a detailed prompt suitable for AI image generation.\n\n[Required Figure Type]\nalgorithm\n\n[Paper Content Description]\n\\documentclass{article}\n\\usepackage{amsmath,amssymb}\n\\usepackage{booktabs}\n\n\\begin{document}\n\n\\title{ESTimator++: Structural Consistency Learning for Online Generic Event Boundary Detection}\n\\maketitle\n\\begin{abstract}\nOnline Generic Event Boundary Detection (On-GEBD) is essential for real-time video understanding, yet remains challenging due to the requirement for efficient, single-pass processing. While Event Segmentation Theory (EST) provides a cognitive foundation, its computational implementation in prior online methods such as ESTimator is limited. This paper presents \\textbf{ESTimator++}, a significantly enhanced framework that advances the original through a more systematic instantiation of EST. Our key contributions are threefold: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation; (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer boundary signals; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} which replaces heuristic thresholding with a learned decision function. These components are integrated via a novel \\textbf{Structural Consistency Loss (SCL)}, enabling end-to-end optimization. Extensive experiments on Kinetics-GEBD and TAPOS benchmarks demonstrate that ESTimator++ achieves state-of-the-art performance for On-GEBD, with average F1 scores of \\textbf{0.778} and \\textbf{0.569}, respectively, outperforming all existing online methods and competing favorably with many offline models. Ablation studies confirm the contribution of each component, with the AMTD providing the most significant single-module gain (+2.9\\%). Remarkably, this is achieved while maintaining high efficiency at \\textbf{8.2 ms per frame}. Our work demonstrates that a principled, integrated design substantially advances the performance of online event segmentation.\n\\end{abstract}\n\n\\section{Introduction}\nThe proliferation of long-form video content on platforms like YouTube and Netflix has created a pressing need for advanced video understanding systems. While tasks such as action recognition are well-established, they predominantly focus on short, trimmed clips with predefined labels. In contrast, the analysis of untrimmed, long-form videos containing complex narratives remains a formidable challenge. Generic Event Boundary Detection (GEBD) tackles this issue by aiming to segment videos into taxonomy-free, semantically coherent units, thereby emulating the human ability to parse continuous visual streams.\n\nCurrent GEBD frameworks operate in an \\textit{offline} manner. They process an entire video segment in one pass, leveraging both past and future context to identify all boundaries simultaneously. This paradigm stands in stark contrast to human perception, which segments events \\textit{online}—sequentially and causally, relying solely on past and present information without any access to future content. To bridge this fundamental gap and more faithfully replicate human cognitive processes, we introduce a novel and more demanding task: \\textbf{Online Generic Event Boundary Detection (On-GEBD)}. In this setting, a model must process a video stream and make immediate, frame-by-frame decisions regarding boundary occurrences, using only historical and current data.\n\nOn-GEBD inherits the inherent challenges of offline GEBD, such as detecting subtle and taxonomy-free transitions, but introduces additional complexities due to its strictly causal and information-limited nature. Existing offline GEBD methods, which often depend on global temporal context through mechanisms like similarity matrices or multi-frame comparisons, are fundamentally incompatible with the online setting. Likewise, naïvely adapting models designed for other online action understanding tasks proves ineffective, as they are not tailored for the fine-grained detection of generic boundaries.\n\nTo address these unique challenges, we propose \\textbf{ESTimator++}, an enhanced online framework inspired by Event Segmentation Theory (EST) from cognitive science. EST suggests that humans continuously predict upcoming visual input based on an ongoing event model and perceive a boundary when a significant prediction error arises. Our framework operationalizes this principle through three core innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer} that maintains a stable representation of the current event for robust future anticipation; (2) a \\textbf{Structured Multi-view Prediction Error} mechanism that derives a richer and more stable discrepancy signal; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator}, a lightweight network that replaces heuristic thresholding with a learned, context-aware decision function. These components are integrated under a novel \\textbf{Structural Consistency Loss} that jointly enforces prediction accuracy and temporal coherence.\n\nExtensive experiments on the Kinetics-GEBD and TAPOS benchmarks demonstrate that \\textbf{ESTimator++} sets a new state-of-the-art for On-GEBD. It significantly surpasses all adapted online baselines and, remarkably, achieves performance competitive with many sophisticated offline methods, despite operating under a strict causal constraint. Ablation studies validate the contribution of each proposed component. The remainder of this paper is organized as follows: we review related work, detail our methodology, present comprehensive experimental results, and provide concluding remarks.\n\n\\section{Related Work}\n\\subsection{Generic Event Boundary Detection}\nGeneric Event Boundary Detection (GEBD) aims to identify boundaries that align with human perceptual segmentation within untrimmed videos, operating without a predefined taxonomy of events. This task differs from traditional video understanding tasks such as Temporal Action Localization (TAL) and Action Recognition, as GEBD handles continuous, open-ended semantics. Recent advancements in offline GEBD typically process entire videos or extensive temporal contexts. Representative methods include UBoCo, which leverages temporal similarity matrices with contrastive learning; DDM-Net, which applies progressive attention on multi-level dense difference maps; and CoSeg, which introduces a reconstruction-based objective inspired by cognitive modeling. While these offline methods have shown progress, they are not designed for streaming video. Our work adapts GEBD to the \\textit{online} setting, requiring instantaneous decisions without access to future frames.\n\n\\subsection{Video Understanding}\nVideo understanding encompasses tasks such as Temporal Action Detection (TAD), Temporal Action Localization (TAL), and Video Instance Segmentation (VIS). State-of-the-art approaches often employ transformer architectures, memory mechanisms, and end-to-end frameworks. A common limitation is their reliance on a closed set of predefined action classes, which restricts applicability in open-world scenarios. Although open-set formulations for TAL exist, they focus on localizing and classifying known actions. In contrast, GEBD detects boundaries for taxonomy-free events, providing a more general approach to video structure understanding.\n\n\\subsection{Online Video Understanding}\nOnline video understanding methods process data in a streaming fashion, sharing core challenges with Online GEBD (On-GEBD). Relevant tasks include Online Action Detection (OAD), Online Temporal Action Localization (On-TAL), and Online Detection of Action Start (ODAS). For OAD, recent works such as LSTR, TeSTra, and MiniROAD demonstrate the efficacy of transformers and recurrent units. For On-TAL, methods like CAG-QIL and OAT have been proposed. While ODAS shares an online nature with On-GEBD, it is designed for a fixed set of action categories. Conversely, On-GEBD must detect subtle, categorical changes between taxonomy-free events, necessitating a distinct methodological approach from conventional online action-centric models.\n\n\\section{Method}\nOur enhanced framework, \\textbf{ESTimator++}, extends the original ESTimator by providing a more systematic computational implementation of Event Segmentation Theory (EST). While retaining the core principle of using prediction discrepancy for online boundary detection, ESTimator++ introduces three key innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation, (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer and more stable boundary cues, and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} that replaces heuristic thresholding with a learned, context-sensitive decision function. These components are unified during training via a novel \\textbf{Structural Consistency Loss (SCL)}, which enforces both local prediction accuracy and global temporal coherence.\n\n\\subsection{Hierarchical Memory-augmented Transformer for Consistent Event Anticipation (HMT-CEA)}\nThe original Consistent Event Anticipation (CEA) employs a standard transformer decoder with a limited look-back window, which may fail to maintain a persistent representation of an ongoing event's gist. Inspired by human cognition—which uses working memory for immediate context and long-term memory for schemas—we propose the HMT-CEA. It maintains two complementary memory structures: a dynamic \\textbf{Main Event Memory ($\\mathbf{M}$)} that captures the abstract, persistent essence of the current event, and a \\textbf{Local Context Cache ($\\mathbf{C}$)} that holds fine-grained recent features.\n\n\\subsubsection{Architecture and State Update}\nGiven a stream of frame features $\\mathbf{f}_t \\in \\mathbb{R}^D$, we first project them into a memory-compatible space: $\\mathbf{h}_t = \\text{Linear}(\\mathbf{f}_t)$. The HMT-CEA operates recurrently, maintaining states $(\\mathbf{M}_t, \\mathbf{C}_t)$.\n\n\\begin{enumerate}\n    \\item \\textbf{Local Context Update}: The cache $\\mathbf{C}_t$ is a FIFO queue of the last $L$ projected features: $\\mathbf{C}_t = [\\mathbf{h}_{t-L}, ..., \\mathbf{h}_{t-1}]$.\n\n    \\item \\textbf{Main Memory Gated Update}: The core memory $\\mathbf{M}_t \\in \\mathbb{R}^{d_m}$ is updated via a gated mechanism using the new observation $\\mathbf{h}_t$ and previous context.\n    \\[\n    \\mathbf{g}_t = \\sigma(\\mathbf{W}_g [\\mathbf{M}_{t-1}; \\bar{\\mathbf{C}}_t; \\mathbf{h}_t] + \\mathbf{b}_g)\n    \\]\n    \\[\n    \\tilde{\\mathbf{M}}_t = \\text{TransformerBlock}(\\mathbf{M}_{t-1}, \\mathbf{C}_t, \\mathbf{h}_t)\n    \\]\n    \\[\n    \\mathbf{M}_t = \\mathbf{g}_t \\odot \\tilde{\\mathbf{M}}_t + (1 - \\mathbf{g}_t) \\odot \\mathbf{M}_{t-1}\n    \\]\n    where $\\bar{\\mathbf{C}}_t$ is the mean-pooled cache, $\\sigma$ is the sigmoid function, $\\odot$ denotes element-wise multiplication, and $\\text{TransformerBlock}$ is a lightweight cross-attention module that allows the memory to attend to the local context and new input.\n\n    \\item \\textbf{Next-Frame Prediction}: The anticipated feature for frame $t$ is generated by fusing information from both memory levels:\n    \\[\n    \\mathbf{z}_t = \\text{Decoder}( \\mathbf{M}_{t-1}, \\mathbf{C}_{t-1} )\n    \\]\n    \\[\n    \\hat{\\mathbf{f}}_t = \\text{Linear}(\\mathbf{z}_t)\n    \\]\n    Here, $\\text{Decoder}$ is a small autoregressive transformer that uses $\\mathbf{M}_{t-1}$ as a persistent query and $\\mathbf{C}_{t-1}$ as keys and values, ensuring causality.\n\\end{enumerate}\n\nThis architecture ensures predictions are grounded in both the immediate past ($\\mathbf{C}$) and a stable, evolving representation of the current event ($\\mathbf{M}$).\n\n\\subsection{Structured Multi-view Prediction Error (SMPE)}\nInstead of a single scalar error, we define a multi-dimensional error vector $\\mathbf{E}_t \\in \\mathbb{R}^3$ that captures different facets of prediction inconsistency, yielding a more robust boundary detection signal.\n\n\\begin{enumerate}\n    \\item \\textbf{Feature Reconstruction Error ($E^{(r)}_t$)}: The primary discrepancy, analogous to the original $\\epsilon_t$, but computed in the projected space for stability.\n    \\[\n    E^{(r)}_t = 1 - \\frac{\\mathbf{h}_t \\cdot \\mathbf{z}_t}{\\|\\mathbf{h}_t\\| \\|\\mathbf{z}_t\\|}\n    \\]\n\n    \\item \\textbf{Predictive Smoothness Error ($E^{(s)}_t$)}: Measures inconsistency between consecutive predictions. Within a consistent event, predictions should vary smoothly.\n    \\[\n    E^{(s)}_t = \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|_2 - \\beta \\cdot \\|\\mathbf{h}_t - \\mathbf{h}_{t-1}\\|_2\n    \\]\n    where $\\beta$ is a scaling factor. This term becomes large only when predictions are jagged despite smooth actual changes.\n\n    \\item \\textbf{Memory Perturbation Error ($E^{(m)}_t$)}: Quantifies the impact of the new frame on the main event memory, reflecting a potential high-level schema violation.\n    \\[\n    E^{(m)}_t = \\|\\mathbf{M}_t - \\mathbf{M}_{t-1}\\|_1\n    \\]\n\\end{enumerate}\n\nThe final SMPE vector is the concatenation: $\\mathbf{E}_t = [E^{(r)}_t, E^{(s)}_t, E^{(m)}_t]^\\top$. This structured error is more informative and less noisy than a scalar counterpart.\n\n\\subsection{Training with Structural Consistency Loss (SCL)}\nThe HMT-CEA is trained by minimizing a novel \\textbf{Structural Consistency Loss (SCL)}, which jointly optimizes prediction accuracy and the temporal structural properties implied by EST.\n\n\\subsubsection{Boundary-Aware Focal Reconstruction Loss}\nWe adapt binary cross-entropy to address class imbalance and hard examples using the primary error $E^{(r)}_t$:\n\\[\n\\mathcal{L}_{Focal}(E^{(r)}_t, y_t) = - \\alpha (1 - p_t)^\\gamma y_t \\log(p_t) - (1 - \\alpha) p_t^\\gamma (1 - y_t) \\log(1 - p_t)\n\\]\nwhere $p_t = \\text{sigmoid}(-\\lambda \\cdot E^{(r)}_t + b)$, $\\lambda$ and $b$ are learnable parameters, $y_t$ is the boundary label, $\\alpha$ balances class frequency, and $\\gamma$ focuses on hard examples.\n\n\\subsubsection{Temporal Coherence Regularization}\nThis unsupervised loss enforces two structural properties:\n\\begin{itemize}\n    \\item \\textbf{Intra-event Prediction Smoothness}: Within non-boundary segments ($y_t=0$), predictions should be smooth:\n    \\[\n    \\mathcal{L}_{Smooth} = \\frac{1}{|\\mathcal{N}|} \\sum_{t \\in \\mathcal{N}} \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|^2_2\n    \\]\n    where $\\mathcal{N}$ is the set of non-boundary frames.\n\n    \\item \\textbf{Intra-event Feature Clustering}: All predictions within a ground-truth event segment should cluster in embedding space:\n    \\[\n    \\mathcal{L}_{Cluster} = \\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{|I_s|} \\sum_{i \\in I_s} \\|\\mathbf{z}_i - \\boldsymbol{\\mu}_s\\|^2_2\n    \\]\n    where $S$ is the number of events in a clip, $I_s$ is the set of frame indices in event $s$, and $\\boldsymbol{\\mu}_s$ is the mean of $\\mathbf{z}_i$ for $i \\in I_s$.\n\\end{itemize}\n\nThe total SCL is: $\\mathcal{L}_{SCL} = \\mathcal{L}_{Focal} + \\lambda_s \\mathcal{L}_{Smooth} + \\lambda_c \\mathcal{L}_{Cluster}$.\n\n\\subsection{Adaptive Multi-scale Temporal Discriminator (AMTD)}\nThe AMTD replaces the heuristic online boundary detector (OBD) with a lightweight temporal convolutional network (TCN) that learns to detect boundaries from the sequence of SMPE vectors $\\{\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t\\}$.\n\n\\subsubsection{Architecture}\nThe AMTD, denoted $\\mathcal{D}_\\phi$, comprises:\n\\begin{enumerate}\n    \\item \\textbf{Multi-scale Feature Extraction}: Parallel 1D convolutions with kernel sizes $k_1, k_2, k_3$ capture short-, medium-, and long-range temporal patterns in $\\mathbf{E}_t$:\n    \\[\n    \\mathbf{s}^{(i)}_t = \\text{ReLU}(\\text{Conv1D}_{k_i}([\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t]))\n    \\]\n\n    \\item \\textbf{Dynamic Fusion}: Multi-scale features are concatenated and weighted adaptively via self-attention:\n    \\[\n    \\mathbf{S}_t = [\\mathbf{s}^{(1)}_t, \\mathbf{s}^{(2)}_t, \\mathbf{s}^{(3)}_t], \\quad \\mathbf{A}_t = \\text{SelfAttention}(\\mathbf{S}_t)\n    \\]\n\n    \\item \\textbf{Boundary Probability Estimation}: The attended feature produces the final online boundary probability:\n    \\[\n    P_t^{boundary} = \\sigma(\\text{MLP}(\\text{Pool}(\\mathbf{A}_t)))\n    \\]\n    A boundary is declared if $P_t^{boundary} > 0.5$.\n\\end{enumerate}\n\n\\subsubsection{Online Operation and Training}\nDuring online inference, AMTD slides over the streaming SMPE vectors using a FIFO buffer of size $\\Delta+1$. It is trained jointly with the HMT-CEA via SCL, where binary labels $y_t$ supervise $P_t^{boundary}$ through a standard cross-entropy loss, forming a fully differentiable, end-to-end optimizable system for online granular event boundary detection (On-GEBD).\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Setup}\n\\label{sec:setup}\n\n\\subsubsection{Benchmark Datasets.}\nWe evaluate our method on two established benchmarks for generic event boundary detection (GEBD). The \\textbf{Kinetics-GEBD} dataset is derived from the Kinetics-400 dataset, comprising approximately 60K videos. Each video is annotated with taxonomy-free event boundaries, containing about five events on average. The dataset is officially split into training (18,794 videos), validation (18,813 videos), and test (17,725 videos) sets. As test set annotations are withheld, we report results on the validation set, consistent with prior work. For cross-validation in certain analyses, we further split the training data into 80\\% for training and 20\\% for validation. The \\textbf{TAPOS} dataset consists of Olympic sports videos originally annotated for action localization. Following the standard adaptation for GEBD, we repurpose it by discarding action labels and treating sub-action boundaries as generic event boundaries. This results in 13,094 training and 1,790 validation action instances.\n\n\\subsubsection{Evaluation Metric.}\nWe adopt the standard Relative Distance (Rel.Dis) metric for GEBD. For a predicted boundary timestamp and its corresponding ground truth, the relative distance is computed as their temporal offset divided by the duration of the union of the predicted and ground-truth event instances. Following common practice, when multiple consecutive frames are predicted as boundaries, the central frame is taken as the final timestamp. Performance is evaluated under ten Rel.Dis thresholds from 0.05 to 0.5 with a step size of 0.05. A prediction is considered correct if its Rel.Dis is below a given threshold. We report the F1 score at each threshold and the average F1 score across all thresholds.\n\n\\subsubsection{Baselines.}\nEstablishing baselines for Online GEBD (On-GEBD) is non-trivial as standard offline GEBD methods are not designed for streaming input. Therefore, we adapt state-of-the-art models from related online video understanding tasks. From Online Action Detection, we adapt TeSTra, OadTR, and MiniROAD. From Online Temporal Action Localization, we adapt Sim-On. For a fair comparison, we modify only the final layer of each model to perform binary classification (boundary vs. non-boundary), denoted with the suffix '-BC'. We also compare against a dedicated online GEBD method, \\textbf{ESTimator}. Furthermore, to contextualize our online performance, we include comparisons with leading offline GEBD methods.\n\n\\subsubsection{Implementation Details.}\nOur proposed method, \\textbf{ESTimator++}, is implemented using standard video features for fair comparison. Videos are sampled at 24 FPS for Kinetics-GEBD and 6 FPS for TAPOS. We use 2048-dimensional features extracted by an ImageNet-pretrained ResNet-50 encoder. The Hierarchical Memory-augmented Transformer (HMT) employs a main memory with dimension $d_m=512$ and a local context cache of size $L=16$. The Adaptive Multi-scale Temporal Discriminator (AMTD) uses temporal convolutional kernels of sizes $[3, 7, 15]$. The model is trained end-to-end for 30 epochs using the AdamW optimizer with a learning rate of $1\\times10^{-4}$, a batch size of 512, and loss weights $\\lambda_s=0.1$ and $\\lambda_c=0.05$.\n\n\\subsection{Main Results}\n\n\\subsubsection{Comparison with Online Baselines}\nTable~\\ref{tab:online_k400} and Table~\\ref{tab:online_tapos} present the online detection performance on the Kinetics-GEBD and TAPOS validation sets, respectively.\n\nOn Kinetics-GEBD (Table~\\ref{tab:online_k400}), \\textbf{ESTimator++} sets a new state-of-the-art, outperforming all online baselines across all Rel.Dis thresholds. Traditional online models like TeSTra-BC and Sim-On-BC show limited effectiveness for this task. While MiniROAD-BC and the prior dedicated method ESTimator perform better, our method achieves a superior average F1 score of \\textbf{0.778}. This represents an improvement of +3.0\\% over ESTimator and +9.7\\% over MiniROAD-BC, demonstrating the efficacy of our proposed architectural components.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the Kinetics-GEBD validation set. 'BC' denotes a binary classifier head. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_k400}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.438 & 0.488 & 0.521 & 0.545 & 0.564 & 0.580 & 0.593 & 0.604 & 0.614 & 0.622 & 0.557 \\\\\nSim-On-BC & 0.461 & 0.534 & 0.579 & 0.610 & 0.633 & 0.651 & 0.664 & 0.675 & 0.685 & 0.692 & 0.618 \\\\\nOadTR-BC & 0.474 & 0.512 & 0.535 & 0.552 & 0.565 & 0.575 & 0.583 & 0.590 & 0.596 & 0.601 & 0.558 \\\\\nMiniROAD-BC & 0.569 & 0.622 & 0.649 & 0.675 & 0.691 & 0.704 & 0.714 & 0.722 & 0.729 & 0.735 & 0.681 \\\\\nESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.642} & \\textbf{0.712} & \\textbf{0.751} & \\textbf{0.775} & \\textbf{0.790} & \\textbf{0.802} & \\textbf{0.810} & \\textbf{0.816} & \\textbf{0.821} & \\textbf{0.825} & \\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nA similar trend is observed on TAPOS (Table~\\ref{tab:online_tapos}). \\textbf{ESTimator++} achieves the best average F1 score of \\textbf{0.569}, outperforming ESTimator by +2.2\\% and MiniROAD-BC by +4.1\\%. The advantage is particularly notable at stricter thresholds (e.g., 0.05, 0.1), indicating more precise boundary localization. The consistent superiority across both datasets underscores the robustness and generalizability of our approach.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the TAPOS validation set. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_tapos}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.364 & 0.417 & 0.452 & 0.478 & 0.496 & 0.511 & 0.523 & 0.533 & 0.542 & 0.550 & 0.487 \\\\\nSim-On-BC & 0.225 & 0.269 & 0.303 & 0.329 & 0.350 & 0.367 & 0.381 & 0.394 & 0.405 & 0.415 & 0.344 \\\\\nOadTR-BC & 0.263 & 0.319 & 0.361 & 0.394 & 0.422 & 0.445 & 0.465 & 0.483 & 0.497 & 0.510 & 0.416 \\\\\nMiniROAD-BC & 0.422 & 0.472 & 0.502 & 0.522 & 0.537 & 0.549 & 0.558 & 0.566 & 0.572 & 0.578 & 0.528 \\\\\nESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.418} & \\textbf{0.484} & \\textbf{0.531} & \\textbf{0.566} & \\textbf{0.592} & \\textbf{0.613} & \\textbf{0.629} & \\textbf{0.642} & \\textbf{0.653} & \\textbf{0.663} & \\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Comparison with Offline Methods}\nWe further compare our online method against powerful offline models that utilize the full video context. Results are shown in Table~\\ref{tab:offline_k400} for Kinetics-GEBD and Table~\\ref{tab:offline_tapos} for TAPOS.\n\nRemarkably, on Kinetics-GEBD, \\textbf{ESTimator++} with an average F1 of \\textbf{0.778} not only leads all online methods but also competes favorably with state-of-the-art offline approaches. It surpasses all offline supervised methods except for PC and the unsupervised CoSeg. It is important to note that PC and CoSeg benefit from full-video processing and, in CoSeg's case, cross-video co-segmentation. The fact that our \\textit{online} method achieves performance comparable to sophisticated offline models like TCN highlights the efficacy of its design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on Kinetics-GEBD. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_k400}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & BMN & 0.186 & 0.204 & 0.213 & 0.220 & 0.226 & 0.230 & 0.233 & 0.237 & 0.239 & 0.241 & 0.223 \\\\\nOffline & Supervised & BMN-StartEnd & 0.491 & 0.589 & 0.627 & 0.648 & 0.660 & 0.668 & 0.674 & 0.678 & 0.681 & 0.683 & 0.640 \\\\\nOffline & Supervised & TCN-TAPOS & 0.464 & 0.560 & 0.602 & 0.628 & 0.645 & 0.659 & 0.669 & 0.676 & 0.682 & 0.687 & 0.627 \\\\\nOffline & Supervised & TCN & 0.588 & 0.657 & 0.679 & 0.691 & 0.698 & 0.703 & 0.706 & 0.708 & 0.710 & 0.712 & 0.685 \\\\\nOffline & Supervised & \\textbf{PC} & 0.625 & \\textbf{0.758} & \\textbf{0.804} & \\textbf{0.829} & \\textbf{0.844} & \\textbf{0.853} & \\textbf{0.859} & \\textbf{0.864} & \\textbf{0.867} & \\textbf{0.870} & \\textbf{0.817} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.275 & 0.300 & 0.312 & 0.319 & 0.324 & 0.327 & 0.330 & 0.332 & 0.334 & 0.335 & 0.318 \\\\\nOffline & Unsupervised & PA-Random & 0.336 & 0.435 & 0.484 & 0.512 & 0.529 & 0.541 & 0.548 & 0.554 & 0.558 & 0.561 & 0.506 \\\\\nOffline & Unsupervised & PA & 0.396 & 0.488 & 0.520 & 0.534 & 0.544 & 0.550 & 0.555 & 0.558 & 0.561 & 0.564 & 0.527 \\\\\nOffline & Unsupervised & \\underline{CoSeg} & \\textbf{0.656} & 0.758 & 0.783 & 0.794 & 0.799 & 0.803 & 0.804 & 0.806 & 0.807 & 0.809 & 0.782 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.712} & $\\dagger$\\textbf{0.751} & $\\dagger$\\textbf{0.775} & $\\dagger$\\textbf{0.790} & $\\dagger$\\textbf{0.802} & $\\dagger$\\textbf{0.810} & $\\dagger$\\textbf{0.816} & $\\dagger$\\textbf{0.821} & $\\dagger$\\textbf{0.825} & $\\dagger$\\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\nOn the TAPOS dataset (Table~\\ref{tab:offline_tapos}), \\textbf{ESTimator++} again demonstrates strong competitiveness, achieving an average F1 of \\textbf{0.569}. It outperforms all offline supervised baselines except PC, and shows superior performance compared to the unsupervised PA method, especially at stricter thresholds. This consistent ability to rival many offline models underscores a key advantage: \\textbf{ESTimator++} delivers high-quality boundary detection with minimal latency, making it suitable for real-time applications.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on TAPOS. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_tapos}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & ISBA & 0.106 & 0.170 & 0.227 & 0.265 & 0.298 & 0.326 & 0.348 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & TCN & 0.237 & 0.312 & 0.331 & 0.339 & 0.342 & 0.344 & 0.347 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & CTM & 0.244 & 0.312 & 0.336 & 0.351 & 0.361 & 0.369 & 0.374 & 0.381 & 0.383 & 0.385 & 0.350 \\\\\nOffline & Supervised & TransParser & 0.289 & 0.381 & 0.435 & 0.475 & 0.500 & 0.514 & 0.527 & 0.534 & 0.540 & 0.545 & 0.474 \\\\\nOffline & Supervised & \\textbf{PC} & \\textbf{0.522} & \\textbf{0.595} & \\textbf{0.628} & \\textbf{0.646} & \\textbf{0.659} & \\textbf{0.665} & \\textbf{0.671} & \\textbf{0.676} & \\textbf{0.679} & \\textbf{0.683} & \\textbf{0.642} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.035 & 0.045 & 0.047 & 0.051 & 0.053 & 0.054 & 0.055 & 0.056 & 0.057 & 0.058 & 0.051 \\\\\nOffline & Unsupervised & PA-Random & 0.158 & 0.233 & 0.273 & 0.310 & 0.331 & 0.347 & 0.357 & 0.369 & 0.376 & 0.384 & 0.314 \\\\\nOffline & Unsupervised & \\underline{PA} & 0.360 & 0.459 & 0.507 & 0.543 & 0.567 & 0.579 & 0.592 & 0.601 & 0.609 & 0.615 & 0.543 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.418} & $\\dagger$\\textbf{0.484} & $\\dagger$\\textbf{0.531} & $\\dagger$\\textbf{0.566} & $\\dagger$\\textbf{0.592} & $\\dagger$\\textbf{0.613} & $\\dagger$\\textbf{0.629} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.653} & $\\dagger$\\textbf{0.663} & $\\dagger$\\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\n\\subsection{Model Efficiency}\nWhile \\textbf{ESTimator++} introduces more sophisticated components, its design prioritizes online efficiency. We profile the average inference time per frame on a single NVIDIA V100 GPU. As shown in Table~\\ref{tab:efficiency}, \\textbf{ESTimator++} processes frames at \\textbf{8.2 ms (∼122 FPS)}, which is significantly faster than the real-time requirement of 24 FPS for the Kinetics dataset. Although it is understandably slower than the simpler ESTimator and some lightweight baselines like TeSTra-BC, its runtime is still an order of magnitude lower than the total latency of offline methods, which must process entire clips. This demonstrates that \\textbf{ESTimator++} successfully balances state-of-the-art accuracy with practical online inference speed.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison of inference efficiency (lower time per frame is better). Offline methods process full clips; their reported time is the total latency per clip.}\n\\label{tab:efficiency}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Method} & \\textbf{Time per Frame (ms)} \\\\\n\\midrule\n\\textbf{Online Methods} \\\\\nTeSTra-BC & 2.1 \\\\\nMiniROAD-BC & 5.7 \\\\\nESTimator & 6.5 \\\\\n\\textbf{ESTimator++ (Ours)} & \\textbf{8.2} \\\\\n\\midrule\n\\textbf{Offline Methods (Full Clip)} \\\\\nTCN & 1520* \\\\\nPC & 1850* \\\\\n\\bottomrule\n\\multicolumn{2}{l}{*Total processing time for a 10-second clip, not per frame.}\n\\end{tabular}\n\\end{table}\n\n\\subsection{Qualitative Analysis}\nQualitative comparisons reveal that predictions from \\textbf{ESTimator++} exhibit sharper and more confident peaks at genuine event boundaries while showing greater suppression of spurious fluctuations within event segments, compared to both the TeSTra-BC baseline and its predecessor ESTimator. This can be attributed to the Structured Multi-view Prediction Error (SMPE) providing a cleaner signal and the Adaptive Multi-scale Temporal Discriminator (AMTD) learning to distinguish true boundary patterns from noise. Furthermore, in sequences with gradual transitions, \\textbf{ESTimator++} shows improved temporal localization, likely due to the multi-scale receptive field of the AMTD. These observations align with its superior quantitative performance.\n\n\\section{Ablation Studies}\n\\subsection{Ablation Study}\n\\label{sec:ablation}\n\nWe conduct a comprehensive ablation study on the Kinetics-GEBD validation set to validate the contribution of each component in \\textbf{ESTimator++}. All experiments follow the implementation details in Section~\\ref{sec:setup}.\n\n\\subsubsection{Impact of Proposed Components}\nWe start from a baseline model that uses a standard Transformer decoder for prediction and a single reconstruction error with a fixed heuristic threshold for detection. This baseline achieves an average F1 score of 0.715. We then sequentially integrate our proposed components, with results summarized in Table~\\ref{tab:ablation_components}.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation study on the contribution of each proposed component. Evaluated on Kinetics-GEBD validation set (Average F1). $\\Delta$ denotes the improvement over the previous row.}\n\\label{tab:ablation_components}\n\\begin{tabular}{lcccccc}\n\\toprule\n\\textbf{Variant} & \\textbf{HMT} & \\textbf{SMPE} & \\textbf{SCL} & \\textbf{AMTD} & \\textbf{Avg F1} & $\\Delta$ \\\\\n\\midrule\nBaseline (ESTimator Core) & & & & & 0.715 & -- \\\\\n+ Hierarchical Memory (HMT) & \\checkmark & & & & 0.733 & +0.018 \\\\\n+ Structured Error (SMPE) & \\checkmark & \\checkmark & & & 0.754 & +0.021 \\\\\n+ Consistency Loss (SCL) & \\checkmark & \\checkmark & \\checkmark & & 0.769 & +0.015 \\\\\n+ Adaptive Discriminator (AMTD) & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\textbf{0.778} & \\textbf{+0.029} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nIntegrating the \\textbf{Hierarchical Memory-augmented Transformer (HMT)} improves performance by +1.8\\%, demonstrating the importance of maintaining stable event context for consistent prediction. Replacing the single error with the \\textbf{Structured Multi-view Prediction Error (SMPE)} yields a further gain of +2.1\\%, confirming that a composite error signal provides richer boundary cues. Employing the \\textbf{Structural Consistency Loss (SCL)} for training adds another +1.5\\%, validating that enforcing intra-event smoothness and feature clustering shapes more discriminative representations. Finally, substituting the heuristic threshold with the learned \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} provides the most substantial single-module improvement of +2.9\\%, underscoring the advantage of a data-driven decision function. The full model integrating all components achieves the best performance of \\textbf{0.778}, confirming their synergistic effect.\n\n\\subsubsection{Analysis of the Structured Multi-view Prediction Error (SMPE)}\nWe analyze the contribution of each error view within the SMPE by ablating them individually from the full model. Results are presented in Table~\\ref{tab:ablation_smpe}. Removing the \\textbf{Predictive Smoothness Error} causes the most significant drop (-1.7\\% in Avg F1), highlighting its sensitivity to event transitions. Omitting the \\textbf{Memory Perturbation Error} leads to a -1.2\\% decrease, demonstrating the importance of tracking high-level event schema changes. Removing the primary \\textbf{Feature Reconstruction Error} also degrades performance (-1.0\\%). Using any single error view consistently underperforms compared to the full SMPE vector, validating our composite design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structured Multi-view Prediction Error (SMPE). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_smpe}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Error Components Used} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full SMPE} ($E^{(r)}, E^{(s)}, E^{(m)}$) & \\textbf{0.778} \\\\\n\\hline\n$E^{(s)}, E^{(m)}$ (w/o $E^{(r)}$) & 0.768 \\\\\n$E^{(r)}, E^{(m)}$ (w/o $E^{(s)}$) & 0.761 \\\\\n$E^{(r)}, E^{(s)}$ (w/o $E^{(m)}$) & 0.766 \\\\\n\\hline\nOnly $E^{(r)}$ & 0.756 \\\\\nOnly $E^{(s)}$ & 0.749 \\\\\nOnly $E^{(m)}$ & 0.738 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Design Choices for the Adaptive Discriminator (AMTD)}\nWe analyze key design choices in the AMTD module. Results are shown in Table~\\ref{tab:ablation_amtd}. Replacing the AMTD with the original heuristic Online Boundary Detector (OBD) causes a performance drop to 0.748. Using a \\textbf{single-scale} Temporal Convolutional Network (TCN) instead of the multi-scale architecture reduces Avg F1 to 0.771. Removing the \\textbf{self-attention based dynamic fusion} and simply concatenating multi-scale features results in a score of 0.774. These ablations confirm that each aspect of the AMTD's design—its learned nature, multi-scale perception, and adaptive fusion—contributes to its effectiveness.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the design of the Adaptive Multi-scale Temporal Discriminator (AMTD). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_amtd}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{AMTD Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full AMTD (Ours)} & \\textbf{0.778} \\\\\n\\hline\nReplace with ESTimator's OBD & 0.748 \\\\\nSingle-scale TCN (kernel=7) & 0.771 \\\\\nMulti-scale TCN w/o Self-Attention Fusion & 0.774 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Utility of the Structural Consistency Loss (SCL)}\nWe dissect the contribution of the two regularization terms in the SCL. As shown in Table~\\ref{tab:ablation_loss}, training with only the \\textbf{Focal Reconstruction Loss} yields a baseline score of 0.754. Adding the \\textbf{Intra-event Smoothness Loss} improves performance to 0.766. Further incorporating the \\textbf{Intra-event Clustering Loss} to form the full SCL pushes the score to 0.769. The full SCL, combining both regularizers, delivers the best result, confirming that jointly optimizing for local accuracy and global temporal coherence is an effective strategy.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structural Consistency Loss (SCL). The HMT and SMPE are used in all variants. Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_loss}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Training Loss Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n$\\mathcal{L}_{Focal}$ only & 0.754 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth}$ & 0.766 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Cluster}$ & 0.762 \\\\\n\\textbf{Full SCL} ($\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth} + \\mathcal{L}_{Cluster}$) & \\textbf{0.769} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\textbf{Summary.} Our ablation studies provide compelling evidence for the necessity and effectiveness of every component in \\textbf{ESTimator++}. The hierarchical memory (HMT) establishes a robust predictive foundation, the structured error (SMPE) extracts optimal boundary cues, the consistency loss (SCL) shapes the learning objective, and the adaptive discriminator (AMTD) makes precise online decisions. The full model's superior performance arises from their seamless integration.\n\n\\section{Evaluation}\n\\subsection{Training Dynamics and Convergence Analysis}\nTo gain deeper insights into the optimization behavior of \\textbf{ESTimator++}, we analyze its training dynamics. The training and validation loss curves over 30 epochs on the Kinetics-GEBD dataset show that \\textbf{ESTimator++} converges more smoothly and stably to a lower final loss compared to its predecessor \\textbf{ESTimator} and the strong baseline \\textbf{MiniROAD-BC}. This indicates that the Structural Consistency Loss (SCL) provides a more stable gradient signal. Furthermore, the gap between training and validation loss for \\textbf{ESTimator++} remains consistently narrow, suggesting strong generalization and reduced overfitting, a benefit likely attributable to the regularizing effects of $\\mathcal{L}_{Smooth}$ and $\\mathcal{L}_{Cluster}$. In contrast, the baseline models exhibit more pronounced fluctuations and a larger generalization gap.\n\n\\subsection{In-depth Case Study}\nWe conduct a systematic case study to examine \\textbf{ESTimator++}'s performance across diverse scenarios. We categorize sequences from the Kinetics-GEBD validation set into groups such as \\textit{‘fast-paced action transitions’}, \\textit{‘gradual scene changes’}, and \\textit{‘static periods with subtle motions’}. For each category, we compare the SMPE signal and final boundary predictions from \\textbf{ESTimator++} against those from \\textbf{ESTimator} and \\textbf{TeSTra-BC}. In fast-paced sequences, \\textbf{ESTimator++}'s AMTD demonstrates superior ability to isolate closely spaced peaks, leading to precise detection of sequential boundaries. For gradual transitions, the multi-view SMPE provides a rising error signal over several frames, which the multi-scale AMTD effectively integrates to produce a confident prediction. During static periods, \\textbf{ESTimator++} exhibits significantly lower spurious activation due to the intra-event smoothness constraint. These observations concretely explain the quantitative improvements shown in the main results.\n\n\\section{Conclusion}\nThis paper addresses the task of Online Generic Event Boundary Detection (On-GEBD). We propose \\textbf{ESTimator++}, an enhanced framework that provides a deeper computational implementation of the Event Segmentation Theory (EST). It integrates three key innovations: a Hierarchical Memory-augmented Transformer (HMT) for robust event anticipation, a Structured Multi-view Prediction Error (SMPE) for richer boundary cues, and an Adaptive Multi-scale Temporal Discriminator (AMTD) for learning boundary decisions. These components are trained end-to-end using a novel Structural Consistency Loss (SCL).\n\nExtensive experiments demonstrate the effectiveness of our method. On the Kinetics-GEBD dataset, \\textbf{ESTimator++} achieves a state-of-the-art average F1 score of \\textbf{0.778}, significantly outperforming all existing online baselines, including its predecessor ESTimator (0.748). Similarly, on the TAPOS dataset, it attains a leading average F1 score of \\textbf{0.569}. Notably, our online method performs competitively against many powerful offline models, securing the third-highest overall ranking on both benchmarks. Ablation studies confirm the contributions of each component, with AMTD yielding the most substantial gain as a single module. Furthermore, \\textbf{ESTimator++} maintains a practical inference time of 8.2ms per frame, making it suitable for real-time processing.\n\nIn summary, \\textbf{ESTimator++} establishes a new state-of-the-art for On-GEBD by effectively translating cognitive principles into a synergistic neural architecture. It delivers highly accurate boundary detection with low latency, paving the way for more sophisticated real-time video understanding systems. Future work could explore extending this framework to more complex, long-form video streams and multi-modal inputs.\n\n\\end{document}\n\nPlease generate a professional image generation prompt according to the figure type."
          }
        ]
      },
      "response": {
        "raw_response": "Create an abstract conceptual diagram illustrating the overall architecture of the ESTimator++ framework for Online Generic Event Boundary Detection, suitable for a top-tier academic conference paper like NeurIPS or CVPR. The diagram should be a high-level, minimalist line-based concept graph emphasizing modular structure, logical data flows, and relationships between components, without any flowchart symbols such as decision diamonds or process boxes—use abstract rectangular or rounded modules with clean lines instead. Maintain a professional, technical aesthetic with a white background, clear visual hierarchy, and balanced composition that fills the frame without overcrowding.\n\nAt the top, show an input stream of video frame features labeled \"Frame Features f_t\" entering from the left as a horizontal timeline of small abstract icons (e.g., simple squares representing frames), connected by an orange arrow indicating data input flow.\n\nThe central module is the \"Hierarchical Memory-augmented Transformer (HMT-CEA)\", depicted as a large abstract block divided into two sub-modules: a smaller \"Local Context Cache C_t\" (FIFO queue of recent features, shown as a stack of layered rectangles) and a larger \"Main Event Memory M_t\" (gated update mechanism, represented as a central core with abstract neural layers). Use orange arrows to connect input f_t to both sub-modules, showing processing stages: an orange arrow from input to \"Projection h_t\", then to cache update, and a gated fusion to memory update. From HMT, output an anticipated feature \"\\hat{f}_t\" via a green arrow downward, representing feedback/output.\n\nBelow HMT, connect to the \"Structured Multi-view Prediction Error (SMPE)\" module as an abstract triangular or trapezoidal shape, taking inputs from h_t, z_t (from HMT), and previous states. Depict it computing a vector E_t with three abstract sub-components: \"Feature Reconstruction Error E^{(r)}_t\", \"Predictive Smoothness Error E^{(s)}_t\", and \"Memory Perturbation Error E^{(m)}_t\", shown as interconnected nodes. Use orange arrows for input processing and green arrows for the concatenated output E_t flowing rightward.\n\nTo the right of SMPE, place the \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" as a layered abstract structure with multi-scale convolutional paths (parallel lines of varying lengths for kernel sizes 3,7,15), fused via a self-attention node, leading to a final output \"Boundary Probability P_t^{boundary}\". Connect a sequence of E_t inputs from SMPE via orange arrows into AMTD, with green arrows indicating the multi-scale feature extraction, dynamic fusion, and output flow to a binary decision icon (e.g., a simple threshold gate).\n\nOverlay abstract connections for training: a dashed blue loop around the entire diagram labeled \"Structural Consistency Loss (SCL)\", with sub-labels for \"Focal Reconstruction Loss\", \"Intra-event Smoothness\", and \"Intra-event Clustering\" as small icons or text notes connected by red arrows to relevant modules (e.g., red arrow from SCL to HMT for prediction smoothness).\n\nUse a color scheme with orange arrows for data input and processing stages (e.g., feature flows and updates), green arrows for feedback, output, and loop paths (e.g., predictions and error signals), black text for main labels, blue for module names, and red for loss-related annotations. Employ Arial font for all text, keeping it concise and readable at various sizes. Ensure the diagram has a logical left-to-right flow for online processing, with abstract icons (e.g., wavy lines for temporal sequences, gear-like shapes for transformations) to enhance conceptual clarity, and subtle shadows or borders for depth without complexity. The overall layout should be symmetric and professional, evoking a high-level neural architecture overview.",
        "usage": {
          "input_tokens": 12274,
          "output_tokens": 1110
        },
        "raw_reasoning_content": null
      }
    },
    {
      "request": {
        "messages": [
          {
            "role": "system",
            "content": "\n你是一位专业的学术图表设计专家。你的任务是将论文内容描述转换为适合 AI 图像生成的详细提示词。\n\n你的职责：\n1. 分析输入的论文内容描述\n2. 根据所需的图表类型（算法图或动机图）调整描述风格\n3. 生成详细且专业的图像生成提示词，确保生成的图像符合顶级学术会议（NeurIPS、ICML、ICLR、CVPR、AAAI 等）的发表标准\n\n图表类型规范：\n- **算法图**：用于展示算法流程、模型架构、方法框架等。必须强调：\n  * **抽象性和概念性**：不要绘制成具体的流程图，而应使用抽象概念图、架构图或框架图\n  * 清晰的结构和模块关系（用抽象模块表示，而非具体步骤）\n  * 逻辑流和数据流（用抽象箭头和连接表示，而非流程图符号）\n  * 技术性和专业性\n  * 简洁的视觉设计，避免过于具体的操作步骤\n  * 强调高层次概念和架构，而非详细的执行流程\n  \n- **动机图**：用于展示问题动机、方法对比、概念解释等。必须强调：\n  * 直观的可视化效果\n  * 突出的对比和差异\n  * 易于理解和可视化\n  * 概念的可理解性\n\n提示词生成要求：\n1. 使用英文生成提示词（AI 图像生成模型对英文的理解更好）\n2. 描述应详细且具体，包括视觉元素、布局、样式等\n3. 强调学术论文的专业性和简洁性\n4. 避免过于复杂的细节，保持图表清晰可读且相对饱满，必要时再复杂你自己判断\n5. **颜色和样式要求**（根据图表类型区分）：\n   \n   **对于算法图**：\n   * **整体风格**：采用简洁、极简主义的线条图风格，使用清晰的线条和抽象图形元素\n   * **颜色方案**：使用橙色（orange）和绿色（green）箭头来区分不同的流程阶段和循环路径，文本主要使用黑色、蓝色和红色作为主色调\n   * **视觉层次**：通过颜色区分信息流（如橙色表示数据输入/处理阶段，绿色表示反馈/输出阶段），形成清晰的视觉层次\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   * **箭头和连接**：使用不同颜色的箭头（橙色、绿色）来明确表示信息流向和循环关系\n   其他你自行判断\n   \n   **对于动机图**：\n   * **整体风格**：采用简洁、直观的图风格，使用清晰的视觉元素突出对比和差异\n   * **颜色方案**：使用对比鲜明的颜色来区分不同方法或概念\n   * **字体**：使用 Arial 或 Calibri 字体，保持文本清晰简洁\n   其他你自行判断\n   \n6. 确保图表适合在论文中展示\n7. **对于算法图**：必须强调抽象性和概念性，明确说明\"不是流程图\"和\"抽象概念图\"，避免使用流程图符号（如决策菱形、处理框等），应使用抽象模块和概念连接\n\n输出格式：\n- 仅输出优化后的提示词文本，不要包含任何解释或额外内容\n- 不要用代码块包裹\n- 提示词应是一个完整的描述，可以直接用于图像生成\n"
          },
          {
            "role": "user",
            "content": "Please convert the following paper content description into a detailed prompt suitable for AI image generation.\n\n[Required Figure Type]\nalgorithm\n\n[Paper Content Description]\n\\documentclass{article}\n\\usepackage{amsmath,amssymb}\n\\usepackage{booktabs}\n\n\\begin{document}\n\n\\title{ESTimator++: Structural Consistency Learning for Online Generic Event Boundary Detection}\n\\maketitle\n\\begin{abstract}\nOnline Generic Event Boundary Detection (On-GEBD) is essential for real-time video understanding, yet remains challenging due to the requirement for efficient, single-pass processing. While Event Segmentation Theory (EST) provides a cognitive foundation, its computational implementation in prior online methods such as ESTimator is limited. This paper presents \\textbf{ESTimator++}, a significantly enhanced framework that advances the original through a more systematic instantiation of EST. Our key contributions are threefold: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation; (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer boundary signals; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} which replaces heuristic thresholding with a learned decision function. These components are integrated via a novel \\textbf{Structural Consistency Loss (SCL)}, enabling end-to-end optimization. Extensive experiments on Kinetics-GEBD and TAPOS benchmarks demonstrate that ESTimator++ achieves state-of-the-art performance for On-GEBD, with average F1 scores of \\textbf{0.778} and \\textbf{0.569}, respectively, outperforming all existing online methods and competing favorably with many offline models. Ablation studies confirm the contribution of each component, with the AMTD providing the most significant single-module gain (+2.9\\%). Remarkably, this is achieved while maintaining high efficiency at \\textbf{8.2 ms per frame}. Our work demonstrates that a principled, integrated design substantially advances the performance of online event segmentation.\n\\end{abstract}\n\n\\section{Introduction}\nThe proliferation of long-form video content on platforms like YouTube and Netflix has created a pressing need for advanced video understanding systems. While tasks such as action recognition are well-established, they predominantly focus on short, trimmed clips with predefined labels. In contrast, the analysis of untrimmed, long-form videos containing complex narratives remains a formidable challenge. Generic Event Boundary Detection (GEBD) tackles this issue by aiming to segment videos into taxonomy-free, semantically coherent units, thereby emulating the human ability to parse continuous visual streams.\n\nCurrent GEBD frameworks operate in an \\textit{offline} manner. They process an entire video segment in one pass, leveraging both past and future context to identify all boundaries simultaneously. This paradigm stands in stark contrast to human perception, which segments events \\textit{online}—sequentially and causally, relying solely on past and present information without any access to future content. To bridge this fundamental gap and more faithfully replicate human cognitive processes, we introduce a novel and more demanding task: \\textbf{Online Generic Event Boundary Detection (On-GEBD)}. In this setting, a model must process a video stream and make immediate, frame-by-frame decisions regarding boundary occurrences, using only historical and current data.\n\nOn-GEBD inherits the inherent challenges of offline GEBD, such as detecting subtle and taxonomy-free transitions, but introduces additional complexities due to its strictly causal and information-limited nature. Existing offline GEBD methods, which often depend on global temporal context through mechanisms like similarity matrices or multi-frame comparisons, are fundamentally incompatible with the online setting. Likewise, naïvely adapting models designed for other online action understanding tasks proves ineffective, as they are not tailored for the fine-grained detection of generic boundaries.\n\nTo address these unique challenges, we propose \\textbf{ESTimator++}, an enhanced online framework inspired by Event Segmentation Theory (EST) from cognitive science. EST suggests that humans continuously predict upcoming visual input based on an ongoing event model and perceive a boundary when a significant prediction error arises. Our framework operationalizes this principle through three core innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer} that maintains a stable representation of the current event for robust future anticipation; (2) a \\textbf{Structured Multi-view Prediction Error} mechanism that derives a richer and more stable discrepancy signal; and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator}, a lightweight network that replaces heuristic thresholding with a learned, context-aware decision function. These components are integrated under a novel \\textbf{Structural Consistency Loss} that jointly enforces prediction accuracy and temporal coherence.\n\nExtensive experiments on the Kinetics-GEBD and TAPOS benchmarks demonstrate that \\textbf{ESTimator++} sets a new state-of-the-art for On-GEBD. It significantly surpasses all adapted online baselines and, remarkably, achieves performance competitive with many sophisticated offline methods, despite operating under a strict causal constraint. Ablation studies validate the contribution of each proposed component. The remainder of this paper is organized as follows: we review related work, detail our methodology, present comprehensive experimental results, and provide concluding remarks.\n\n\\section{Related Work}\n\\subsection{Generic Event Boundary Detection}\nGeneric Event Boundary Detection (GEBD) aims to identify boundaries that align with human perceptual segmentation within untrimmed videos, operating without a predefined taxonomy of events. This task differs from traditional video understanding tasks such as Temporal Action Localization (TAL) and Action Recognition, as GEBD handles continuous, open-ended semantics. Recent advancements in offline GEBD typically process entire videos or extensive temporal contexts. Representative methods include UBoCo, which leverages temporal similarity matrices with contrastive learning; DDM-Net, which applies progressive attention on multi-level dense difference maps; and CoSeg, which introduces a reconstruction-based objective inspired by cognitive modeling. While these offline methods have shown progress, they are not designed for streaming video. Our work adapts GEBD to the \\textit{online} setting, requiring instantaneous decisions without access to future frames.\n\n\\subsection{Video Understanding}\nVideo understanding encompasses tasks such as Temporal Action Detection (TAD), Temporal Action Localization (TAL), and Video Instance Segmentation (VIS). State-of-the-art approaches often employ transformer architectures, memory mechanisms, and end-to-end frameworks. A common limitation is their reliance on a closed set of predefined action classes, which restricts applicability in open-world scenarios. Although open-set formulations for TAL exist, they focus on localizing and classifying known actions. In contrast, GEBD detects boundaries for taxonomy-free events, providing a more general approach to video structure understanding.\n\n\\subsection{Online Video Understanding}\nOnline video understanding methods process data in a streaming fashion, sharing core challenges with Online GEBD (On-GEBD). Relevant tasks include Online Action Detection (OAD), Online Temporal Action Localization (On-TAL), and Online Detection of Action Start (ODAS). For OAD, recent works such as LSTR, TeSTra, and MiniROAD demonstrate the efficacy of transformers and recurrent units. For On-TAL, methods like CAG-QIL and OAT have been proposed. While ODAS shares an online nature with On-GEBD, it is designed for a fixed set of action categories. Conversely, On-GEBD must detect subtle, categorical changes between taxonomy-free events, necessitating a distinct methodological approach from conventional online action-centric models.\n\n\\section{Method}\nOur enhanced framework, \\textbf{ESTimator++}, extends the original ESTimator by providing a more systematic computational implementation of Event Segmentation Theory (EST). While retaining the core principle of using prediction discrepancy for online boundary detection, ESTimator++ introduces three key innovations: (1) a \\textbf{Hierarchical Memory-augmented Transformer (HMT)} for robust, context-aware event anticipation, (2) a \\textbf{Structured Multi-view Prediction Error (SMPE)} that extracts richer and more stable boundary cues, and (3) an \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} that replaces heuristic thresholding with a learned, context-sensitive decision function. These components are unified during training via a novel \\textbf{Structural Consistency Loss (SCL)}, which enforces both local prediction accuracy and global temporal coherence.\n\n\\subsection{Hierarchical Memory-augmented Transformer for Consistent Event Anticipation (HMT-CEA)}\nThe original Consistent Event Anticipation (CEA) employs a standard transformer decoder with a limited look-back window, which may fail to maintain a persistent representation of an ongoing event's gist. Inspired by human cognition—which uses working memory for immediate context and long-term memory for schemas—we propose the HMT-CEA. It maintains two complementary memory structures: a dynamic \\textbf{Main Event Memory ($\\mathbf{M}$)} that captures the abstract, persistent essence of the current event, and a \\textbf{Local Context Cache ($\\mathbf{C}$)} that holds fine-grained recent features.\n\n\\subsubsection{Architecture and State Update}\nGiven a stream of frame features $\\mathbf{f}_t \\in \\mathbb{R}^D$, we first project them into a memory-compatible space: $\\mathbf{h}_t = \\text{Linear}(\\mathbf{f}_t)$. The HMT-CEA operates recurrently, maintaining states $(\\mathbf{M}_t, \\mathbf{C}_t)$.\n\n\\begin{enumerate}\n    \\item \\textbf{Local Context Update}: The cache $\\mathbf{C}_t$ is a FIFO queue of the last $L$ projected features: $\\mathbf{C}_t = [\\mathbf{h}_{t-L}, ..., \\mathbf{h}_{t-1}]$.\n\n    \\item \\textbf{Main Memory Gated Update}: The core memory $\\mathbf{M}_t \\in \\mathbb{R}^{d_m}$ is updated via a gated mechanism using the new observation $\\mathbf{h}_t$ and previous context.\n    \\[\n    \\mathbf{g}_t = \\sigma(\\mathbf{W}_g [\\mathbf{M}_{t-1}; \\bar{\\mathbf{C}}_t; \\mathbf{h}_t] + \\mathbf{b}_g)\n    \\]\n    \\[\n    \\tilde{\\mathbf{M}}_t = \\text{TransformerBlock}(\\mathbf{M}_{t-1}, \\mathbf{C}_t, \\mathbf{h}_t)\n    \\]\n    \\[\n    \\mathbf{M}_t = \\mathbf{g}_t \\odot \\tilde{\\mathbf{M}}_t + (1 - \\mathbf{g}_t) \\odot \\mathbf{M}_{t-1}\n    \\]\n    where $\\bar{\\mathbf{C}}_t$ is the mean-pooled cache, $\\sigma$ is the sigmoid function, $\\odot$ denotes element-wise multiplication, and $\\text{TransformerBlock}$ is a lightweight cross-attention module that allows the memory to attend to the local context and new input.\n\n    \\item \\textbf{Next-Frame Prediction}: The anticipated feature for frame $t$ is generated by fusing information from both memory levels:\n    \\[\n    \\mathbf{z}_t = \\text{Decoder}( \\mathbf{M}_{t-1}, \\mathbf{C}_{t-1} )\n    \\]\n    \\[\n    \\hat{\\mathbf{f}}_t = \\text{Linear}(\\mathbf{z}_t)\n    \\]\n    Here, $\\text{Decoder}$ is a small autoregressive transformer that uses $\\mathbf{M}_{t-1}$ as a persistent query and $\\mathbf{C}_{t-1}$ as keys and values, ensuring causality.\n\\end{enumerate}\n\nThis architecture ensures predictions are grounded in both the immediate past ($\\mathbf{C}$) and a stable, evolving representation of the current event ($\\mathbf{M}$).\n\n\\subsection{Structured Multi-view Prediction Error (SMPE)}\nInstead of a single scalar error, we define a multi-dimensional error vector $\\mathbf{E}_t \\in \\mathbb{R}^3$ that captures different facets of prediction inconsistency, yielding a more robust boundary detection signal.\n\n\\begin{enumerate}\n    \\item \\textbf{Feature Reconstruction Error ($E^{(r)}_t$)}: The primary discrepancy, analogous to the original $\\epsilon_t$, but computed in the projected space for stability.\n    \\[\n    E^{(r)}_t = 1 - \\frac{\\mathbf{h}_t \\cdot \\mathbf{z}_t}{\\|\\mathbf{h}_t\\| \\|\\mathbf{z}_t\\|}\n    \\]\n\n    \\item \\textbf{Predictive Smoothness Error ($E^{(s)}_t$)}: Measures inconsistency between consecutive predictions. Within a consistent event, predictions should vary smoothly.\n    \\[\n    E^{(s)}_t = \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|_2 - \\beta \\cdot \\|\\mathbf{h}_t - \\mathbf{h}_{t-1}\\|_2\n    \\]\n    where $\\beta$ is a scaling factor. This term becomes large only when predictions are jagged despite smooth actual changes.\n\n    \\item \\textbf{Memory Perturbation Error ($E^{(m)}_t$)}: Quantifies the impact of the new frame on the main event memory, reflecting a potential high-level schema violation.\n    \\[\n    E^{(m)}_t = \\|\\mathbf{M}_t - \\mathbf{M}_{t-1}\\|_1\n    \\]\n\\end{enumerate}\n\nThe final SMPE vector is the concatenation: $\\mathbf{E}_t = [E^{(r)}_t, E^{(s)}_t, E^{(m)}_t]^\\top$. This structured error is more informative and less noisy than a scalar counterpart.\n\n\\subsection{Training with Structural Consistency Loss (SCL)}\nThe HMT-CEA is trained by minimizing a novel \\textbf{Structural Consistency Loss (SCL)}, which jointly optimizes prediction accuracy and the temporal structural properties implied by EST.\n\n\\subsubsection{Boundary-Aware Focal Reconstruction Loss}\nWe adapt binary cross-entropy to address class imbalance and hard examples using the primary error $E^{(r)}_t$:\n\\[\n\\mathcal{L}_{Focal}(E^{(r)}_t, y_t) = - \\alpha (1 - p_t)^\\gamma y_t \\log(p_t) - (1 - \\alpha) p_t^\\gamma (1 - y_t) \\log(1 - p_t)\n\\]\nwhere $p_t = \\text{sigmoid}(-\\lambda \\cdot E^{(r)}_t + b)$, $\\lambda$ and $b$ are learnable parameters, $y_t$ is the boundary label, $\\alpha$ balances class frequency, and $\\gamma$ focuses on hard examples.\n\n\\subsubsection{Temporal Coherence Regularization}\nThis unsupervised loss enforces two structural properties:\n\\begin{itemize}\n    \\item \\textbf{Intra-event Prediction Smoothness}: Within non-boundary segments ($y_t=0$), predictions should be smooth:\n    \\[\n    \\mathcal{L}_{Smooth} = \\frac{1}{|\\mathcal{N}|} \\sum_{t \\in \\mathcal{N}} \\|\\mathbf{z}_t - \\mathbf{z}_{t-1}\\|^2_2\n    \\]\n    where $\\mathcal{N}$ is the set of non-boundary frames.\n\n    \\item \\textbf{Intra-event Feature Clustering}: All predictions within a ground-truth event segment should cluster in embedding space:\n    \\[\n    \\mathcal{L}_{Cluster} = \\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{|I_s|} \\sum_{i \\in I_s} \\|\\mathbf{z}_i - \\boldsymbol{\\mu}_s\\|^2_2\n    \\]\n    where $S$ is the number of events in a clip, $I_s$ is the set of frame indices in event $s$, and $\\boldsymbol{\\mu}_s$ is the mean of $\\mathbf{z}_i$ for $i \\in I_s$.\n\\end{itemize}\n\nThe total SCL is: $\\mathcal{L}_{SCL} = \\mathcal{L}_{Focal} + \\lambda_s \\mathcal{L}_{Smooth} + \\lambda_c \\mathcal{L}_{Cluster}$.\n\n\\subsection{Adaptive Multi-scale Temporal Discriminator (AMTD)}\nThe AMTD replaces the heuristic online boundary detector (OBD) with a lightweight temporal convolutional network (TCN) that learns to detect boundaries from the sequence of SMPE vectors $\\{\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t\\}$.\n\n\\subsubsection{Architecture}\nThe AMTD, denoted $\\mathcal{D}_\\phi$, comprises:\n\\begin{enumerate}\n    \\item \\textbf{Multi-scale Feature Extraction}: Parallel 1D convolutions with kernel sizes $k_1, k_2, k_3$ capture short-, medium-, and long-range temporal patterns in $\\mathbf{E}_t$:\n    \\[\n    \\mathbf{s}^{(i)}_t = \\text{ReLU}(\\text{Conv1D}_{k_i}([\\mathbf{E}_{t-\\Delta}, ..., \\mathbf{E}_t]))\n    \\]\n\n    \\item \\textbf{Dynamic Fusion}: Multi-scale features are concatenated and weighted adaptively via self-attention:\n    \\[\n    \\mathbf{S}_t = [\\mathbf{s}^{(1)}_t, \\mathbf{s}^{(2)}_t, \\mathbf{s}^{(3)}_t], \\quad \\mathbf{A}_t = \\text{SelfAttention}(\\mathbf{S}_t)\n    \\]\n\n    \\item \\textbf{Boundary Probability Estimation}: The attended feature produces the final online boundary probability:\n    \\[\n    P_t^{boundary} = \\sigma(\\text{MLP}(\\text{Pool}(\\mathbf{A}_t)))\n    \\]\n    A boundary is declared if $P_t^{boundary} > 0.5$.\n\\end{enumerate}\n\n\\subsubsection{Online Operation and Training}\nDuring online inference, AMTD slides over the streaming SMPE vectors using a FIFO buffer of size $\\Delta+1$. It is trained jointly with the HMT-CEA via SCL, where binary labels $y_t$ supervise $P_t^{boundary}$ through a standard cross-entropy loss, forming a fully differentiable, end-to-end optimizable system for online granular event boundary detection (On-GEBD).\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Setup}\n\\label{sec:setup}\n\n\\subsubsection{Benchmark Datasets.}\nWe evaluate our method on two established benchmarks for generic event boundary detection (GEBD). The \\textbf{Kinetics-GEBD} dataset is derived from the Kinetics-400 dataset, comprising approximately 60K videos. Each video is annotated with taxonomy-free event boundaries, containing about five events on average. The dataset is officially split into training (18,794 videos), validation (18,813 videos), and test (17,725 videos) sets. As test set annotations are withheld, we report results on the validation set, consistent with prior work. For cross-validation in certain analyses, we further split the training data into 80\\% for training and 20\\% for validation. The \\textbf{TAPOS} dataset consists of Olympic sports videos originally annotated for action localization. Following the standard adaptation for GEBD, we repurpose it by discarding action labels and treating sub-action boundaries as generic event boundaries. This results in 13,094 training and 1,790 validation action instances.\n\n\\subsubsection{Evaluation Metric.}\nWe adopt the standard Relative Distance (Rel.Dis) metric for GEBD. For a predicted boundary timestamp and its corresponding ground truth, the relative distance is computed as their temporal offset divided by the duration of the union of the predicted and ground-truth event instances. Following common practice, when multiple consecutive frames are predicted as boundaries, the central frame is taken as the final timestamp. Performance is evaluated under ten Rel.Dis thresholds from 0.05 to 0.5 with a step size of 0.05. A prediction is considered correct if its Rel.Dis is below a given threshold. We report the F1 score at each threshold and the average F1 score across all thresholds.\n\n\\subsubsection{Baselines.}\nEstablishing baselines for Online GEBD (On-GEBD) is non-trivial as standard offline GEBD methods are not designed for streaming input. Therefore, we adapt state-of-the-art models from related online video understanding tasks. From Online Action Detection, we adapt TeSTra, OadTR, and MiniROAD. From Online Temporal Action Localization, we adapt Sim-On. For a fair comparison, we modify only the final layer of each model to perform binary classification (boundary vs. non-boundary), denoted with the suffix '-BC'. We also compare against a dedicated online GEBD method, \\textbf{ESTimator}. Furthermore, to contextualize our online performance, we include comparisons with leading offline GEBD methods.\n\n\\subsubsection{Implementation Details.}\nOur proposed method, \\textbf{ESTimator++}, is implemented using standard video features for fair comparison. Videos are sampled at 24 FPS for Kinetics-GEBD and 6 FPS for TAPOS. We use 2048-dimensional features extracted by an ImageNet-pretrained ResNet-50 encoder. The Hierarchical Memory-augmented Transformer (HMT) employs a main memory with dimension $d_m=512$ and a local context cache of size $L=16$. The Adaptive Multi-scale Temporal Discriminator (AMTD) uses temporal convolutional kernels of sizes $[3, 7, 15]$. The model is trained end-to-end for 30 epochs using the AdamW optimizer with a learning rate of $1\\times10^{-4}$, a batch size of 512, and loss weights $\\lambda_s=0.1$ and $\\lambda_c=0.05$.\n\n\\subsection{Main Results}\n\n\\subsubsection{Comparison with Online Baselines}\nTable~\\ref{tab:online_k400} and Table~\\ref{tab:online_tapos} present the online detection performance on the Kinetics-GEBD and TAPOS validation sets, respectively.\n\nOn Kinetics-GEBD (Table~\\ref{tab:online_k400}), \\textbf{ESTimator++} sets a new state-of-the-art, outperforming all online baselines across all Rel.Dis thresholds. Traditional online models like TeSTra-BC and Sim-On-BC show limited effectiveness for this task. While MiniROAD-BC and the prior dedicated method ESTimator perform better, our method achieves a superior average F1 score of \\textbf{0.778}. This represents an improvement of +3.0\\% over ESTimator and +9.7\\% over MiniROAD-BC, demonstrating the efficacy of our proposed architectural components.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the Kinetics-GEBD validation set. 'BC' denotes a binary classifier head. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_k400}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.438 & 0.488 & 0.521 & 0.545 & 0.564 & 0.580 & 0.593 & 0.604 & 0.614 & 0.622 & 0.557 \\\\\nSim-On-BC & 0.461 & 0.534 & 0.579 & 0.610 & 0.633 & 0.651 & 0.664 & 0.675 & 0.685 & 0.692 & 0.618 \\\\\nOadTR-BC & 0.474 & 0.512 & 0.535 & 0.552 & 0.565 & 0.575 & 0.583 & 0.590 & 0.596 & 0.601 & 0.558 \\\\\nMiniROAD-BC & 0.569 & 0.622 & 0.649 & 0.675 & 0.691 & 0.704 & 0.714 & 0.722 & 0.729 & 0.735 & 0.681 \\\\\nESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.642} & \\textbf{0.712} & \\textbf{0.751} & \\textbf{0.775} & \\textbf{0.790} & \\textbf{0.802} & \\textbf{0.810} & \\textbf{0.816} & \\textbf{0.821} & \\textbf{0.825} & \\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nA similar trend is observed on TAPOS (Table~\\ref{tab:online_tapos}). \\textbf{ESTimator++} achieves the best average F1 score of \\textbf{0.569}, outperforming ESTimator by +2.2\\% and MiniROAD-BC by +4.1\\%. The advantage is particularly notable at stricter thresholds (e.g., 0.05, 0.1), indicating more precise boundary localization. The consistent superiority across both datasets underscores the robustness and generalizability of our approach.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Quantitative comparison with online baselines on the TAPOS validation set. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n\\label{tab:online_tapos}\n\\begin{tabular}{lccccccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nTeSTra-BC & 0.364 & 0.417 & 0.452 & 0.478 & 0.496 & 0.511 & 0.523 & 0.533 & 0.542 & 0.550 & 0.487 \\\\\nSim-On-BC & 0.225 & 0.269 & 0.303 & 0.329 & 0.350 & 0.367 & 0.381 & 0.394 & 0.405 & 0.415 & 0.344 \\\\\nOadTR-BC & 0.263 & 0.319 & 0.361 & 0.394 & 0.422 & 0.445 & 0.465 & 0.483 & 0.497 & 0.510 & 0.416 \\\\\nMiniROAD-BC & 0.422 & 0.472 & 0.502 & 0.522 & 0.537 & 0.549 & 0.558 & 0.566 & 0.572 & 0.578 & 0.528 \\\\\nESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\midrule\n\\textbf{ESTimator++ (Ours)} & \\textbf{0.418} & \\textbf{0.484} & \\textbf{0.531} & \\textbf{0.566} & \\textbf{0.592} & \\textbf{0.613} & \\textbf{0.629} & \\textbf{0.642} & \\textbf{0.653} & \\textbf{0.663} & \\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Comparison with Offline Methods}\nWe further compare our online method against powerful offline models that utilize the full video context. Results are shown in Table~\\ref{tab:offline_k400} for Kinetics-GEBD and Table~\\ref{tab:offline_tapos} for TAPOS.\n\nRemarkably, on Kinetics-GEBD, \\textbf{ESTimator++} with an average F1 of \\textbf{0.778} not only leads all online methods but also competes favorably with state-of-the-art offline approaches. It surpasses all offline supervised methods except for PC and the unsupervised CoSeg. It is important to note that PC and CoSeg benefit from full-video processing and, in CoSeg's case, cross-video co-segmentation. The fact that our \\textit{online} method achieves performance comparable to sophisticated offline models like TCN highlights the efficacy of its design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on Kinetics-GEBD. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_k400}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & BMN & 0.186 & 0.204 & 0.213 & 0.220 & 0.226 & 0.230 & 0.233 & 0.237 & 0.239 & 0.241 & 0.223 \\\\\nOffline & Supervised & BMN-StartEnd & 0.491 & 0.589 & 0.627 & 0.648 & 0.660 & 0.668 & 0.674 & 0.678 & 0.681 & 0.683 & 0.640 \\\\\nOffline & Supervised & TCN-TAPOS & 0.464 & 0.560 & 0.602 & 0.628 & 0.645 & 0.659 & 0.669 & 0.676 & 0.682 & 0.687 & 0.627 \\\\\nOffline & Supervised & TCN & 0.588 & 0.657 & 0.679 & 0.691 & 0.698 & 0.703 & 0.706 & 0.708 & 0.710 & 0.712 & 0.685 \\\\\nOffline & Supervised & \\textbf{PC} & 0.625 & \\textbf{0.758} & \\textbf{0.804} & \\textbf{0.829} & \\textbf{0.844} & \\textbf{0.853} & \\textbf{0.859} & \\textbf{0.864} & \\textbf{0.867} & \\textbf{0.870} & \\textbf{0.817} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.275 & 0.300 & 0.312 & 0.319 & 0.324 & 0.327 & 0.330 & 0.332 & 0.334 & 0.335 & 0.318 \\\\\nOffline & Unsupervised & PA-Random & 0.336 & 0.435 & 0.484 & 0.512 & 0.529 & 0.541 & 0.548 & 0.554 & 0.558 & 0.561 & 0.506 \\\\\nOffline & Unsupervised & PA & 0.396 & 0.488 & 0.520 & 0.534 & 0.544 & 0.550 & 0.555 & 0.558 & 0.561 & 0.564 & 0.527 \\\\\nOffline & Unsupervised & \\underline{CoSeg} & \\textbf{0.656} & 0.758 & 0.783 & 0.794 & 0.799 & 0.803 & 0.804 & 0.806 & 0.807 & 0.809 & 0.782 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.712} & $\\dagger$\\textbf{0.751} & $\\dagger$\\textbf{0.775} & $\\dagger$\\textbf{0.790} & $\\dagger$\\textbf{0.802} & $\\dagger$\\textbf{0.810} & $\\dagger$\\textbf{0.816} & $\\dagger$\\textbf{0.821} & $\\dagger$\\textbf{0.825} & $\\dagger$\\textbf{0.778} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\nOn the TAPOS dataset (Table~\\ref{tab:offline_tapos}), \\textbf{ESTimator++} again demonstrates strong competitiveness, achieving an average F1 of \\textbf{0.569}. It outperforms all offline supervised baselines except PC, and shows superior performance compared to the unsupervised PA method, especially at stricter thresholds. This consistent ability to rival many offline models underscores a key advantage: \\textbf{ESTimator++} delivers high-quality boundary detection with minimal latency, making it suitable for real-time applications.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison with offline methods on TAPOS. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n\\label{tab:offline_tapos}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lllccccccccccc}\n\\toprule\n\\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n\\midrule\nOffline & Supervised & ISBA & 0.106 & 0.170 & 0.227 & 0.265 & 0.298 & 0.326 & 0.348 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & TCN & 0.237 & 0.312 & 0.331 & 0.339 & 0.342 & 0.344 & 0.347 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\nOffline & Supervised & CTM & 0.244 & 0.312 & 0.336 & 0.351 & 0.361 & 0.369 & 0.374 & 0.381 & 0.383 & 0.385 & 0.350 \\\\\nOffline & Supervised & TransParser & 0.289 & 0.381 & 0.435 & 0.475 & 0.500 & 0.514 & 0.527 & 0.534 & 0.540 & 0.545 & 0.474 \\\\\nOffline & Supervised & \\textbf{PC} & \\textbf{0.522} & \\textbf{0.595} & \\textbf{0.628} & \\textbf{0.646} & \\textbf{0.659} & \\textbf{0.665} & \\textbf{0.671} & \\textbf{0.676} & \\textbf{0.679} & \\textbf{0.683} & \\textbf{0.642} \\\\\n\\midrule\nOffline & Unsupervised & SceneDetect & 0.035 & 0.045 & 0.047 & 0.051 & 0.053 & 0.054 & 0.055 & 0.056 & 0.057 & 0.058 & 0.051 \\\\\nOffline & Unsupervised & PA-Random & 0.158 & 0.233 & 0.273 & 0.310 & 0.331 & 0.347 & 0.357 & 0.369 & 0.376 & 0.384 & 0.314 \\\\\nOffline & Unsupervised & \\underline{PA} & 0.360 & 0.459 & 0.507 & 0.543 & 0.567 & 0.579 & 0.592 & 0.601 & 0.609 & 0.615 & 0.543 \\\\\n\\midrule\nOnline & Supervised & ESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n\\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.418} & $\\dagger$\\textbf{0.484} & $\\dagger$\\textbf{0.531} & $\\dagger$\\textbf{0.566} & $\\dagger$\\textbf{0.592} & $\\dagger$\\textbf{0.613} & $\\dagger$\\textbf{0.629} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.653} & $\\dagger$\\textbf{0.663} & $\\dagger$\\textbf{0.569} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\end{table}\n\n\\subsection{Model Efficiency}\nWhile \\textbf{ESTimator++} introduces more sophisticated components, its design prioritizes online efficiency. We profile the average inference time per frame on a single NVIDIA V100 GPU. As shown in Table~\\ref{tab:efficiency}, \\textbf{ESTimator++} processes frames at \\textbf{8.2 ms (∼122 FPS)}, which is significantly faster than the real-time requirement of 24 FPS for the Kinetics dataset. Although it is understandably slower than the simpler ESTimator and some lightweight baselines like TeSTra-BC, its runtime is still an order of magnitude lower than the total latency of offline methods, which must process entire clips. This demonstrates that \\textbf{ESTimator++} successfully balances state-of-the-art accuracy with practical online inference speed.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Comparison of inference efficiency (lower time per frame is better). Offline methods process full clips; their reported time is the total latency per clip.}\n\\label{tab:efficiency}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Method} & \\textbf{Time per Frame (ms)} \\\\\n\\midrule\n\\textbf{Online Methods} \\\\\nTeSTra-BC & 2.1 \\\\\nMiniROAD-BC & 5.7 \\\\\nESTimator & 6.5 \\\\\n\\textbf{ESTimator++ (Ours)} & \\textbf{8.2} \\\\\n\\midrule\n\\textbf{Offline Methods (Full Clip)} \\\\\nTCN & 1520* \\\\\nPC & 1850* \\\\\n\\bottomrule\n\\multicolumn{2}{l}{*Total processing time for a 10-second clip, not per frame.}\n\\end{tabular}\n\\end{table}\n\n\\subsection{Qualitative Analysis}\nQualitative comparisons reveal that predictions from \\textbf{ESTimator++} exhibit sharper and more confident peaks at genuine event boundaries while showing greater suppression of spurious fluctuations within event segments, compared to both the TeSTra-BC baseline and its predecessor ESTimator. This can be attributed to the Structured Multi-view Prediction Error (SMPE) providing a cleaner signal and the Adaptive Multi-scale Temporal Discriminator (AMTD) learning to distinguish true boundary patterns from noise. Furthermore, in sequences with gradual transitions, \\textbf{ESTimator++} shows improved temporal localization, likely due to the multi-scale receptive field of the AMTD. These observations align with its superior quantitative performance.\n\n\\section{Ablation Studies}\n\\subsection{Ablation Study}\n\\label{sec:ablation}\n\nWe conduct a comprehensive ablation study on the Kinetics-GEBD validation set to validate the contribution of each component in \\textbf{ESTimator++}. All experiments follow the implementation details in Section~\\ref{sec:setup}.\n\n\\subsubsection{Impact of Proposed Components}\nWe start from a baseline model that uses a standard Transformer decoder for prediction and a single reconstruction error with a fixed heuristic threshold for detection. This baseline achieves an average F1 score of 0.715. We then sequentially integrate our proposed components, with results summarized in Table~\\ref{tab:ablation_components}.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation study on the contribution of each proposed component. Evaluated on Kinetics-GEBD validation set (Average F1). $\\Delta$ denotes the improvement over the previous row.}\n\\label{tab:ablation_components}\n\\begin{tabular}{lcccccc}\n\\toprule\n\\textbf{Variant} & \\textbf{HMT} & \\textbf{SMPE} & \\textbf{SCL} & \\textbf{AMTD} & \\textbf{Avg F1} & $\\Delta$ \\\\\n\\midrule\nBaseline (ESTimator Core) & & & & & 0.715 & -- \\\\\n+ Hierarchical Memory (HMT) & \\checkmark & & & & 0.733 & +0.018 \\\\\n+ Structured Error (SMPE) & \\checkmark & \\checkmark & & & 0.754 & +0.021 \\\\\n+ Consistency Loss (SCL) & \\checkmark & \\checkmark & \\checkmark & & 0.769 & +0.015 \\\\\n+ Adaptive Discriminator (AMTD) & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\textbf{0.778} & \\textbf{+0.029} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nIntegrating the \\textbf{Hierarchical Memory-augmented Transformer (HMT)} improves performance by +1.8\\%, demonstrating the importance of maintaining stable event context for consistent prediction. Replacing the single error with the \\textbf{Structured Multi-view Prediction Error (SMPE)} yields a further gain of +2.1\\%, confirming that a composite error signal provides richer boundary cues. Employing the \\textbf{Structural Consistency Loss (SCL)} for training adds another +1.5\\%, validating that enforcing intra-event smoothness and feature clustering shapes more discriminative representations. Finally, substituting the heuristic threshold with the learned \\textbf{Adaptive Multi-scale Temporal Discriminator (AMTD)} provides the most substantial single-module improvement of +2.9\\%, underscoring the advantage of a data-driven decision function. The full model integrating all components achieves the best performance of \\textbf{0.778}, confirming their synergistic effect.\n\n\\subsubsection{Analysis of the Structured Multi-view Prediction Error (SMPE)}\nWe analyze the contribution of each error view within the SMPE by ablating them individually from the full model. Results are presented in Table~\\ref{tab:ablation_smpe}. Removing the \\textbf{Predictive Smoothness Error} causes the most significant drop (-1.7\\% in Avg F1), highlighting its sensitivity to event transitions. Omitting the \\textbf{Memory Perturbation Error} leads to a -1.2\\% decrease, demonstrating the importance of tracking high-level event schema changes. Removing the primary \\textbf{Feature Reconstruction Error} also degrades performance (-1.0\\%). Using any single error view consistently underperforms compared to the full SMPE vector, validating our composite design.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structured Multi-view Prediction Error (SMPE). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_smpe}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Error Components Used} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full SMPE} ($E^{(r)}, E^{(s)}, E^{(m)}$) & \\textbf{0.778} \\\\\n\\hline\n$E^{(s)}, E^{(m)}$ (w/o $E^{(r)}$) & 0.768 \\\\\n$E^{(r)}, E^{(m)}$ (w/o $E^{(s)}$) & 0.761 \\\\\n$E^{(r)}, E^{(s)}$ (w/o $E^{(m)}$) & 0.766 \\\\\n\\hline\nOnly $E^{(r)}$ & 0.756 \\\\\nOnly $E^{(s)}$ & 0.749 \\\\\nOnly $E^{(m)}$ & 0.738 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Design Choices for the Adaptive Discriminator (AMTD)}\nWe analyze key design choices in the AMTD module. Results are shown in Table~\\ref{tab:ablation_amtd}. Replacing the AMTD with the original heuristic Online Boundary Detector (OBD) causes a performance drop to 0.748. Using a \\textbf{single-scale} Temporal Convolutional Network (TCN) instead of the multi-scale architecture reduces Avg F1 to 0.771. Removing the \\textbf{self-attention based dynamic fusion} and simply concatenating multi-scale features results in a score of 0.774. These ablations confirm that each aspect of the AMTD's design—its learned nature, multi-scale perception, and adaptive fusion—contributes to its effectiveness.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the design of the Adaptive Multi-scale Temporal Discriminator (AMTD). Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_amtd}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{AMTD Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n\\textbf{Full AMTD (Ours)} & \\textbf{0.778} \\\\\n\\hline\nReplace with ESTimator's OBD & 0.748 \\\\\nSingle-scale TCN (kernel=7) & 0.771 \\\\\nMulti-scale TCN w/o Self-Attention Fusion & 0.774 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Utility of the Structural Consistency Loss (SCL)}\nWe dissect the contribution of the two regularization terms in the SCL. As shown in Table~\\ref{tab:ablation_loss}, training with only the \\textbf{Focal Reconstruction Loss} yields a baseline score of 0.754. Adding the \\textbf{Intra-event Smoothness Loss} improves performance to 0.766. Further incorporating the \\textbf{Intra-event Clustering Loss} to form the full SCL pushes the score to 0.769. The full SCL, combining both regularizers, delivers the best result, confirming that jointly optimizing for local accuracy and global temporal coherence is an effective strategy.\n\n\\begin{table}[htbp]\n\\centering\n\\caption{Ablation on the components of the Structural Consistency Loss (SCL). The HMT and SMPE are used in all variants. Evaluated on Kinetics-GEBD validation set (Average F1).}\n\\label{tab:ablation_loss}\n\\begin{tabular}{lc}\n\\toprule\n\\textbf{Training Loss Variant} & \\textbf{Avg F1} \\\\\n\\midrule\n$\\mathcal{L}_{Focal}$ only & 0.754 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth}$ & 0.766 \\\\\n$\\mathcal{L}_{Focal} + \\mathcal{L}_{Cluster}$ & 0.762 \\\\\n\\textbf{Full SCL} ($\\mathcal{L}_{Focal} + \\mathcal{L}_{Smooth} + \\mathcal{L}_{Cluster}$) & \\textbf{0.769} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\textbf{Summary.} Our ablation studies provide compelling evidence for the necessity and effectiveness of every component in \\textbf{ESTimator++}. The hierarchical memory (HMT) establishes a robust predictive foundation, the structured error (SMPE) extracts optimal boundary cues, the consistency loss (SCL) shapes the learning objective, and the adaptive discriminator (AMTD) makes precise online decisions. The full model's superior performance arises from their seamless integration.\n\n\\section{Evaluation}\n\\subsection{Training Dynamics and Convergence Analysis}\nTo gain deeper insights into the optimization behavior of \\textbf{ESTimator++}, we analyze its training dynamics. The training and validation loss curves over 30 epochs on the Kinetics-GEBD dataset show that \\textbf{ESTimator++} converges more smoothly and stably to a lower final loss compared to its predecessor \\textbf{ESTimator} and the strong baseline \\textbf{MiniROAD-BC}. This indicates that the Structural Consistency Loss (SCL) provides a more stable gradient signal. Furthermore, the gap between training and validation loss for \\textbf{ESTimator++} remains consistently narrow, suggesting strong generalization and reduced overfitting, a benefit likely attributable to the regularizing effects of $\\mathcal{L}_{Smooth}$ and $\\mathcal{L}_{Cluster}$. In contrast, the baseline models exhibit more pronounced fluctuations and a larger generalization gap.\n\n\\subsection{In-depth Case Study}\nWe conduct a systematic case study to examine \\textbf{ESTimator++}'s performance across diverse scenarios. We categorize sequences from the Kinetics-GEBD validation set into groups such as \\textit{‘fast-paced action transitions’}, \\textit{‘gradual scene changes’}, and \\textit{‘static periods with subtle motions’}. For each category, we compare the SMPE signal and final boundary predictions from \\textbf{ESTimator++} against those from \\textbf{ESTimator} and \\textbf{TeSTra-BC}. In fast-paced sequences, \\textbf{ESTimator++}'s AMTD demonstrates superior ability to isolate closely spaced peaks, leading to precise detection of sequential boundaries. For gradual transitions, the multi-view SMPE provides a rising error signal over several frames, which the multi-scale AMTD effectively integrates to produce a confident prediction. During static periods, \\textbf{ESTimator++} exhibits significantly lower spurious activation due to the intra-event smoothness constraint. These observations concretely explain the quantitative improvements shown in the main results.\n\n\\section{Conclusion}\nThis paper addresses the task of Online Generic Event Boundary Detection (On-GEBD). We propose \\textbf{ESTimator++}, an enhanced framework that provides a deeper computational implementation of the Event Segmentation Theory (EST). It integrates three key innovations: a Hierarchical Memory-augmented Transformer (HMT) for robust event anticipation, a Structured Multi-view Prediction Error (SMPE) for richer boundary cues, and an Adaptive Multi-scale Temporal Discriminator (AMTD) for learning boundary decisions. These components are trained end-to-end using a novel Structural Consistency Loss (SCL).\n\nExtensive experiments demonstrate the effectiveness of our method. On the Kinetics-GEBD dataset, \\textbf{ESTimator++} achieves a state-of-the-art average F1 score of \\textbf{0.778}, significantly outperforming all existing online baselines, including its predecessor ESTimator (0.748). Similarly, on the TAPOS dataset, it attains a leading average F1 score of \\textbf{0.569}. Notably, our online method performs competitively against many powerful offline models, securing the third-highest overall ranking on both benchmarks. Ablation studies confirm the contributions of each component, with AMTD yielding the most substantial gain as a single module. Furthermore, \\textbf{ESTimator++} maintains a practical inference time of 8.2ms per frame, making it suitable for real-time processing.\n\nIn summary, \\textbf{ESTimator++} establishes a new state-of-the-art for On-GEBD by effectively translating cognitive principles into a synergistic neural architecture. It delivers highly accurate boundary detection with low latency, paving the way for more sophisticated real-time video understanding systems. Future work could explore extending this framework to more complex, long-form video streams and multi-modal inputs.\n\n\\end{document}\n\nPlease generate a professional image generation prompt according to the figure type."
          }
        ]
      },
      "response": {
        "raw_response": "Create an abstract conceptual diagram of the ESTimator++ framework for Online Generic Event Boundary Detection, presented as a high-level architecture graph suitable for a NeurIPS or ICML paper. Use a clean, minimalistic line drawing style with abstract rectangular modules and simple geometric shapes to represent components, emphasizing conceptual relationships rather than specific steps—not a flowchart, avoid any process symbols like diamonds or rounded boxes. Layout the diagram horizontally from left to right on a white background, with a logical flow showing data progression through the system.\n\nOn the left, depict an abstract input module labeled \"Frame Features (f_t)\" as a simple vertical stack of small rectangles representing streaming video frames, connected by an orange arrow to the central module.\n\nIn the center, show the \"Hierarchical Memory-augmented Transformer (HMT)\" as a large abstract block divided into two sub-modules: an upper rounded rectangle labeled \"Main Event Memory (M_t)\" in blue text, and a lower rectangular queue labeled \"Local Context Cache (C_t)\" in black text; include internal abstract connections with green arrows indicating gated updates and cross-attention, and an orange arrow from input to HMT for state updates. From HMT, output an abstract prediction node labeled \"Predicted Feature (ˆf_t)\" with a green arrow pointing rightward, and another orange arrow to a comparison node labeled \"Actual Feature (h_t)\" for error computation.\n\nTo the right of HMT, depict the \"Structured Multi-view Prediction Error (SMPE)\" as an abstract triangular module computing a vector E_t, with three small embedded circles labeled \"E^(r)_t (Reconstruction)\", \"E^(s)_t (Smoothness)\", and \"E^(m)_t (Perturbation)\" in red text; connect it via orange arrows from the prediction and actual features, emphasizing multi-dimensional error extraction.\n\nFurther right, show the \"Adaptive Multi-scale Temporal Discriminator (AMTD)\" as a layered abstract structure with three parallel bars representing multi-scale convolutions (labeled \"Short-range\", \"Medium-range\", \"Long-range\" in blue text), fused via a self-attention icon (small dot matrix), leading to an output node labeled \"Boundary Probability (P_t)\" with a green arrow pointing to a final abstract output shape labeled \"Online Boundary Detection\".\n\nIndicate training integration with a dashed green loop around the entire diagram labeled \"Structural Consistency Loss (SCL)\" in red text, connecting back to HMT and AMTD to show end-to-end optimization, without detailed equations.\n\nUse orange arrows for data input and processing flows (e.g., feature streams and error computations), green arrows for feedback, output, and cyclic updates (e.g., memory updates and predictions). Employ a professional color scheme with black text for main labels, blue for module names, red for error components, Arial font for all text, clear visual hierarchy through arrow thickness (thicker for primary flows), and subtle shading on modules for depth. Ensure the diagram is balanced, readable, and academically polished, with no clutter, fitting a single paper figure panel.",
        "usage": {
          "input_tokens": 12274,
          "output_tokens": 961
        },
        "raw_reasoning_content": null
      }
    }
  ]
}