{
  "stage": "planning_main_result",
  "timestamp": "2026-01-17T06:34:07.273636+00:00",
  "request": {
    "messages": [
      {
        "role": "system",
        "content": "You are an expert academic paper figure analysis assistant. Your task is to read\nthe LaTeX snippet of an experiment or ablation section and decide which figures\nshould be created, then return detailed, structured information for each figure.\n\nRequirements:\n1. Carefully read the LaTeX snippet (experiment or ablation part).\n2. Identify all content that should be visualized: metrics, comparisons, ablations, **tables, and existing figure environments** (even if a table/figure is already present, you should still propose a corresponding visualization for our automatic pipeline).\n3. For each figure that should be created, generate:\n   - filename: a descriptive English file name, e.g. \"performance_comparison.png\"\n   - description: a **detailed** description of what the figure shows\n   - old_string: a text span from the LaTeX snippet near where the figure should be inserted\n   - new_string: the replacement text that includes the LaTeX code for inserting the figure\n\nGuidelines:\n1. **Figure diversity is crucial**: You should propose a **variety of figure types** based on the data characteristics:\n   - **Bar charts**: for comparing discrete categories (methods, datasets, components)\n   - **Line plots**: for showing trends over continuous variables (epochs, learning rates, time)\n   - **Scatter plots**: for correlation analysis or distribution visualization\n   - **Heatmaps**: for matrix data (confusion matrices, correlation matrices, parameter grids)\n   - **Box plots**: for statistical distributions and comparisons\n   - **Violin plots**: for detailed distribution shapes\n   - **Multi-panel/subplot figures**: for combining related visualizations\n   - **3D plots**: for three-dimensional data relationships\n   - **Pie charts**: for proportional comparisons (use sparingly)\n   - **Error bar plots**: for showing uncertainty/confidence intervals\n   - Choose the most appropriate visualization type for each data pattern.\n\n2. **Experiment sections** usually need:\n   - performance comparison plots (different methods on different datasets)\n   - result visualizations (accuracy, F1, loss curves, etc.)\n   - method comparison plots (overall comparison of multiple methods)\n   - dataset distribution plots (if multiple datasets are involved)\n   - visual summaries of important **tables** (e.g., converting a performance table into a bar/line chart)\n\n3. **Ablation sections** usually need:\n   - component ablation plots (effect of removing/adding components)\n   - hyper-parameter sensitivity plots\n   - ablation comparison plots (effect of each configuration)\n\n4. **File naming rules**:\n   - use lowercase letters and underscores\n   - filenames must be descriptive, e.g. \"main_results.png\", \"ablation_study.png\"\n   - avoid Chinese characters or special symbols\n\n5. **Rules for old_string and new_string** (STRICT - both must be non-empty and based on actual LaTeX content):\n   - `old_string`: **MUST NOT be empty**. It must be an actual text span from the input LaTeX snippet near where the figure should be inserted\n     - it can be a full sentence, part of a paragraph, or context around where the figure is referenced\n   - `new_string`: **MUST NOT be empty**. It must be based on `old_string`, with the LaTeX figure code inserted\n\nLevel of detail (very important):\n- The **description** should be as detailed as reasonably possible, mentioning:\n  - what variables/metrics are on each axis,\n  - which methods or configurations are compared,\n  - which datasets or settings are used,\n  - what trends or key conclusions the figure is meant to highlight.\n- The **figure code** in `new_string` should also be detailed:\n  - use an informative `\\caption{...}` that summarizes the main insights,\n  - use a meaningful `\\label{...}` that follows the naming style `fig:<short_name>`.\n\nOutput format and minimal coverage (strict):\n- You must output a single JSON object wrapped in a `json` code block. The number of items in `figures`\n  must cover **all** visualizable elements you find in the snippet (not limited to two). \n- **Important**: Propose diverse figure types based on the data. Do not limit yourself to only bar charts or line plots.\n- Generic example showing figure diversity:\n\n```json\n{\n  \"figures\": [\n    {\n      \"filename\": \"performance_comparison.png\",\n      \"description\": \"A bar chart comparing accuracy and F1 scores across multiple methods (Method A, Method B, Our Method) on three datasets (Dataset X, Y, Z). X-axis: methods, Y-axis: performance metrics. Shows that Our Method outperforms all baselines on every dataset.\",\n      \"old_string\": \"The experimental results are shown as follows. Our method achieves the best performance on all three datasets.\",\n      \"new_string\": \"The experimental results are shown as follows.\\begin{figure}[htbp]\\centering\\includegraphics[width=0.8\\textwidth]{performance_comparison.png}\\caption{Comparison of accuracy and F1 scores of our method and baselines on three datasets A, B, and C. Our method consistently outperforms all baselines on every dataset.}\\label{fig:performance_comparison}\\end{figure} Our method achieves the best performance on all three datasets.\"\n    },\n    ...\n    {\n      \"filename\": \"loss_curves.png\",\n      \"description\": \"A line plot showing training and validation loss curves over epochs. X-axis: training epochs (0-100), Y-axis: loss values. Multiple lines represent different methods, with error bands showing standard deviation across runs. Demonstrates convergence behavior and overfitting patterns.\",\n      \"old_string\": \"The training process shows consistent convergence across all methods.\",\n      \"new_string\": \"The training process shows consistent convergence across all methods.\\begin{figure}[htbp]\\centering\\includegraphics[width=0.8\\textwidth]{loss_curves.png}\\caption{Training and validation loss curves for different methods over 100 epochs. Error bands indicate standard deviation across 5 runs.}\\label{fig:loss_curves}\\end{figure}\"\n    }\n  ]\n}\n```\n\nField definitions (with abstract descriptions):\n- `figures`: an array of all figure objects to be generated\n  - `filename`: string, the figure file name (use `.png` extension). Abstract: A descriptive identifier for the figure file, following naming conventions.\n  - `description`: string, a **detailed** description of what the figure shows. Abstract: A comprehensive textual specification that describes the figure's content, including chart type, axes, data variables, methods/datasets compared, and key insights to highlight.\n  - `old_string`: string, the original LaTeX text span to be replaced (**MUST NOT be empty**, must be actual text from the input LaTeX snippet). Abstract: A contiguous text segment from the input LaTeX that serves as the insertion anchor point for the figure.\n  - `new_string`: string, the replacement text that contains the LaTeX code to insert the figure (**MUST NOT be empty**, must be based on actual LaTeX content). Abstract: The modified LaTeX text that includes the original `old_string` content plus a complete LaTeX figure environment with proper formatting, caption, and label.\n\nImportant constraints:\n- You **must** output valid JSON (double quotes for keys and strings), with no extra comments or explanations.\n- Apart from the `json` code block, you MUST NOT output any other content.\n- **Figure diversity**: When multiple figures are needed, use **different visualization types** (bar charts, line plots, heatmaps, scatter plots, etc.) rather than repeating the same chart type. Match the visualization type to the data characteristics.\n- You should return an empty `figures` array **only if** the snippet truly contains no quantitative results, comparisons, ablations, tables, or figure-worthy content.\n- If the snippet includes any performance numbers, tables (e.g. `\\begin{table}...`), or existing figure environments (e.g. `\\begin{figure}...`), you **must** propose at least one figure in `figures` that visualizes or summarizes these results (even if similar content is already shown in the LaTeX).\n- Each figure must have a unique `filename`.\n- **CRITICAL**: Both `old_string` and `new_string` for each figure **MUST NOT be empty strings**. They must contain actual text from the input LaTeX snippet. If you cannot find suitable text in the input to use as `old_string`, do NOT include that figure in the output."
      },
      {
        "role": "user",
        "content": "[LaTeX 片段]\n\n   1: \\section{Experiments}\n   2: \\label{sec:experiments}\n   3: \n   4: \\subsection{Setup}\n   5: \\label{sec:setup}\n   6: \n   7: \\subsubsection{Benchmark Datasets.}\n   8: We evaluate our method on two established benchmarks for generic event boundary detection (GEBD). The \\textbf{Kinetics-GEBD} dataset is derived from the Kinetics-400 dataset, comprising approximately 60K videos. Each video is annotated with taxonomy-free event boundaries, containing about five events on average. The dataset is officially split into training (18,794 videos), validation (18,813 videos), and test (17,725 videos) sets. As test set annotations are withheld, we report results on the validation set, consistent with prior work. For cross-validation in certain analyses, we further split the training data into 80\\% for training and 20\\% for validation. The \\textbf{TAPOS} dataset consists of Olympic sports videos originally annotated for action localization. Following the standard adaptation for GEBD, we repurpose it by discarding action labels and treating sub-action boundaries as generic event boundaries. This results in 13,094 training and 1,790 validation action instances.\n   9: \n  10: \\subsubsection{Evaluation Metric.}\n  11: We adopt the standard Relative Distance (Rel.Dis) metric for GEBD. For a predicted boundary timestamp and its corresponding ground truth, the relative distance is computed as their temporal offset divided by the duration of the union of the predicted and ground-truth event instances. Following common practice, when multiple consecutive frames are predicted as boundaries, the central frame is taken as the final timestamp. Performance is evaluated under ten Rel.Dis thresholds from 0.05 to 0.5 with a step size of 0.05. A prediction is considered correct if its Rel.Dis is below a given threshold. We report the F1 score at each threshold and the average F1 score across all thresholds.\n  12: \n  13: \\subsubsection{Baselines.}\n  14: Establishing baselines for Online GEBD (On-GEBD) is non-trivial as standard offline GEBD methods are not designed for streaming input. Therefore, we adapt state-of-the-art models from related online video understanding tasks. From Online Action Detection, we adapt TeSTra, OadTR, and MiniROAD. From Online Temporal Action Localization, we adapt Sim-On. For a fair comparison, we modify only the final layer of each model to perform binary classification (boundary vs. non-boundary), denoted with the suffix '-BC'. We also compare against a dedicated online GEBD method, \\textbf{ESTimator}. Furthermore, to contextualize our online performance, we include comparisons with leading offline GEBD methods.\n  15: \n  16: \\subsubsection{Implementation Details.}\n  17: Our proposed method, \\textbf{ESTimator++}, is implemented using standard video features for fair comparison. Videos are sampled at 24 FPS for Kinetics-GEBD and 6 FPS for TAPOS. We use 2048-dimensional features extracted by an ImageNet-pretrained ResNet-50 encoder. The Hierarchical Memory-augmented Transformer (HMT) employs a main memory with dimension $d_m=512$ and a local context cache of size $L=16$. The Adaptive Multi-scale Temporal Discriminator (AMTD) uses temporal convolutional kernels of sizes $[3, 7, 15]$. The model is trained end-to-end for 30 epochs using the AdamW optimizer with a learning rate of $1\\times10^{-4}$, a batch size of 512, and loss weights $\\lambda_s=0.1$ and $\\lambda_c=0.05$.\n  18: \n  19: \\subsection{Main Results}\n  20: \n  21: \\subsubsection{Comparison with Online Baselines}\n  22: Table~\\ref{tab:online_k400} and Table~\\ref{tab:online_tapos} present the online detection performance on the Kinetics-GEBD and TAPOS validation sets, respectively.\n  23: \n  24: On Kinetics-GEBD (Table~\\ref{tab:online_k400}), \\textbf{ESTimator++} sets a new state-of-the-art, outperforming all online baselines across all Rel.Dis thresholds. Traditional online models like TeSTra-BC and Sim-On-BC show limited effectiveness for this task. While MiniROAD-BC and the prior dedicated method ESTimator perform better, our method achieves a superior average F1 score of \\textbf{0.778}. This represents an improvement of +3.0\\% over ESTimator and +9.7\\% over MiniROAD-BC, demonstrating the efficacy of our proposed architectural components.\n  25: \n  26: \\begin{table}[htbp]\n  27: \\centering\n  28: \\caption{Quantitative comparison with online baselines on the Kinetics-GEBD validation set. 'BC' denotes a binary classifier head. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n  29: \\label{tab:online_k400}\n  30: \\begin{tabular}{lccccccccccc}\n  31: \\toprule\n  32: \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n  33: \\midrule\n  34: TeSTra-BC & 0.438 & 0.488 & 0.521 & 0.545 & 0.564 & 0.580 & 0.593 & 0.604 & 0.614 & 0.622 & 0.557 \\\\\n  35: Sim-On-BC & 0.461 & 0.534 & 0.579 & 0.610 & 0.633 & 0.651 & 0.664 & 0.675 & 0.685 & 0.692 & 0.618 \\\\\n  36: OadTR-BC & 0.474 & 0.512 & 0.535 & 0.552 & 0.565 & 0.575 & 0.583 & 0.590 & 0.596 & 0.601 & 0.558 \\\\\n  37: MiniROAD-BC & 0.569 & 0.622 & 0.649 & 0.675 & 0.691 & 0.704 & 0.714 & 0.722 & 0.729 & 0.735 & 0.681 \\\\\n  38: ESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n  39: \\midrule\n  40: \\textbf{ESTimator++ (Ours)} & \\textbf{0.642} & \\textbf{0.712} & \\textbf{0.751} & \\textbf{0.775} & \\textbf{0.790} & \\textbf{0.802} & \\textbf{0.810} & \\textbf{0.816} & \\textbf{0.821} & \\textbf{0.825} & \\textbf{0.778} \\\\\n  41: \\bottomrule\n  42: \\end{tabular}\n  43: \\end{table}\n  44: \n  45: A similar trend is observed on TAPOS (Table~\\ref{tab:online_tapos}). \\textbf{ESTimator++} achieves the best average F1 score of \\textbf{0.569}, outperforming ESTimator by +2.2\\% and MiniROAD-BC by +4.1\\%. The advantage is particularly notable at stricter thresholds (e.g., 0.05, 0.1), indicating more precise boundary localization. The consistent superiority across both datasets underscores the robustness and generalizability of our approach.\n  46: \n  47: \\begin{table}[htbp]\n  48: \\centering\n  49: \\caption{Quantitative comparison with online baselines on the TAPOS validation set. Best results are in \\textbf{bold}, second best are \\underline{underlined}.}\n  50: \\label{tab:online_tapos}\n  51: \\begin{tabular}{lccccccccccc}\n  52: \\toprule\n  53: \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n  54: \\midrule\n  55: TeSTra-BC & 0.364 & 0.417 & 0.452 & 0.478 & 0.496 & 0.511 & 0.523 & 0.533 & 0.542 & 0.550 & 0.487 \\\\\n  56: Sim-On-BC & 0.225 & 0.269 & 0.303 & 0.329 & 0.350 & 0.367 & 0.381 & 0.394 & 0.405 & 0.415 & 0.344 \\\\\n  57: OadTR-BC & 0.263 & 0.319 & 0.361 & 0.394 & 0.422 & 0.445 & 0.465 & 0.483 & 0.497 & 0.510 & 0.416 \\\\\n  58: MiniROAD-BC & 0.422 & 0.472 & 0.502 & 0.522 & 0.537 & 0.549 & 0.558 & 0.566 & 0.572 & 0.578 & 0.528 \\\\\n  59: ESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n  60: \\midrule\n  61: \\textbf{ESTimator++ (Ours)} & \\textbf{0.418} & \\textbf{0.484} & \\textbf{0.531} & \\textbf{0.566} & \\textbf{0.592} & \\textbf{0.613} & \\textbf{0.629} & \\textbf{0.642} & \\textbf{0.653} & \\textbf{0.663} & \\textbf{0.569} \\\\\n  62: \\bottomrule\n  63: \\end{tabular}\n  64: \\end{table}\n  65: \n  66: \\subsubsection{Comparison with Offline Methods}\n  67: We further compare our online method against powerful offline models that utilize the full video context. Results are shown in Table~\\ref{tab:offline_k400} for Kinetics-GEBD and Table~\\ref{tab:offline_tapos} for TAPOS.\n  68: \n  69: Remarkably, on Kinetics-GEBD, \\textbf{ESTimator++} with an average F1 of \\textbf{0.778} not only leads all online methods but also competes favorably with state-of-the-art offline approaches. It surpasses all offline supervised methods except for PC and the unsupervised CoSeg. It is important to note that PC and CoSeg benefit from full-video processing and, in CoSeg's case, cross-video co-segmentation. The fact that our \\textit{online} method achieves performance comparable to sophisticated offline models like TCN highlights the efficacy of its design.\n  70: \n  71: \\begin{table}[htbp]\n  72: \\centering\n  73: \\caption{Comparison with offline methods on Kinetics-GEBD. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n  74: \\label{tab:offline_k400}\n  75: \\resizebox{\\textwidth}{!}{%\n  76: \\begin{tabular}{lllccccccccccc}\n  77: \\toprule\n  78: \\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n  79: \\midrule\n  80: Offline & Supervised & BMN & 0.186 & 0.204 & 0.213 & 0.220 & 0.226 & 0.230 & 0.233 & 0.237 & 0.239 & 0.241 & 0.223 \\\\\n  81: Offline & Supervised & BMN-StartEnd & 0.491 & 0.589 & 0.627 & 0.648 & 0.660 & 0.668 & 0.674 & 0.678 & 0.681 & 0.683 & 0.640 \\\\\n  82: Offline & Supervised & TCN-TAPOS & 0.464 & 0.560 & 0.602 & 0.628 & 0.645 & 0.659 & 0.669 & 0.676 & 0.682 & 0.687 & 0.627 \\\\\n  83: Offline & Supervised & TCN & 0.588 & 0.657 & 0.679 & 0.691 & 0.698 & 0.703 & 0.706 & 0.708 & 0.710 & 0.712 & 0.685 \\\\\n  84: Offline & Supervised & \\textbf{PC} & 0.625 & \\textbf{0.758} & \\textbf{0.804} & \\textbf{0.829} & \\textbf{0.844} & \\textbf{0.853} & \\textbf{0.859} & \\textbf{0.864} & \\textbf{0.867} & \\textbf{0.870} & \\textbf{0.817} \\\\\n  85: \\midrule\n  86: Offline & Unsupervised & SceneDetect & 0.275 & 0.300 & 0.312 & 0.319 & 0.324 & 0.327 & 0.330 & 0.332 & 0.334 & 0.335 & 0.318 \\\\\n  87: Offline & Unsupervised & PA-Random & 0.336 & 0.435 & 0.484 & 0.512 & 0.529 & 0.541 & 0.548 & 0.554 & 0.558 & 0.561 & 0.506 \\\\\n  88: Offline & Unsupervised & PA & 0.396 & 0.488 & 0.520 & 0.534 & 0.544 & 0.550 & 0.555 & 0.558 & 0.561 & 0.564 & 0.527 \\\\\n  89: Offline & Unsupervised & \\underline{CoSeg} & \\textbf{0.656} & 0.758 & 0.783 & 0.794 & 0.799 & 0.803 & 0.804 & 0.806 & 0.807 & 0.809 & 0.782 \\\\\n  90: \\midrule\n  91: Online & Supervised & ESTimator & 0.620 & 0.687 & 0.724 & 0.746 & 0.762 & 0.774 & 0.782 & 0.789 & 0.795 & 0.799 & 0.748 \\\\\n  92: \\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.712} & $\\dagger$\\textbf{0.751} & $\\dagger$\\textbf{0.775} & $\\dagger$\\textbf{0.790} & $\\dagger$\\textbf{0.802} & $\\dagger$\\textbf{0.810} & $\\dagger$\\textbf{0.816} & $\\dagger$\\textbf{0.821} & $\\dagger$\\textbf{0.825} & $\\dagger$\\textbf{0.778} \\\\\n  93: \\bottomrule\n  94: \\end{tabular}%\n  95: }\n  96: \\end{table}\n  97: \n  98: On the TAPOS dataset (Table~\\ref{tab:offline_tapos}), \\textbf{ESTimator++} again demonstrates strong competitiveness, achieving an average F1 of \\textbf{0.569}. It outperforms all offline supervised baselines except PC, and shows superior performance compared to the unsupervised PA method, especially at stricter thresholds. This consistent ability to rival many offline models underscores a key advantage: \\textbf{ESTimator++} delivers high-quality boundary detection with minimal latency, making it suitable for real-time applications.\n  99: \n 100: \\begin{table}[htbp]\n 101: \\centering\n 102: \\caption{Comparison with offline methods on TAPOS. Offline results are from their original papers. Best overall score is in \\textbf{bold}. Top-3 rankings are indicated: \\textbf{1st}, \\underline{2nd}, $\\dagger$3rd.}\n 103: \\label{tab:offline_tapos}\n 104: \\resizebox{\\textwidth}{!}{%\n 105: \\begin{tabular}{lllccccccccccc}\n 106: \\toprule\n 107: \\textbf{Setting} & \\textbf{Sup.} & \\textbf{Method} & \\textbf{0.05} & \\textbf{0.1} & \\textbf{0.15} & \\textbf{0.2} & \\textbf{0.25} & \\textbf{0.3} & \\textbf{0.35} & \\textbf{0.4} & \\textbf{0.45} & \\textbf{0.5} & \\textbf{Avg} \\\\\n 108: \\midrule\n 109: Offline & Supervised & ISBA & 0.106 & 0.170 & 0.227 & 0.265 & 0.298 & 0.326 & 0.348 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\n 110: Offline & Supervised & TCN & 0.237 & 0.312 & 0.331 & 0.339 & 0.342 & 0.344 & 0.347 & 0.348 & 0.348 & 0.348 & 0.330 \\\\\n 111: Offline & Supervised & CTM & 0.244 & 0.312 & 0.336 & 0.351 & 0.361 & 0.369 & 0.374 & 0.381 & 0.383 & 0.385 & 0.350 \\\\\n 112: Offline & Supervised & TransParser & 0.289 & 0.381 & 0.435 & 0.475 & 0.500 & 0.514 & 0.527 & 0.534 & 0.540 & 0.545 & 0.474 \\\\\n 113: Offline & Supervised & \\textbf{PC} & \\textbf{0.522} & \\textbf{0.595} & \\textbf{0.628} & \\textbf{0.646} & \\textbf{0.659} & \\textbf{0.665} & \\textbf{0.671} & \\textbf{0.676} & \\textbf{0.679} & \\textbf{0.683} & \\textbf{0.642} \\\\\n 114: \\midrule\n 115: Offline & Unsupervised & SceneDetect & 0.035 & 0.045 & 0.047 & 0.051 & 0.053 & 0.054 & 0.055 & 0.056 & 0.057 & 0.058 & 0.051 \\\\\n 116: Offline & Unsupervised & PA-Random & 0.158 & 0.233 & 0.273 & 0.310 & 0.331 & 0.347 & 0.357 & 0.369 & 0.376 & 0.384 & 0.314 \\\\\n 117: Offline & Unsupervised & \\underline{PA} & 0.360 & 0.459 & 0.507 & 0.543 & 0.567 & 0.579 & 0.592 & 0.601 & 0.609 & 0.615 & 0.543 \\\\\n 118: \\midrule\n 119: Online & Supervised & ESTimator & 0.394 & 0.455 & 0.499 & 0.532 & 0.558 & 0.578 & 0.594 & 0.608 & 0.619 & 0.629 & 0.547 \\\\\n 120: \\textbf{Online} & \\textbf{Supervised} & \\textbf{ESTimator++ (Ours)} & $\\dagger$\\textbf{0.418} & $\\dagger$\\textbf{0.484} & $\\dagger$\\textbf{0.531} & $\\dagger$\\textbf{0.566} & $\\dagger$\\textbf{0.592} & $\\dagger$\\textbf{0.613} & $\\dagger$\\textbf{0.629} & $\\dagger$\\textbf{0.642} & $\\dagger$\\textbf{0.653} & $\\dagger$\\textbf{0.663} & $\\dagger$\\textbf{0.569} \\\\\n 121: \\bottomrule\n 122: \\end{tabular}%\n 123: }\n 124: \\end{table}\n 125: \n 126: \\subsection{Model Efficiency}\n 127: While \\textbf{ESTimator++} introduces more sophisticated components, its design prioritizes online efficiency. We profile the average inference time per frame on a single NVIDIA V100 GPU. As shown in Table~\\ref{tab:efficiency}, \\textbf{ESTimator++} processes frames at \\textbf{8.2 ms (∼122 FPS)}, which is significantly faster than the real-time requirement of 24 FPS for the Kinetics dataset. Although it is understandably slower than the simpler ESTimator and some lightweight baselines like TeSTra-BC, its runtime is still an order of magnitude lower than the total latency of offline methods, which must process entire clips. This demonstrates that \\textbf{ESTimator++} successfully balances state-of-the-art accuracy with practical online inference speed.\n 128: \n 129: \\begin{table}[htbp]\n 130: \\centering\n 131: \\caption{Comparison of inference efficiency (lower time per frame is better). Offline methods process full clips; their reported time is the total latency per clip.}\n 132: \\label{tab:efficiency}\n 133: \\begin{tabular}{lc}\n 134: \\toprule\n 135: \\textbf{Method} & \\textbf{Time per Frame (ms)} \\\\\n 136: \\midrule\n 137: \\textbf{Online Methods} \\\\\n 138: TeSTra-BC & 2.1 \\\\\n 139: MiniROAD-BC & 5.7 \\\\\n 140: ESTimator & 6.5 \\\\\n 141: \\textbf{ESTimator++ (Ours)} & \\textbf{8.2} \\\\\n 142: \\midrule\n 143: \\textbf{Offline Methods (Full Clip)} \\\\\n 144: TCN & 1520* \\\\\n 145: PC & 1850* \\\\\n 146: \\bottomrule\n 147: \\multicolumn{2}{l}{*Total processing time for a 10-second clip, not per frame.}\n 148: \\end{tabular}\n 149: \\end{table}\n 150: \n 151: \\subsection{Qualitative Analysis}\n 152: Qualitative comparisons reveal that predictions from \\textbf{ESTimator++} exhibit sharper and more confident peaks at genuine event boundaries while showing greater suppression of spurious fluctuations within event segments, compared to both the TeSTra-BC baseline and its predecessor ESTimator. This can be attributed to the Structured Multi-view Prediction Error (SMPE) providing a cleaner signal and the Adaptive Multi-scale Temporal Discriminator (AMTD) learning to distinguish true boundary patterns from noise. Furthermore, in sequences with gradual transitions, \\textbf{ESTimator++} shows improved temporal localization, likely due to the multi-scale receptive field of the AMTD. These observations align with its superior quantitative performance.\n\n请根据上述 LaTeX 片段，分析需要绘制哪些图表，并返回 JSON 格式的结果。\n"
      }
    ]
  },
  "response": {
    "raw_response": "```json\n{\n  \"figures\": [\n    {\n      \"filename\": \"online_kinetics_f1_lineplot.png\",\n      \"description\": \"A line plot comparing F1 scores of various online methods (TeSTra-BC, Sim-On-BC, OadTR-BC, MiniROAD-BC, ESTimator, ESTimator++) across Rel.Dis thresholds from 0.05 to 0.5 on the Kinetics-GEBD validation set. X-axis: Rel.Dis thresholds (0.05 to 0.5 in steps of 0.05), Y-axis: F1 scores. Each method is represented by a distinct line with markers. Highlights that ESTimator++ consistently outperforms others, especially at lower thresholds, with an average F1 of 0.778, showing +3.0% improvement over ESTimator.\",\n      \"old_string\": \"On Kinetics-GEBD (Table~\\\\ref{tab:online_k400}), \\\\textbf{ESTimator++} sets a new state-of-the-art, outperforming all online baselines across all Rel.Dis thresholds.\",\n      \"new_string\": \"On Kinetics-GEBD (Table~\\\\ref{tab:online_k400}), \\\\textbf{ESTimator++} sets a new state-of-the-art, outperforming all online baselines across all Rel.Dis thresholds.\\\\begin{figure}[htbp]\\\\centering\\\\includegraphics[width=0.8\\\\textwidth]{online_kinetics_f1_lineplot.png}\\\\caption{Line plot of F1 scores versus Rel.Dis thresholds for online methods on Kinetics-GEBD. ESTimator++ achieves the highest performance across all thresholds, with an average F1 of 0.778.}\\\\label{fig:online_kinetics_f1_lineplot}\\\\end{figure}\"\n    },\n    {\n      \"filename\": \"online_tapos_f1_heatmap.png\",\n      \"description\": \"A heatmap visualizing F1 scores for online methods (TeSTra-BC, Sim-On-BC, OadTR-BC, MiniROAD-BC, ESTimator, ESTimator++) across Rel.Dis thresholds from 0.05 to 0.5 and the average on the TAPOS validation set. Rows: methods, Columns: Rel.Dis thresholds plus average. Color intensity represents F1 score values (darker for higher scores). Emphasizes ESTimator++'s superiority at stricter thresholds, achieving an average F1 of 0.569, with +2.2% over ESTimator.\",\n      \"old_string\": \"A similar trend is observed on TAPOS (Table~\\\\ref{tab:online_tapos}). \\\\textbf{ESTimator++} achieves the best average F1 score of \\\\textbf{0.569}, outperforming ESTimator by +2.2\\\\% and MiniROAD-BC by +4.1\\\\%.\",\n      \"new_string\": \"A similar trend is observed on TAPOS (Table~\\\\ref{tab:online_tapos}). \\\\textbf{ESTimator++} achieves the best average F1 score of \\\\textbf{0.569}, outperforming ESTimator by +2.2\\\\% and MiniROAD-BC by +4.1\\\\%.\\\\begin{figure}[htbp]\\\\centering\\\\includegraphics[width=0.8\\\\textwidth]{online_tapos_f1_heatmap.png}\\\\caption{Heatmap of F1 scores for online methods across Rel.Dis thresholds on TAPOS. ESTimator++ shows the highest values, particularly at strict thresholds, with an average of 0.569.}\\\\label{fig:online_tapos_f1_heatmap}\\\\end{figure}\"\n    },\n    {\n      \"filename\": \"offline_kinetics_avgf1_barchart.png\",\n      \"description\": \"A bar chart comparing average F1 scores of offline and online methods (including supervised and unsupervised offline like BMN, TCN, PC, CoSeg, and online like ESTimator, ESTimator++) on the Kinetics-GEBD dataset. X-axis: methods grouped by setting (offline supervised, offline unsupervised, online), Y-axis: average F1 scores. Bars are colored by setting, with error bars if applicable (though not in data). Illustrates that ESTimator++ (0.778) rivals top offline methods like CoSeg (0.782) and surpasses most supervised offline ones except PC.\",\n      \"old_string\": \"Remarkably, on Kinetics-GEBD, \\\\textbf{ESTimator++} with an average F1 of \\\\textbf{0.778} not only leads all online methods but also competes favorably with state-of-the-art offline approaches.\",\n      \"new_string\": \"Remarkably, on Kinetics-GEBD, \\\\textbf{ESTimator++} with an average F1 of \\\\textbf{0.778} not only leads all online methods but also competes favorably with state-of-the-art offline approaches.\\\\begin{figure}[htbp]\\\\centering\\\\includegraphics[width=0.8\\\\textwidth]{offline_kinetics_avgf1_barchart.png}\\\\caption{Bar chart of average F1 scores for offline and online methods on Kinetics-GEBD. ESTimator++ achieves 0.778, competing closely with top offline methods.}\\\\label{fig:offline_kinetics_avgf1_barchart}\\\\end{figure}\"\n    },\n    {\n      \"filename\": \"offline_tapos_f1_scatterplot.png\",\n      \"description\": \"A scatter plot showing F1 scores versus Rel.Dis thresholds for offline and online methods on the TAPOS dataset. X-axis: Rel.Dis thresholds (0.05 to 0.5), Y-axis: F1 scores. Each method is a scatter line with points, differentiated by colors and markers (e.g., PC, TCN, PA, ESTimator, ESTimator++). Demonstrates ESTimator++'s performance (avg 0.569) outperforming most offline supervised methods except PC, with strong results at lower thresholds.\",\n      \"old_string\": \"On the TAPOS dataset (Table~\\\\ref{tab:offline_tapos}), \\\\textbf{ESTimator++} again demonstrates strong competitiveness, achieving an average F1 of \\\\textbf{0.569}.\",\n      \"new_string\": \"On the TAPOS dataset (Table~\\\\ref{tab:offline_tapos}), \\\\textbf{ESTimator++} again demonstrates strong competitiveness, achieving an average F1 of \\\\textbf{0.569}.\\\\begin{figure}[htbp]\\\\centering\\\\includegraphics[width=0.8\\\\textwidth]{offline_tapos_f1_scatterplot.png}\\\\caption{Scatter plot of F1 scores versus Rel.Dis thresholds for methods on TAPOS. ESTimator++ rivals many offline methods with an average of 0.569.}\\\\label{fig:offline_tapos_f1_scatterplot}\\\\end{figure}\"\n    },\n    {\n      \"filename\": \"inference_efficiency_barchart.png\",\n      \"description\": \"A bar chart comparing inference time per frame (in ms) for online methods (TeSTra-BC, MiniROAD-BC, ESTimator, ESTimator++) and noting total clip times for offline methods (TCN, PC). X-axis: methods, separated into online and offline groups, Y-axis: time (ms), with logarithmic scale for offline to show scale difference. Highlights ESTimator++'s efficiency at 8.2 ms per frame, balancing speed and accuracy for real-time use.\",\n      \"old_string\": \"As shown in Table~\\\\ref{tab:efficiency}, \\\\textbf{ESTimator++} processes frames at \\\\textbf{8.2 ms (∼122 FPS)}, which is significantly faster than the real-time requirement of 24 FPS for the Kinetics dataset.\",\n      \"new_string\": \"As shown in Table~\\\\ref{tab:efficiency}, \\\\textbf{ESTimator++} processes frames at \\\\textbf{8.2 ms (∼122 FPS)}, which is significantly faster than the real-time requirement of 24 FPS for the Kinetics dataset.\\\\begin{figure}[htbp]\\\\centering\\\\includegraphics[width=0.8\\\\textwidth]{inference_efficiency_barchart.png}\\\\caption{Bar chart of inference times for online and offline methods. ESTimator++ achieves efficient real-time performance at 8.2 ms per frame.}\\\\label{fig:inference_efficiency_barchart}\\\\end{figure}\"\n    }\n  ]\n}\n```",
    "usage": {
      "input_tokens": 7938,
      "output_tokens": 2717
    }
  },
  "parsed_success": true,
  "figures_count": 5
}